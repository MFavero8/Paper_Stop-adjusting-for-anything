{"title":"Simulation example","markdown":{"headingText":"Simulation example","headingAttr":{"id":"sec-ex1","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r}\n#| label: load-packages\n#| message: false\n\n#Load packages\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(rstan)\n\n```\n\n## Data-generating process\n\nThe data-generating process is described via the directed acyclic graph (DAG) in @fig-DAG.ex1-1. In this DAG, indoor air temperature $(\\text{T})$ influences productivity $(\\text{P})$ directly, while HVAC system $(\\text{H})$ influences productivity $(\\text{P})$ both directly (e.g., with air distribution and noise) and indirectly, passing through indoor air temperature $(\\text{T})$.\n\n```{r}\n#| label: fig-DAG.ex1-1\n#| fig-cap: 'Graphical representation via DAG of the data-generating process.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 4.5\n#| fig-height: 2.7\n#| out-width: '60%'\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\ndag_coords.ex1 <-\n  data.frame(name = c('T', 'H', 'P'),\n             x = c(1, 3.5, 6),\n             y = c(1, 3, 1))\n\nDAG.ex1 <-\n  dagify(T ~ H,\n         P ~ T + H,\n         coords = dag_coords.ex1)\n\nggplot(data = DAG.ex1, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(colour = 'black', size = 10, parse = TRUE,\n                family = c('mono'),\n                label = c(expression(bold(H)), expression(bold(P)),  expression(bold(T)))) +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  annotate('text', x = 1, y = 0.7, label = 'indoor air temperature', \n           size = 4, hjust = 0.4, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 3.3, label = 'HVAC system', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'productivity', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n```\n\nThe DAG in @fig-DAG.ex1-1 can be written as:\n\n-   $T \\sim f_{T}(H)$, read as 'indoor air temperature $(\\text{T})$ is some function of HVAC system $(\\text{H})$'.\n-   $P \\sim f_{P}(T, H)$, read as 'productivity $(\\text{P})$ is some function of indoor air temperature $(\\text{T})$ and HVAC system $(\\text{H})$'.\n\n## Synthetic data set\n\nTo generate synthetic data, we defined the custom function `data.sim_ex1()`. This function takes as inputs the sample size `n` and generates synthetic data according to the DAG in @fig-DAG.ex1-1.\n\n```{r}\ndata.sim_ex1 <- function(n) {\n  b_hvac.prod = c(0, 5) #direct causal effect of H on P\n  b_temp.prod = -1 #direct causal effect of T on P\n  #Simulate HVAC\n  H <- factor(sample(c('classic', 'alternative'), size = n, replace = TRUE))  \n  #Simulate indoor air temperature\n  T <- rnorm(n = n, mean = ifelse(H == 'classic', 21, 22), sd = 0.7)\n  #Simulate productivity\n  P <- rnorm(n = n, mean = ifelse(H == 'classic', 75 + b_hvac.prod[1] + b_temp.prod * T, 75 + b_hvac.prod[2] + b_temp.prod * T), sd = 2)  \n  #Return tibble with simulated values\n  return(tibble(H, T, P))\n  }\n```\n\nFrom this data generation mechanism, we simulated the target population, which consists of one million observations.\n\n```{r}\n\nset.seed(2025) #set random number for reproducibility\n#Simulate the population\nex1_population <- data.sim_ex1(n = 1e6)\n#Set HVAC reference category to 'classic'\nex1_population$H <- relevel(ex1_population$H, ref = 'classic')\n#View the data frame\nex1_population\n```\n\nFrom this population, we obtained one data set of five thousand observations using simple random sampling.\n\n```{r}\nn_sims <- 1e3 #number of data sets to simulate\n\n#Set random number for reproducibility\nset.seed(2025)  \n#Generate a vector of random numbers for reproducibility\nex1_random.seed <- sample(1:1e5, size = n_sims, replace = FALSE)\n```\n\n```{r}\nset.seed(ex1_random.seed[1])\n#Sample one data set of 5,000 observations\nex1_sample.random <- \n  ex1_population %>% \n  slice_sample(n = 5e3) #take a simple random sample of size 5,000 \n\n#View the data frame\nex1_sample.random\n```\n\n## Data analysis\n\nIn this example, the target of our analysis is the total average causal effect, ACE (also known as total average treatment effect, ATE) of indoor air temperature $(\\text{T})$ on productivity $(\\text{P})$, which stands for the expected increase of $\\text{P}$ in response to a unit increase in $\\text{T}$ due to an intervention. The causal effect of interest is visualized in @fig-DAG.ex1-2.\n\n```{r}\n#| label: fig-DAG.ex1-2\n#| fig-cap: 'Graphical representation via DAG of the data-generating process. The green line indicates the causal question of interest, and the number on the path indicates the total average causal effect.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 4.5\n#| fig-height: 2.7\n#| out-width: '60%'\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\nggplot(data = DAG.ex1, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_dag_text(colour = 'black', size = 10, parse = TRUE,\n                family = c('mono'),\n                label = c(expression(bold(H)), expression(bold(P)),  expression(bold(T)))) +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  annotate('text', x = 1, y = 0.7, label = 'indoor air temperature', \n           size = 4, hjust = 0.4, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 3.3, label = 'HVAC system', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'productivity', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  #causal effect number\n  annotate('text', x = 3.5, y = 1.2, label = '-1', \n           size = 4.5, hjust = 0.5, colour = 'black', parse = TRUE) +\n\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n```\n\n### Identification\n\nThe first step to answer the causal question of interest is identification. Identification answers a ‘theoretical’ question by determining whether a causal effect can, in principle, be estimated from observed data. The backdoor criterion and its generalization, the adjustment criterion, allow us to understand whether our causal effect of interest can be identified and, if so, which variables we should (or should not) statistically adjust for (i.e., the adjustment set) to estimate the causal effect from the data.\n\nGiven its simplicity, we will first apply the backdoor criterion to identify valid adjustment sets to estimate the causal effect of interest. If the backdoor criterion is not applicable, we will apply its generalization, the adjustment criterion.\n\n#### Backdoor criterion\n\nApplying the backdoor criterion revealed the existence of a backdoor path (i.e., a non-causal path) from indoor air temperature $(\\text{T})$ to productivity $(\\text{P})$. Specifically, the backdoor path is $\\text{T} \\leftarrow \\text{H} \\rightarrow \\text{P}$. Since HVAC system $(\\text{H})$ is a common cause of $\\text{T}$ and $\\text{P}$, association can flow from $\\text{T}$ to $\\text{P}$ through $\\text{H}$. As a result, there is confounding. To close this backdoor path we need to adjust for $\\text{H}$.\n\nGiven the DAG in @fig-DAG.ex1-2, we can use the `adjustmentSets()` function to identify the adjustment set algorithmically. It is essential to note that this function applies the adjustment criterion and not the backdoor criterion. As such, the `adjustmentSets()` function can find adjustment sets even when the backdoor criterion is not applicable.\n\n```{r}\n#| code-fold: false\n\nadjustmentSets(DAG.ex1,\n               exposure = 'T', #indoor air temperature\n               outcome = 'P', #productivity\n               type = 'all', \n               effect = 'total', \n               max.results = Inf)\n\n```\n\nAs expected, the resulting adjustment set includes HVAC system $(\\text{H})$. Therefore, to get the correct estimate of the atotal average causal effect, we need to adjust for $\\text{H}$; failing to do so will lead to bias.\n\n### Estimation\n\nFollowing the identification step is the estimation step. This step addresses a statistical question by determining how the causal effect identified in the previous step can be estimated. To perform this step, we used a parametric (model-based) estimator, specifically, linear regression. This was possible because we designed the illustrative examples to be simple and with a linear relationship between the variables. This way, we limited the complexity of the examples themselves and shifted the focus to the application of the backdoor criterion to define 'correct' adjustment sets.\n\nFor transparency and understanding, all (implicit) assumptions used for this illustrative example are (explicitly) provided in @tbl-summary.ex1.\n\n```{r}\n#| label: tbl-summary.ex1\n#| tbl-cap: Summary description of the simulation example\n#| echo: false\n\nis_empty <- function(x){\n    return(ifelse(x == '', TRUE, FALSE))\n  }\n\ndata.frame(colA = c('Research question', rep('Assumptions', 6), rep('Variables', 3)),\n           colB = c('Total average causal effect (ACE) of indoor air temperature (T) on productivity (P).',\n                    'Random sample (simple random sampling): everyone in the population has an equal chance of being selected into the sample.',\n                    'Limited random variability: large sample size.',\n                    'Independence of observations: each observation represents independent bits of information.',\n                    'No confounding: the DAG includes all shared causes among the variables.',\n                    'No model error: perfect functional form specification.',\n                    'No measurement error: all variables are measured perfectly.', \n                    'Indoor air temperature (T): continuous variable [unit: °C]',\n                    \"HVAC (H): categorical variable ['classic'; 'alternative']\",\n                    \"Produtivity (P): continous variable [unit: -]\")) %>%\n  group_by(colA) %>% \n  mutate(colA = ifelse(row_number() == 1, colA, '')) %>% \n  ungroup() %>% \n  gt() %>%\n  tab_options(column_labels.hidden = TRUE) %>%\n  tab_style(style = list(cell_borders(sides = c('top', 'bottom'), weight = px(0))),\n            locations = list(cells_body(columns = colA, rows = is_empty(colA)))) %>%\n  tab_style(style = list(cell_text(weight = 'bold', v_align = 'top')),\n            locations = list(cells_body(columns = colA))) %>%\n  tab_style(style = list(cell_text(v_align = 'top')),\n            locations = list(cells_body(columns = colB))) %>%  \n  tab_style(style = list(cell_fill(color = 'white')),\n            locations = list(cells_body(columns = colA))) %>%\n  cols_width(colA ~ pct(22)) \n\n```\n\nTo carry out the estimation step, we utilised linear regression within both the frequentist and Bayesian frameworks. Specifically, we will run two regression models:\n\n-   `Model.1` will include only indoor air temperature $(\\text{T})$ as predictor;\n-   `Model.2` will include both indoor air temperature $(\\text{T})$ and HVAC system $(\\text{H})$ as predictors.\n\nThe results of the fitted statistical models (i.e., `Model.1` and `Model.2`) are presented here.\n\n#### Frequentist framework\n\n```{r}\n\n#Fit the linear regression model with T and P (Model 1)\nex1_Model.1 <-\n  lm(formula = P ~ T,\n     data = ex1_sample.random)\n#View of the model summary\nsummary(ex1_Model.1)\n```\n\n```{r}\n\n#Fit the linear regression model with T , H and P (Model 2)\nex1_Model.2 <-\n  lm(formula = P ~ T + H,\n     data = ex1_sample.random)\n#View of the model summary\nsummary(ex1_Model.2)\n```\n\nThe estimated coefficients for the two models are then plotted in @fig-DAG.ex1-estimate.\n\n```{r}\n#| label: fig-DAG.ex1-estimate\n#| fig-cap: 'Estimates of the temperature coefficient for the two models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 1.5\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\nb_temp.prod = -1 \n\ndata.frame(model = c('Model.1', 'Model.2'),\n           estimate = c(coef(ex1_Model.1)['T'], coef(ex1_Model.2)['T']),\n           lower.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 1], confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 1]),\n           upper.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 2], confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 2])) %>%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.25, 1.25)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n```\n\n@fig-DAG.ex1-estimate shows the estimates (point estimate and 95 confidence interval) of the coefficient for temperature for `Model.1` and `Model.2`.\n\nFor `Model.1` we found a positive coefficient between indoor air temperature $(\\text{T})$ and productivity $(\\text{P})$ equal to `r sprintf(coef(ex1_Model.1)['T'], fmt = '%#.3f')` with 95% CI \\[`r sprintf(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', ], fmt = '%#.3f')`\\]. Since the 95% CI does not include zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = `r sprintf('%.3g', summary(ex1_Model.1)$coef['T', 4])`). Additionally, since the 95% CI does not include the data-generating parameter for indoor air temperature (i.e., `b_temp.prod = -1`), we can deduce that the estimated coefficient for indoor air temperature is statistically significantly different from -1 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the `linearHypothesis()` function. Specifically,\n\n```{r}\n#Test for b_temp.prod = -1\ncar::linearHypothesis(ex1_Model.1, 'T = -1') # car:: access the function from the car package without loading it in the environment \n```\n\nThe resulting p-value is `r sprintf('%.3g', car::linearHypothesis(ex1_Model.1, 'T = -1')[2,'Pr(>F)'])`, which indicates that we can reject the null hypothesis (i.e., `T = -1`) at the 0.05 level. This result suggests that the regression coefficient for indoor air temperature (i.e., `r sprintf(coef(ex1_Model.1)['T'], fmt = '%#.3f')`) is statistically significantly different from -1. Since the estimated causal effect from `Model.1` is biased, we would erroneously conclude that an increase of 1°C in indoor air temperature causes an increase in productivity by `r sprintf(coef(ex1_Model.1)['T'], fmt = '%#.3f')` units (95% CI \\[`r sprintf(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', ], fmt = '%#.3f')`\\]).\n\nFor `Model.2` we found a negative coefficient between indoor air temperature $(\\text{T})$ and productivity $(\\text{P})$ equal to `r sprintf(coef(ex1_Model.2)['T'], fmt = '%#.3f')` with 95% CI \\[`r sprintf(confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', ], fmt = '%#.3f')`\\]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = `r sprintf('%.3g', summary(ex1_Model.2)$coef['T', 4])`). Additionally, since the 95% CI includes the data-generating parameter for indoor air temperature (i.e., `b_temp.prod = -1`), we can deduce that the estimated coefficient for indoor air temperature is not statistically significantly different from -1 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the `linearHypothesis()` function. Specifically,\n\n```{r}\n#Test for T = -1\ncar::linearHypothesis(ex1_Model.2, 'T = -1') # car:: access the function from the car package without loading it in the environment \n```\n\nThe resulting p-value is `r sprintf('%.3g', car::linearHypothesis(ex1_Model.2, 'T = -1')[2,'Pr(>F)'])`, which indicates that we fail to reject the null hypothesis (i.e., `T = -1`) at the 0.05 level. This result suggests that the regression coefficient for indoor air temperature (i.e., `r sprintf(coef(ex1_Model.2)['T'], fmt = '%#.3f')`) is not statistically significantly different from -1. Since the estimated causal effect from `Model.2` is unbiased, we would correctly conclude that an increase of 1°C in indoor air temperature causes a decrease in productivity by `r sprintf(coef(ex1_Model.2)['T'], fmt = '%#.3f')` units (95% CI \\[`r sprintf(confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', ], fmt = '%#.3f')`\\]).\n\nImportantly, for both `Model.1` and `Model.2`, the 95% confidence interval means that if we were to repeat the sampling process and calculate the interval many times, 95% of those calculated intervals would contain the true population parameter. To highlight this, we can repeat the analysis by fitting the two models to one thousand data sets randomly selected from our population.\n\nThe for-loop shown in the code below performs the following operations. First, sample (using simple random sample) a data set of 5,000 observations from the target population. Subsequently, perform linear regression using `Model.1` and `Model.2` and store the estimated coefficients for `temperature`, its standard error and 95% confidence interval in the data frame `coefs_ex1`. This operation is repeated a thousand times, resulting in the data frame `coefs_ex1` containing the estimates (point estimate, standard error and confidence interval) of a thousand random samples of size 5,000 using both `Model.1` and `Model.2`.\n\n```{r}\nn_model <- c('mod.1', 'mod.2')     \n\nn_row <- n_sims*length(n_model)\n\n#Create an empty data frame\nempty.df <- data.frame(matrix(NA, nrow = n_row, ncol = 7))\n#Rename the data frame columns\ncolnames(empty.df) <- c('sim.id', 'estimate', 'se', 'CI_2.5', 'CI_97.5',\n                        'model', 'coverage')\n\n#Sample a thousand data sets of 5,000 observations and perform linear regression \ncoefs_ex1 <- empty.df #assign the empty data frame\nk = 1\nfor (i in 1:n_sims){\n  set.seed(ex1_random.seed[i]) #set unique seed for each simulation \n#Sample data set from population   \n  sample.random <- \n    ex1_population %>% \n    slice_sample(n = 5e3) #take a simple random sample of size 5,000\n#Fit models\n  for (j in 1:length(n_model)){\n    if (n_model[j] == 'mod.1'){\n      fit <- lm(formula = P ~ T,\n                data = sample.random)\n    } else {\n      fit <- lm(formula = P ~ T + H,\n                data = sample.random)\n    }  \n#Compile matrix  \n  coefs_ex1[k, 1] <- i #simulation ID\n  coefs_ex1[k, 2] <- coef(fit)['T'] #point estimate\n  coefs_ex1[k, 3] <- summary(fit)$coef['T','Std. Error'] #standard error\n  coefs_ex1[k, 4:5] <- confint(fit, level = 0.95, type = 'Wald')['T', ] #confidence interval (Wald)\n  coefs_ex1[k, 6] <- n_model[j] #sample size\n  k = k + 1\n  }\n}\ncoefs_ex1 <- as_tibble(coefs_ex1)\n#View the data frame\ncoefs_ex1\n\n```\n\nThe `coverage` is defined by setting its value to `1` if the confidence interval overlaps the data-generating parameter for `temperature` (i.e., `b_temp.prod = -1`) and `0` otherwise.\n\n```{r}\n\n#Calculate coverage\ncoefs_ex1 <-\n  coefs_ex1 %>%\n  mutate(coverage = case_when(CI_2.5 > b_temp.prod | CI_97.5 < b_temp.prod ~ 0,\n                              CI_2.5 <= b_temp.prod & CI_97.5 >= b_temp.prod ~ 1,\n                              .default = NA))\n#View the data frame\ncoefs_ex1\n```\n\nThe results are then plotted in @fig-DAG.ex1-CI.\n\n```{r}\n#| label: fig-DAG.ex1-CI\n#| fig-cap: 'Estimates of the temperature coefficient (only the first hundreds of the thousand simulations are shown). The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. In red are highlighted the confidence intervals that do not overlap the parameter used to generate the data (dashed blue line).'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 4\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\nmodel_names <- c('mod.1' = 'Model.1', \n                 'mod.2' = 'Model.2')\n\nggplot(data = subset(coefs_ex1, sim.id <= 2e2), aes(x = sim.id, y = estimate, ymin = CI_2.5, ymax = CI_97.5, colour = as.factor(coverage))) + \n  geom_hline(yintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 1, fill = 'white', stroke = 0.5) +\n  scale_colour_manual('', values = c('black', '#FF4040'),\n                      breaks = c('1','0')) +\n  scale_y_continuous('Estimate') +\n  scale_x_continuous('Simulation ID') +\n  facet_wrap(model~., \n             labeller = labeller(model = model_names),\n             nrow = 2,\n             scales = 'fixed') +\n  theme(legend.position = 'none', \n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n```\n\n```{r}\n#| echo: false\n\n#Calculate average coverage \ncoverage <- coefs_ex1 %>%\n  select(model, estimate, coverage) %>%\n  group_by(model) %>%\n  summarise(mean.coverage = sprintf('%0.1f%%', mean(coverage)*100))\n```\n\n@fig-DAG.ex1-CI shows the first 200 estimates (point estimate and confidence interval) of the coefficient for indoor air temperature for `Model.1` and `Model.2`. If all the thousand simulations are considered, the frequency of the coverage of the calculated confidence intervals (i.e., how many times the confidence intervals overlap the data-generating parameter) is `r coverage[1, 'mean.coverage']` and `r coverage[2, 'mean.coverage']` for `Model.1` and `Model.2`, respectively. Since the estimates of the causal effect for `Model.1` are biased, the calculated confidence intervals for this model do not have the expected coverage. In fact, confidence intervals only quantify the uncertainty due to random error (i.e., sample variability), not systematic error (i.e., bias). Instead, the estimate from `Model.2` are unbiased and the calculated 95% confidence intervals have the expected coverage (i.e., overlap the data-generating parameter 95% of the times).\n\nWe can also visualize all the 1,000 the estimates of the coefficient for indoor air temperature for `Model.1` and `Model.2` (i.e., the white dots in @fig-DAG.ex1-CI).\n\n```{r}\n#| label: fig-DAG.ex1-mean-estimate\n#| fig-cap: 'Histogram of the 1,000 estimates of the temperature coefficient for the two models. The blue dashed line represents the parameter used to generate the data.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 3\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\nggplot(data = coefs_ex1, aes(x = estimate, y = ifelse(after_stat(count) > 0, after_stat(count), NA))) +\n  geom_histogram(binwidth = 0.05, fill = 'white', colour = 'grey50') +\n  geom_vline(xintercept = -1, colour = 'blue', linetype = 'dashed') +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.5, 1.5)) +\n  facet_grid(model~., \n             labeller = labeller(model = model_names),\n             scales = 'fixed') +\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n```\n\n```{r}\n#| echo: false\n\n#Calculate average estimate\nmean.estimate <- coefs_ex1 %>%\n  select(model, estimate) %>%\n  group_by(model) %>%\n  summarise(mean.estimate = mean(estimate),\n            sd.estimate = sd(estimate))\n```\n\nIn @fig-DAG.ex1-mean-estimate, the estimated coefficients are visualized through a histogram. Here, we can see that the estimates for `Model.1` are not clustered around the data-generating parameter (blue dashed line) having a mean estimate of `r sprintf(mean.estimate[1, 'mean.estimate'], fmt = '%#.3f')` (with `r sprintf(mean.estimate[1, 'sd.estimate'], fmt = '%#.3f')` standard deviation). In contrast, the estimates for `Model.2` having a mean estimate of `r sprintf(mean.estimate[2, 'mean.estimate'], fmt = '%#.3f')` (with `r sprintf(mean.estimate[2, 'sd.estimate'], fmt = '%#.3f')` standard deviation) are centred around the data-generating parameter.\n\nAs expected, the inclusion of HVAC system $(\\text{H})$ in the regression model (i.e., using `Model.2`) leads to the correct estimate of the total average causal effect, while its exclusion (i.e., using `Model.1`) leads to bias. Specifically, the estimated effect is reversed. This phenomenon is known as Simpson's Paradox. Generally, Simpson's Paradox happens when a trend that appears in different groups of data reverses (or disappears) when the groups are combined.\n\nTo better explain this apparent paradox, we can plot the data and the fitted regression lines for only the data with `HVAC = classic`, the data with `HVAC = alternative`, and the data combined (i.e., without considering the groups).\n\n```{r}\n#| label: fig-DAG.ex1-paradox\n#| fig-cap: 'Visualization of the reversed regression line direction in the total sample compared to the subgroups.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-dpi: 900\n#| fig-width: 6\n#| fig-height: 5\n#| warning: false\n#| code-fold: true\n#| message: false\n\n\npred_ex1_Model.1_separete <-\n  rbind(cbind(filter(ex1_sample.random, H == 'classic'), \n              predict(lm(formula = P ~ T, data = filter(ex1_sample.random, H == 'classic')), \n                      interval = 'confidence', level = 0.95), pred = 'classic'), \n        cbind(filter(ex1_sample.random, H == 'alternative'), \n              predict(lm(formula = P ~ T, data = filter(ex1_sample.random, H == 'alternative')),\n                      interval = 'confidence', level = 0.95), pred = 'alternative'))\n\npred_ex1_Model.1 <- \n  cbind(ex1_sample.random, \n      predict(ex1_Model.1, interval = 'confidence', level = 0.95))\n\n#Plot \nex1_sample.random %>%\n  ggplot(aes(x = T, y = P)) +\n  geom_point(aes(colour = H), shape = 19, size = 2, alpha = 0.10, stroke = NA) +\n  geom_ribbon(data = pred_ex1_Model.1_separete, aes(x = T , y = fit, ymin = lwr, ymax = upr, fill = H), alpha = 0.2, show.legend = NA) +\n  geom_line(data = pred_ex1_Model.1_separete, aes(x = T , y = fit, colour = H)) + \n  geom_ribbon(data = pred_ex1_Model.1, aes(x = T , y = fit, ymin = lwr, ymax = upr, fill = 'all.data'), alpha = 0.2, show.legend = NA) +\n  geom_line(data = pred_ex1_Model.1, aes(x = T , y = fit, colour = 'all.data')) + \n  scale_colour_manual('',\n                      breaks = c('classic', 'alternative', 'all.data'),\n                      labels = c('classic', 'alternative', 'all data'),\n                      values = c('#D55E00','#0072B2', 'black')) +\n  scale_fill_manual('',\n                    breaks = c('classic', 'alternative', 'all.data'),\n                    labels = c('classic', 'alternative', 'all data'),\n                    values = c('#D55E00','#0072B2', 'black')) +\n  scale_x_continuous('Indoor air temperature') + \n  scale_y_continuous('Productivity') +\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + \n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n  \n```\n\nIn @fig-DAG.ex1-paradox the three colored lines represent the regression lines for only the data with `HVAC = classic` (in orange), the data with `HVAC = alternative` (in blue), and the combined data (in black). The regression lines in orange and blue are obtained by fitting the following regression models:\n\n-   subset of data with `H == 'classic'`\n\n```{r}\n#Filter data with H == 'classic'\nex1_sample.random_classic <- filter(ex1_sample.random, H == 'classic')\n#Fit the linear regression model for only data with H == 'classic'\nex1_Model.1_classic <- lm(formula = P ~ T, data = ex1_sample.random_classic)\n#View of the model summary\nex1_Model.1_classic\n```\n\n-   subset of data with `H == 'alternative'`\n\n```{r}\n#Filter data with H == 'alternative'\nex1_sample.random_alternative <- filter(ex1_sample.random, H == 'alternative')\n#Fit the linear regression model for only data with H == 'alternative'\nex1_Model.1_alternative <- lm(formula = P ~ T, data = ex1_sample.random_alternative)      \n#View of the model summary\nex1_Model.1_alternative        \n```\n\nWe can visualize the estimated coefficients for these new regression models and compared them with the one estimated from `Model.1` and `Model.2`.\n\n```{r}\n#| label: fig-DAG.ex1-estimate2\n#| fig-cap: 'Estimates of the temperature coefficient for the four models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 2\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\n#Plot\ndata.frame(model = c('Model.1', 'Model.1_classic', 'Model.1_alternative', 'Model.2'),\n           estimate = c(coef(ex1_Model.1)['T'], \n                        coef(ex1_Model.1_classic)['T'], \n                        coef(ex1_Model.1_alternative)['T'], \n                        coef(ex1_Model.2)['T']),\n           lower.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 1],\n                           confint(ex1_Model.1_classic, level = 0.95, type = 'Wald')['T', 1],\n                           confint(ex1_Model.1_alternative, level = 0.95, type = 'Wald')['T', 1],\n                           confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 1]),\n           upper.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 2], \n                           confint(ex1_Model.1_classic, level = 0.95, type = 'Wald')['T', 2], \n                           confint(ex1_Model.1_alternative, level = 0.95, type = 'Wald')['T', 2], \n                           confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 2])) %>%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.25, 1.25)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n```\n\n@fig-DAG.ex1-estimate2, we can see that only the estimated coefficients for `Model.1` are biased. As explained in the identification step, to identify the causal effect of indoor air temperature $(\\text{T})$ on productivity $(\\text{P})$, we need to block the backdoor path (i.e., the non-causal path) $\\text{T} \\leftarrow \\text{H} \\rightarrow \\text{P}$. Fitting a separate regression model to the subset of data with `H == 'classic'` and one to `H == 'alternative'` is equivalent to conditioning for $\\text{H}$, closing the backdoor path and allowing unbiased estimation of the causal effect of indoor air temperature $(\\text{T})$ on productivity $(\\text{P})$ (although this comes at the price of a larger standard error and consequently wider confidence intervals compared to `Model.2`, where $\\text{H}$ is added as a predictor).\n\n#### Bayesian framework\n\n<details>\n\n<summary>Click to expand</summary>\n\n```{r}\n#| message: false\n\n#Fit the linear regression model with T and P (Model 1)\nex1_Model.1 <- stan_glm(formula = P ~ T,\n                        family = gaussian(),\n                        data = ex1_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = 0, scale = 2),\n                        #Prior intercept\n                        prior_intercept = normal(location = 56, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility   \n#View of the model\nex1_Model.1\n```\n\n```{r}\n#| message: false\n\n#Fit the linear regression model with T , H and P (Model 2)\nex1_Model.2 <- stan_glm(formula = P ~ T + H,\n                        family = gaussian(),\n                        data = ex1_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(2, 5)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 56, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex1_Model.2\n```\n\nIn Bayesian analysis, there are important diagnostics that have to be carried out in order to assessed the convergence and efficiency of the Markov Chains. This is done by using the `monitor()` function which computes summaries of MCMC (Markov Chain Monte Carlo) draws and monitor convergence. Specifically, we will look at `Rhat`, `Bulk_ESS` and `Tail_ESS` metrics.\n\n```{r}\n#Diagnostics for model 1\nmonitor(ex1_Model.1$stanfit)  \n```\n\n```{r}\n#Diagnostics for model 2\nmonitor(ex1_Model.2$stanfit) \n```\n\n`Rhat` is a metric used to assess the convergence of Markov Chain Monte Carlo (MCMC) simulations. It helps determine if the MCMC chains have adequately explored the target posterior distribution. Specifically, it compares the between- and within-chain estimates for model parameters: If chains have not mixed well (i.e., the between- and within-chain estimates do not agree), R-hat is larger than 1. A general rule of thumb is to use the sample only if R-hat is less than 1.05; a larger value suggests that the chains have not mixed well, and the results might not be reliable. In our two models, all `Rhat` are equal to 1 indicating that the chains have mixed well and have adequately explored the target posterior distribution.\n\n`Bulk_ESS` and `Tail_ESS` stand for 'Bulk Effective Sample Size' and 'Tail Effective Sample Size,' respectively. Since MCMC samples are not truly independent (they are correlated), these metrics assess the sampling efficiencies, that is, they help evaluating how efficiently the MCMC sampler is exploring the parameters space.\n\n-   `Bulk_ESS` is useful measure for sampling efficiency in the bulk (center) of the distribution (e.g., efficiency of mean and median estimates);\n-   `Tail_ESS` is useful measure for sampling efficiency in the tails of the distribution (e.g., efficiency of variance and tail quantile estimates). A general rule of thumb is that both `Bulk-ESS` and `Tail-ESS` should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable. In our two models, all `Bulk-ESS` and `Tail-ESS` are well above 400 (i.e., 100 multiplied by 4, the number of chains we used) indicating that estimates of posterior quantiles are reliable.\n\nSince we have established that the posteriors are reliable, we can now explore the model estimates.\n\nThe estimated coefficients for the two models are then plotted in @fig-DAG.ex1-estimate-Bayesian.\n\n```{r}\n#| label: fig-DAG.ex1-estimate-Bayesian\n#| fig-cap: 'Posterior distribution of the temperature coefficient for the two models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 2\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\n#Extract draws from model 1 \npost_ex1_Model.1 <-\n  ex1_Model.1 %>% \n  spread_draws(T) %>% #extract draws from the fitted model\n  mutate(model = 'Model.1') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex1_Model.2 <-\n  ex1_Model.2 %>% \n  spread_draws(T) %>% #extract draws from the fitted model\n  mutate(model = 'Model.2') #add a new column to specify that the model\n\n#Combine draws\nplot.post <- rbind(post_ex1_Model.1, post_ex1_Model.2)\n\n# Plot\nplot.post  %>%\n  ggplot(aes(y = model, x = T)) +\n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.25, 1.25)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n```\n\n```{r}\n#| echo: false\n\nplot.post_mean.hdi <- \n  plot.post %>% \n  group_by(model) %>%\n  mean_hdi(.width = .95) #calculate 95 highest density intervals (HDI)\n```\n\n@fig-DAG.ex1-estimate-Bayesian shows the estimates (mean and 95 HDI) of the coefficient for temperature for `Model.1` and `Model.2`.\n\nFor `Model.1` we found a positive coefficient between indoor air temperature $(\\text{T})$ and productivity $(\\text{P})$ (mean = `r sprintf(plot.post_mean.hdi[1, 'T'], fmt = '%#.3f')`, 95% HDI \\[`r sprintf(plot.post_mean.hdi[1, '.lower'], fmt = '%#.3f')`, `r sprintf(plot.post_mean.hdi[1, '.upper'], fmt = '%#.3f')`\\]). The estimated causal effect is therefore biased, leading to the wrong conclusion that an increase of 1°C in indoor air temperature causes an increase in productivity by `r sprintf(plot.post_mean.hdi[1, 'T'], fmt = '%#.3f')` units, on average. In contrast, for `Model.2` we found a negative coefficient between indoor air temperature $(\\text{T})$ and productivity $(\\text{P})$ (mean = `r sprintf(plot.post_mean.hdi[2, 'T'], fmt = '%#.3f')`, 95% HDI \\[`r sprintf(plot.post_mean.hdi[2, '.lower'], fmt = '%#.3f')`, `r sprintf(plot.post_mean.hdi[2, '.upper'], fmt = '%#.3f')`\\]). The estimated causal effect is therefore unbiased, leading to the correct conclusion that an increase of 1°C in indoor air temperature causes a decrease in productivity by `r sprintf(plot.post_mean.hdi[2, 'T'], fmt = '%#.3f')` units, on average.\n\nImportantly, for both `Model.1` and `Model.2`, the 95% HDI is the range of parameter values within which the most credible 95% of the posterior distribution falls. Unlike frequentist confidence interval, the Bayesian 95% HDI has a direct probabilistic meaning: every point inside the HDI has a higher probability density than any point outside the interval. Therefore, given the model, the prior and the data, we can say that there is a 95% probability that the data-generating parameter (i.e., `b_temp.prod = -1`) lies within the HDI. However, since `Model.1` leads to a biased estimate, we will reach the wrong conclusion by stating that there is a 95% probability that the data-generating parameter lies within the \\[`r sprintf(plot.post_mean.hdi[1, '.lower'], fmt = '%#.3f')`, `r sprintf(plot.post_mean.hdi[1, '.upper'], fmt = '%#.3f')`\\] interval. This probability is 0%.\n\nAs expected, the inclusion of HVAC system $(\\text{H})$ in the regression model (i.e., using `Model.2`) leads to the correct estimate of the total average causal effect, while its exclusion (i.e., using `Model.1`) leads to bias. Specifically, the estimated effect is reversed. This phenomenon is known as Simpson's Paradox. Generally, Simpson's Paradox happens when a trend that appears in different groups of data reverses (or disappears) when the groups are combined.\n\nTo better explain this apparent paradox, we can plot the data and the fitted regression lines for only the data with `HVAC = classic`, the data with `HVAC = alternative`, and the data combined (i.e., without considering the groups).\n\n```{r}\n#| label: fig-DAG.ex1-paradox-Bayesian\n#| fig-cap: 'Visualization of the reversed regression line direction in the total sample compared to the subgroups.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-dpi: 900\n#| fig-width: 6\n#| fig-height: 5\n#| warning: false\n#| code-fold: true\n#| message: false\n\n#Expected value of the posterior predictive distribution (epred) for model 1\nepred_ex1_Model.1 <- \n  ex1_Model.1 %>%\n  epred_draws(newdata = ex1_sample.random, \n              seed = 2025) %>%\n  group_by(T) %>%\n  mean_hdi(.epred, .width = .95) %>%\n  ungroup()\n\n#Expected value of the posterior predictive distribution (epred) for model 2\nepred_ex1_Model.2 <- \n  ex1_Model.2 %>%\n  epred_draws(newdata = ex1_sample.random, \n              seed = 2025) %>%\n  group_by(T, H) %>%\n  mean_hdi(.epred, .width = .95) %>%\n  ungroup()\n\n#Plot \nex1_sample.random %>%\n  ggplot(aes(x = T, y = P)) +\n  geom_point(aes(colour = H), shape = 19, size = 2, alpha = 0.15, stroke = NA) +\n  \n  geom_ribbon(data = epred_ex1_Model.2, aes(x = T , y = .epred, ymin = .lower, ymax = .upper, fill = H), alpha = 0.2, show.legend = NA) +\n  geom_line(data = epred_ex1_Model.2, aes(x = T , y = .epred, colour = H)) + \n  \n  geom_ribbon(data = epred_ex1_Model.1, aes(x = T , y = .epred, ymin = .lower, ymax = .upper, fill = 'all.data'), alpha = 0.2, show.legend = NA) +\n  geom_line(data = epred_ex1_Model.1, aes(x = T , y = .epred, colour = 'all.data')) + \n  scale_colour_manual('',\n                      breaks = c('classic', 'alternative', 'all.data'),\n                      labels = c('classic', 'alternative', 'all data'),\n                      values = c('#D55E00','#0072B2', 'black')) +\n  scale_fill_manual('',\n                    breaks = c('classic', 'alternative', 'all.data'),\n                    labels = c('classic', 'alternative', 'all data'),\n                    values = c('#D55E00','#0072B2', 'black')) +\n  scale_x_continuous('Indoor air temperature') + \n  scale_y_continuous('Productivity') +\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + \n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n```\n\nIn @fig-DAG.ex1-paradox-Bayesian the three colored lines represent the regression lines for only the data with `HVAC = classic` (in orange), the data with `HVAC = alternative` (in blue), and the combined data (in black). The regression lines in orange and blue are obtained by fitting the following regression models:\n\n-   subset of data with `H == 'classic'`\n\n```{r}\n#| message: false\n\n#Filter data with H == 'classic'\nex1_sample.random_classic <- filter(ex1_sample.random, H == 'classic')\n#Fit the linear regression model for only data with H == 'classic'\nex1_Model.1_classic <- stan_glm(formula = P ~ T,\n                                    family = gaussian(),\n                                    data = ex1_sample.random_classic,\n                                    #Prior coefficients\n                                    prior = normal(location = 0, scale = 5),\n                                    #Prior intercept\n                                    prior_intercept = normal(location = 56, scale = 10),\n                                    #Prior sigma\n                                    prior_aux = exponential(rate = 0.5),\n                                    iter = 4000, warmup = 1000, \n                                    save_warmup = TRUE,\n                                    chains = 4, cores = 4,\n                                    seed = 2025) #for reproducibility\n#View of the model\nex1_Model.1_classic\n```\n\n-   subset of data with `H == 'alternative'`\n\n```{r}\n#| message: false\n\n#Filter data with H == 'alternative'\nex1_sample.random_alternative <- filter(ex1_sample.random, H == 'alternative')\n#Fit the linear regression model for only data with H == 'alternative'\nex1_Model.1_alternative <- stan_glm(formula = P ~ T,\n                                    family = gaussian(),\n                                    data = ex1_sample.random_alternative,\n                                    #Prior coefficients\n                                    prior = normal(location = 0, scale = 5),\n                                    #Prior intercept\n                                    prior_intercept = normal(location = 56, scale = 10),\n                                    #Prior sigma\n                                    prior_aux = exponential(rate = 0.5),\n                                    iter = 4000, warmup = 1000, \n                                    save_warmup = TRUE,\n                                    chains = 4, cores = 4,\n                                    seed = 2025) #for reproducibility\n#View of the model\nex1_Model.1_alternative\n```\n\nWe can visualize the estimated coefficients for these new regression models and compared them with the one estimated from `Model.1` and `Model.2`.\n\n```{r}\n#| label: fig-DAG.ex1-estimate-Bayesian2\n#| fig-cap: 'Posterior distribution of the temperature coefficient for the four models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.'\n#| fig-cap-location: bottom\n#| fig-align: center\n#| fig-width: 6\n#| fig-height: 3\n#| fig-dpi: 900\n#| warning: false\n#| code-fold: true\n\n#Extract draws from model 1 \npost_ex1_Model.1_classic <-\n  ex1_Model.1_classic %>% \n  spread_draws(T) %>% #extract draws from the fitted model\n  mutate(model = 'Model.1_classic') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex1_Model.1_alternative <-\n  ex1_Model.1_alternative %>% \n  spread_draws(T) %>% #extract draws from the fitted model\n  mutate(model = 'Model.1_alternative') #add a new column to specify that the model\n\n#Combine draws\nplot.post1 <- rbind(post_ex1_Model.1, post_ex1_Model.1_classic, post_ex1_Model.1_alternative, post_ex1_Model.2)\n\n#Plot\nplot.post1  %>%\n  ggplot(aes(y = model, x = T)) +\n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.5, 1.5)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n```\n\n@fig-DAG.ex1-estimate-Bayesian2, we can see that only the estimated coefficients for `Model.1` are biased. As explained in the identification step, to identify the causal effect of indoor air temperature $(\\text{T})$ on productivity $(\\text{P})$, we need to block the backdoor path (i.e., the non-causal path) $\\text{T} \\leftarrow \\text{H} \\rightarrow \\text{P}$. Fitting a separate regression model to the subset of data with `H == 'classic'` and one to `H == 'alternative'` is equivalent to conditioning for $\\text{H}$, closing the backdoor path and allowing unbiased estimation of the causal effect of indoor air temperature $(\\text{T})$ on productivity $(\\text{P})$ (although this comes at the price of a larger standard error and consequently wider credible interval compared to `Model.2`, where $\\text{H}$ is added as a predictor).\n\n</details>\n\n## Session info {.unnumbered}\n\nVersion information about R, the OS and attached or loaded packages.\n\n```{r}\n\nsessionInfo()\n\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"embed-resources":false,"output-file":"ex1_simulation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.44","nocite":"[@R2025], [@Rstudio2025], [@ggdag2025], [@dagitty2025], [@rstanarm2025], [@tidybayes2025], [@ggdist2025a], [@ggdist2025b], [@tidyverse2025], [@gt2025], [@rstan2025], [@bayesboot2025], [@car2025]\n","bibliography":["references.bib"],"csl":"elsevier-harvard.csl","crossref":{"fig-title":"Fig.","fig-prefix":"Fig.","eq-prefix":"Eq.","title-delim":"."},"editor":"visual","number-depth":3,"theme":["cosmo"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}