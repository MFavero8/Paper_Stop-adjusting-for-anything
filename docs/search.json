[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary material",
    "section": "",
    "text": "Preface\nThis notebook is specifically designed to serve as a detailed guide for the examples with synthetic data that have been discussed in the paper titled “Stop adjusting for anything: How the backdoor and adjustment criteria empower causal human-centric research”. The primary objective of this notebook is to provide a comprehensive and practical explanation of the entire modelling process, along with detailed insights into the various methodologies and techniques employed in the process.\nOur aim is to provide a valuable resource for researchers and practitioners at all levels of proficiency —both experienced and inexperienced. By providing a clear and practical explanation of the modelling methods and techniques used in the examples, we hope to enable researchers and practitioners to gain a deeper understanding of the subject.\nHowever, it is important to note that this notebook is not intended and should not be used as a set of recipes to solve problems. Instead, it should be viewed as a map to guide researchers through the modelling process.\nContributors and license\nThis document was written by Matteo Favero and Jian Pan with feedback from Andrew Sonta, Ruiji Sun, and Marcel Schweiker.\nThis work is openly licensed via a Creative Commons Attribution 4.0 International (CC BY 4.0).\nFor attribution, please cite this work as (BibTeX citation):\n\n@article{PJ2025114002,\n  title = {Stop adjusting for anything: How the backdoor and adjustment criteria empower causal human-centric research},\n  author = {Pan, Jian and  Favero, Matteo and Sonta, Andrew and Sun, Ruiji and Damrath, Julia and Schweiker, Marcel},\n  journal = {Building and Environment},\n  pages = {114002},\n  year = {2025},\n  issn = {0360-1323},\n  doi = {https://doi.org/10.1016/j.buildenv.2025.114002}\n}\n\nThe corresponding primary article can be found at https://doi.org/10.1016/j.buildenv.2025.114002.\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Causal diagrams\nTo understand bias in causal research, it is important to recognize that biases are often spurious (non-causal) associations that arise from underlying causal structures. These structures can be formally represented as causal diagrams using Directed Acyclic Graphs (DAGs), which are visual models where variables are represented as nodes and causal relationships are depicted as arrows (edges) pointing from causes to their consequences. In a DAG, a causal path is a sequence of variables connected by directed edges (arrows) where all arrows point in the same direction, from the cause to the consequence. Conversely, a non-causal path contains at least one edge that points against the direction of the assumed causal flow. DAGs are acyclic, meaning they do not include feedback loops: no variable can causally influence itself, either directly or indirectly, within a single time point. DAGs are qualitative, nonparametric tools. They illustrate assumed causal relationships without specifying their strength or functional form. The arrows do not imply deterministic links, but rather the possibility of influence given appropriate conditions. Despite their simplicity, DAGs provide powerful insights into how statistical associations emerge—and how they can mislead us if misinterpreted. Biases can be characterized graphically based on the structural patterns in DAGs. There are four fundamental causal structures that serve as the basic building blocks for constructing more complex causal relationships in DAGs:\nCodelibrary(ggdag) # for DAG\nlibrary(dagitty)\nlibrary(ggplot2) # for visualisation\n\ndag_coords.intro &lt;-\n  data.frame(name = c('C', 'I', 'V', 'S', 'D', 'P'),\n             x = c(1, 1, 3.5, 6, 6, 3.5),\n             y = c(1, 2, 3, 1, 2, 2))\n\nDAG.intro &lt;-\n  dagify(I ~ C,\n         C ~ V,\n         P ~ V,\n         S ~ C + V,\n         D ~ S, \n         coords = dag_coords.intro)\n\nnode_labels &lt;- c(\n  C = 'bold(C)',\n  I = 'bold(I)',\n  V = 'bold(V)',\n  S = 'bold(S)',\n  D = 'bold(D)',\n  P = 'bold(P)'\n)\n\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono', \n                 fontface = 'bold') + \n  annotate('text', x = 3.5, y = 3.3, label = 'ventilation', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 2.3, label = 'IAQ index', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 2.3, label = 'daily productivity', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'power usage', \n             size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 0.7, label = 'CO2', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'sleep quality', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 1. Example of a graphical representation of a causal structure via a Directed Acyclic Graph.\nFor example, let us consider the hypothetical causal structure described in Fig. 1. Here, CO2 concentration \\((\\text{C})\\) affects sleep quality \\((\\text{S})\\), while ventilation \\((\\text{V})\\) reduces CO2 but also affects sleep quality \\((\\text{S})\\), for example, by increasing noise. Moreover, high ventilation increases power usage \\((\\text{P})\\), CO2 concentration influences the Indoor Air Quality (IAQ) index \\((\\text{I})\\), and low sleep quality leads to decreased daily productivity \\((\\text{D})\\). In the context of this example, ventilation can act as a confounder (common cause) of CO2 concentration and sleep quality (i.e., \\(\\text{C} \\leftarrow \\text{V} \\rightarrow \\text{S}\\)), while CO2 concentration can be seen as a mediator between ventilation and sleep quality (i.e., \\(\\text{V} \\rightarrow \\text{C} \\rightarrow \\text{S}\\)). Sleep quality can serve as a collider (common effect) between ventilation and CO2 concentration (i.e., \\(\\text{V} \\rightarrow \\text{S} \\leftarrow \\text{C}\\)), while power usage can function as a descendant of ventilation (i.e., \\(\\text{V} \\rightarrow \\text{P}\\)). To better understand the four fundamental causal structures, it is helpful to visualize them graphically. Importantly, these fundamental causal structures are not inherent properties of the variables—they depend entirely on the specific DAG and the causal question of interest.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#causal-diagrams",
    "href": "intro.html#causal-diagrams",
    "title": "Introduction",
    "section": "",
    "text": "Confounder (also called fork or common cause): A variable that causally affects (at least) two others.\nMediator (also called pipe or chain): A variable lying on the causal pathway from the cause to the consequence.\nCollider (also called common effect): A variable that is causally influenced by (at least) two others.\nDescendant: A variable that is causally influenced by another variable.\n\n\n\nConfounder and descendant\n\nCode#example a: C -&gt; S is open\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #backdoor path: common cause\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05) + #'#C77CFF'\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05) +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +  \n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n#example b: C -&gt; S is closed\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #backdoor path: common cause\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05) +\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05) +\n  #adjusted variable\n#  geom_point(x = 3.5, y = 3, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_point(x = 3.5, y = 3, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +  \n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n#example c: C -&gt; S is partially closed\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #backdoor path: common cause\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05, linetype = '12') +\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05, linetype = '12') +\n  #adjusted variable\n#  geom_point(x = 3.5, y = 2, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_point(x = 3.5, y = 2, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +  \n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 2. Illustrations of confounder and descendant. Green lines indicate the causal questions of interest; orange lines indicate open paths; light blue lines indicate closed paths; dashed orange and light blue lines indicate partially open and closed paths, respectively; a grey-filled box indicates statistical adjustment.\n\n\n\nIn Fig. 2, we are interested in the total average causal effect (green line) of CO2 concentration \\((\\text{C})\\) on sleep quality \\((\\text{S})\\). In this setting, ventilation \\((\\text{V})\\) is a confounder (i.e., common cause) of \\(\\text{C}\\) and \\(\\text{S}\\) and creates a non-causal path between \\(\\text{C}\\) and \\(\\text{S}\\) (i.e., \\(\\text{C} \\leftarrow \\text{V} \\rightarrow \\text{S}\\)). Leaving a confounder unadjusted keeps the non-causal path open (orange line in Fig. 2 (a)), introducing bias. Adjusting for a confounder (e.g., via including it as a predictor in a regression model) blocks this path. Fig. 2 (b) illustrates this, where the grey-filled box visually represents the adjustment, and the light blue line indicates the blocked path. Adjusting for the descendant of a confounder is illustrated in Fig. 2 (c). Generally, adjusting for a descendant has a similar effect as adjusting for its direct parent variable, but the impact depends on how well the descendant captures the parent’s influence. The effect will not be identical unless the descendant fully represents the parent (i.e., if the descendant is a perfect proxy for the parent). Assuming that power usage \\((\\text{P})\\) is not a perfect proxy for ventilation \\((\\text{V})\\), adjusting for \\(\\text{P}\\) (grey-filled box) will only partially close the path (dashed light blue line). As a result, bias will occur, but its absolute magnitude will be less than that with a fully open path.\nMediator and descendant\n\nCode#example a: V -&gt; S is open\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #mediation path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05) +\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05) +\n  #visualise causal effect path\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n#example b: V -&gt; S is closed\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #mediation path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05) +\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05) +\n  #adjusted variable\n#  geom_point(x = 1, y = 1, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_point(x = 1, y = 1, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  #visualise causal effect path\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n#example c: V -&gt; S is partially closed\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #mediation path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05, linetype = '12') +\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05, linetype = '12') +\n  #adjusted variable\n#  geom_point(x = 1, y = 2, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_point(x = 1, y = 2, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  #visualise causal effect path\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 3. Illustrations of mediator and descendant. Green lines indicate the causal questions of interest; orange lines indicate open paths; light blue lines indicate closed paths; dashed orange and light blue lines indicate partially open and closed paths, respectively; a grey-filled box indicates statistical adjustment.\n\n\n\nIn Fig. 3, we are interested in the direct average causal effect (green line) of ventilation \\((\\text{V})\\) on sleep quality \\((\\text{S})\\). In this setting, CO2 concentration \\((\\text{C})\\) is a mediator of \\(\\text{V}\\) and \\(\\text{S}\\) (i.e., \\(\\text{V} \\rightarrow \\text{C} \\rightarrow \\text{S}\\)). The path \\(\\text{V} \\rightarrow \\text{C} \\rightarrow \\text{S}\\) is a causal path, specifically the indirect causal effect of \\(\\text{V}\\) on \\(\\text{S}\\). However, it is not the causal effect of interest, since we are interested in the direct causal effect of \\(\\text{V}\\) on \\(\\text{S}\\). Leaving the mediator unadjusted keeps that causal path open (orange line in Fig. 3 (a)). Adjusting for a mediator blocks the causal pathway on which it lies. Fig. 3 (b) illustrates this, where the grey-filled box visually represents the adjustment, and the light blue line indicates the blocked path. Adjusting for the descendant of a mediator is illustrated in Fig. 3 (c). Assuming IAQ index \\((\\text{I})\\) is not a perfect proxy of CO2 concentration \\((\\text{C})\\), adjusting for \\(\\text{I}\\) (grey-filled box) will only partially close the path (dashed light blue line).\nCollider and descendant\n\nCode#example a: V -&gt; C is closed\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #backdoor path: common effect\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05) +\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05) +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n#example b: V -&gt; C is open\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #backdoor path: common effect\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05) +\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05) +\n  #adjusted variable\n#  geom_point(x = 6, y = 1, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_point(x = 6, y = 1, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n#example c: V -&gt; C is partially open\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #backdoor path: common effect\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05, linetype = '12') +\n  geom_segment(x = 3.5, xend = 6, y = 3, yend = 1,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05, linetype = '12') +\n  #adjusted variable\n#  geom_point(x = 6, y = 2, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_point(x = 6, y = 2, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 3.5, y = 1, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  \n  geom_dag_text(aes(label = node_labels[name]),\n                colour = 'black', size = 10, parse = TRUE,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = c('mono'), \n                 fontface = c('bold')) + \n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 4. Illustrations of collider and descendant. Green lines indicate the causal questions of interest; orange lines indicate open paths; light blue lines indicate closed paths; dashed orange and light blue lines indicate partially open and closed paths, respectively; a grey-filled box indicates statistical adjustment.\n\n\n\nIn Fig. 4, we are interested in the total average causal effect (green line) of ventilation \\((\\text{V})\\) on CO2 concentration \\((\\text{C})\\). In this setting, sleep quality \\((\\text{S})\\) is a collider (i.e., common effect) of \\(\\text{V}\\) and \\(\\text{C}\\) and creates a non-causal path between \\(\\text{V}\\) and \\(\\text{C}\\) (i.e., \\(\\text{V} \\rightarrow \\text{S} \\leftarrow \\text{C}\\)). Leaving a collider unadjusted keeps the non-causal path closed (light blue line in Fig. 4 (a)). Adjusting for a collider opens this path, introducing bias. Fig. 4 (b) illustrates this, where the grey-filled box visually represents the adjustment, and the orange line indicates the opened path. Adjusting for the descendant of a collider is illustrated in Fig. 4 (c). Assuming that daily productivity \\((\\text{D})\\) is not a perfect proxy of sleep quality \\((\\text{S})\\), adjusting for \\(\\text{D}\\) (grey-filled box) will only partially open the path (dashed orange line). As a result, bias will occur, but its absolute magnitude will be less than that with a fully open path.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#backdoor-criterion-and-adjustment-criterion",
    "href": "intro.html#backdoor-criterion-and-adjustment-criterion",
    "title": "Introduction",
    "section": "Backdoor criterion and adjustment criterion",
    "text": "Backdoor criterion and adjustment criterion\nA central question in causal inference is whether it is possible to isolate and identify the true causal effect. For this purpose, a simple yet powerful graphical tool exists: the backdoor criterion. While it may sound technical, its logic is intuitive and has become increasingly relevant in human-centric building science research.\nThe backdoor criterion is a test that can be applied directly to the DAG to determine whether a set(s) of variables exists that is sufficient to identify the total average causal effect of interest. This rule can be applied purely visually using DAGs without requiring any statistical equations. Two conditions must be met to satisfy the backdoor criterion, specifically:\n\nThe adjustment set (i.e., the group of variables that must be adjusted for to accurately estimate the causal effect of interest) must not include any descendant of the cause of interest;\nThe adjustment set must block every path between the cause and its consequence that starts with an arrow pointing into the cause. These paths are called backdoor paths.\n\nThe core idea of the backdoor criterion for identifying the total average causal effect of a treatment on the outcome is to block all backdoor paths between them, without adjusting for any descendant of the treatment. When these two conditions are satisfied, the total average causal effect is identifiable, that is, it is in principle estimable from the data. If these conditions are not met, under the given causal assumptions, the causal effect may not be identifiable. There are cases where a valid adjustment set exists (i.e., one can identify the causal effect by adjusting), but the backdoor criterion cannot find it. To address this issue, Shpitser et al. introduced the adjustment criterion, which generalized the backdoor criterion. Specifically, if a valid adjustment set exists, the adjustment criterion will identify at least one such set.\nThe adjustment criterion is also based on two conditions. Assuming that \\(\\text{X}\\) is the cause and \\(\\text{Y}\\) is the outcome of interest, the conditions are:\n\nThe adjustment set must exclude any descendant of \\(\\text{X}\\) that lies on a causal path from \\(\\text{X}\\) to \\(\\text{Y}\\);\nThe adjustment set must block all non-causal paths from \\(\\text{X}\\) to \\(\\text{Y}\\).\n\nThese conditions differ from those of the backdoor criterion. The first condition permits the inclusion of descendants of \\(\\text{X}\\), provided they are not on the causal path from \\(\\text{X}\\) to \\(\\text{Y}\\). The second condition requires closing all non-causal paths, not just the backdoor paths (since backdoor paths are non-causal paths, but not all non-causal paths are backdoor paths). If either of these conditions is violated, under the given causal assumptions, no valid adjustment set exists.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#causation-and-prediction",
    "href": "intro.html#causation-and-prediction",
    "title": "Introduction",
    "section": "Causation and prediction",
    "text": "Causation and prediction\n\nCodelibrary(ggdag) # for DAG\nlibrary(dagitty)\nlibrary(dplyr)\nlibrary(ggplot2) # for visualisation\nlibrary(ggforce)\n\ndag_coords.intro &lt;-\n  data.frame(name = c('X5', 'X1', 'X3', 'X7', 'Y', 'X2', 'X4', 'X8', 'X6'),\n             x = c(1, 3.5, 3.5, 3.5, 6, 8.5, 8.5, 8.5, 11),\n             y = c(2, 4, 3, 1, 2, 4, 3, 1, 2))\n\nDAG.intro &lt;-\n  dagify(X3 ~ X1,\n         X4 ~ X2,\n         Y ~ X3 + X4,\n         X7 ~ Y + X5,\n         X8 ~ Y + X6,\n         X9 ~ X7, \n         coords = dag_coords.intro)\n\nnode_labels &lt;- c(\n  X1 = 'bold(X[1])', \n  X2 = 'bold(X[2])', \n  X3 = 'bold(X[3])', \n  X4 = 'bold(X[4])', \n  X5 = 'bold(X[5])',\n  X6 = 'bold(X[6])', \n  X7 = 'bold(X[7])', \n  X8 = 'bold(X[8])', \n  Y = 'bold(Y)'\n)\n\nhighlight.nodes_cause &lt;- c('X3', 'X4', 'Y')\n\nhighlight.nodes_prediction &lt;- c('X3', 'X4', 'Y', 'X5', 'X7', 'X8', 'X6')\n\nhighlight.nodes_collider1 &lt;- c('Y', 'X5', 'X7')\n\nhighlight.nodes_collider2 &lt;- c('Y',  'X8', 'X6')\n\n\nggplot(data = DAG.intro, aes(x = x, y = y, xend = xend, yend = yend)) +\n geom_mark_hull(\n    data = dag_coords.intro %&gt;%\n      filter(name %in% highlight.nodes_cause) %&gt;%\n      { centroid &lt;- c(mean(.$x), mean(.$y))\n        mutate(., \n               x = x + 0.1 * (x - centroid[1]),\n               y = y + 0.1 * (y - centroid[2]))\n        },\n    #subset(dag_coords.intro, name %in% highlight.nodes_cause),\n    aes(x = x, y = y), inherit.aes = FALSE,\n    colour = alpha('#CC79A7', 0.5),\n    size = 2) +\n   geom_mark_hull(\n    data = dag_coords.intro %&gt;%\n      filter(name %in% highlight.nodes_prediction) %&gt;%\n      { centroid &lt;- c(mean(.$x), mean(.$y))\n        mutate(., \n               x = x + 0.05 * (x - centroid[1]),\n               y = y + 0.05 * (y - centroid[2]))\n        },\n    #subset(dag_coords.intro, name %in% highlight.nodes_prediction),\n    aes(x = x, y = y), inherit.aes = FALSE,\n    colour = alpha('#0072B2', 0.5),\n    size = 2) +\n     geom_mark_hull(\n    data = dag_coords.intro %&gt;%\n      filter(name %in% highlight.nodes_collider1) %&gt;%\n      { centroid &lt;- c(mean(.$x), mean(.$y))\n        mutate(., \n               x = x + 0.1 * (x - centroid[1]),\n               y = y + 0.1 * (y - centroid[2]))\n        },\n    #subset(dag_coords.intro, name %in% highlight.nodes_collider1),\n    aes(x = x, y = y), inherit.aes = FALSE,\n    colour = alpha('#E69F00', 0.5),\n    size = 2) +\n     geom_mark_hull(\n    data = dag_coords.intro %&gt;%\n      filter(name %in% highlight.nodes_collider2) %&gt;%\n      { centroid &lt;- c(mean(.$x), mean(.$y))\n        mutate(., \n               x = x + 0.1 * (x - centroid[1]),\n               y = y + 0.1 * (y - centroid[2]))\n        },\n    #subset(dag_coords.intro, name %in% highlight.nodes_collider2),\n    aes(x = x, y = y), inherit.aes = FALSE,\n    colour = alpha('#E69F00', 0.5), \n    size = 2) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  coord_cartesian(xlim = c(0.5, 11.5), ylim = c(0.8, 4.2)) +\n  theme_dag()\n\n\n\n\n\n\nFig. 5. Illustrations of causation and prediction for an outcome \\(\\text{Y}\\). The pink outline indicates the direct causes of \\(\\text{Y}\\); the blue outline indicates the Markov blanket for \\(\\text{Y}\\); the orange outlines indicate the colliders.\n\n\n\n\nAs previously mentioned, in a Directed Acyclic Graph (DAG), causation follows the the direction of the arrow but association does not. In Fig. 5, for example, \\(\\text{X}_1\\) and \\(\\text{X}_3\\) are associated because there is a line connecting them, but since the arrow goes from \\(\\text{X}_1\\) to \\(\\text{X}_3\\) (i.e., \\(\\text{X}_1 \\rightarrow \\text{X}_3\\)), the DAG assumes that \\(\\text{X}_1\\) is causing \\(\\text{X}_3\\): changing \\(\\text{X}_1\\) will change \\(\\text{X}_3\\), but changing \\(\\text{X}_3\\) will not affect \\(\\text{X}_1\\). This directionality is critical for causal inference, which seeks to answer “what-if” questions about the effect of actively intervening on a variable. Pure prediction, in contrast, relies on observed associations. To predict \\(\\text{X}_1\\), one could use \\(\\text{X}_3\\), even though \\(\\text{X}_3\\) is the consequence of \\(\\text{X}_1\\). If the association between \\(\\text{X}_1\\) and \\(\\text{X}_3\\) is positive, observing a high value for \\(\\text{X}_3\\) would imply observing a high value for \\(\\text{X}_1\\). The important point is that this implies observing \\(\\text{X}_1\\) and \\(\\text{X}_3\\) rather than directly changing them. Manually setting \\(\\text{X}_3\\) to a high value will not affect \\(\\text{X}_1\\).\nTo better illustrate the difference between causation and prediction, consider that in Fig. 5 the outcome of interest is \\(\\text{Y}\\). Here, \\(\\text{Y}\\) is directly causes by \\(\\text{X}_3\\) and \\(\\text{X}_4\\) and indirectly by \\(\\text{X}_1\\) (i.e., \\(\\text{X}_1 \\rightarrow \\text{X}_3 \\rightarrow \\text{Y}\\)) and \\(\\text{X}_2\\) (i.e., \\(\\text{X}_2 \\rightarrow \\text{X}_4 \\rightarrow \\text{Y}\\)). As such, all causal information for \\(\\text{Y}\\) is contained in \\(\\text{X}_3\\) and \\(\\text{X}_4\\) (the pink outline in Fig. 5). However, if the goal is to predict \\(\\text{Y}\\), a more accurate prediction can be obtained by including also \\(\\text{X}_5\\), \\(\\text{X}_6\\), \\(\\text{X}_7\\) and \\(\\text{X}_8\\) (the blue outline in Fig. 5). This is the case because, by blocking all paths leading to \\(\\text{Y}\\), these variables (i.e., \\(\\text{X}_3\\), \\(\\text{X}_4\\), \\(\\text{X}_5\\), \\(\\text{X}_6\\), \\(\\text{X}_7\\) and \\(\\text{X}_8\\)) encompass all the information available in the data about \\(\\text{Y}\\). The set of variables that contains all information about \\(\\text{Y}\\) (or more formally, the set of variables that renders \\(\\text{Y}\\) conditionally independent of all other variables in the DAG) is called the Markov blanket.\nA predictive model can leverage all the information contained in the Markov blanket to improve accuracy, even though some of the variables are not causal (i.e., \\(\\text{X}_5\\), \\(\\text{X}_6\\), \\(\\text{X}_7\\) and \\(\\text{X}_8\\)). A causal model should only include variables that are needed to identify the causal effect of interest. As explained previously, \\(\\text{X}_5 \\rightarrow \\text{X}_7 \\leftarrow \\text{Y}\\) and \\(\\text{Y} \\rightarrow \\text{X}_8 \\leftarrow \\text{X}_6\\) are colliders (the orange outline in Fig. 5). Therefore, including \\(\\text{X}_5\\), \\(\\text{X}_6\\), \\(\\text{X}_7\\) and \\(\\text{X}_8\\) will introduce bias.\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "ex1_simulation.html",
    "href": "ex1_simulation.html",
    "title": "1  Simulation example",
    "section": "",
    "text": "1.1 Data-generating process\nThe data-generating process is described via the directed acyclic graph (DAG) in Fig. 1.1. In this DAG, indoor air temperature \\((\\text{T})\\) influences productivity \\((\\text{P})\\) directly, while HVAC system \\((\\text{H})\\) influences productivity \\((\\text{P})\\) both directly (e.g., with air distribution and noise) and indirectly, passing through indoor air temperature \\((\\text{T})\\).\nCodedag_coords.ex1 &lt;-\n  data.frame(name = c('T', 'H', 'P'),\n             x = c(1, 3.5, 6),\n             y = c(1, 3, 1))\n\nDAG.ex1 &lt;-\n  dagify(T ~ H,\n         P ~ T + H,\n         coords = dag_coords.ex1)\n\nnode_labels &lt;- c(\n  T = 'bold(T)', \n  H = 'bold(H)', \n  P = 'bold(P)'\n)\n\n\nggplot(data = DAG.ex1, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 0.7, label = 'indoor air temperature', \n           size = 4, hjust = 0.4, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 3.3, label = 'HVAC system', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'productivity', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 1.1. Graphical representation via DAG of the data-generating process.\nThe DAG in Fig. 1.1 can be written as:",
    "crumbs": [
      "Simpson's paradox",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex1_simulation.html#data-generating-process",
    "href": "ex1_simulation.html#data-generating-process",
    "title": "1  Simulation example",
    "section": "",
    "text": "\\(T \\sim f_{T}(H)\\), read as ‘indoor air temperature \\((\\text{T})\\) is some function of HVAC system \\((\\text{H})\\)’.\n\n\\(P \\sim f_{P}(T, H)\\), read as ‘productivity \\((\\text{P})\\) is some function of indoor air temperature \\((\\text{T})\\) and HVAC system \\((\\text{H})\\)’.",
    "crumbs": [
      "Simpson's paradox",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex1_simulation.html#synthetic-data-set",
    "href": "ex1_simulation.html#synthetic-data-set",
    "title": "1  Simulation example",
    "section": "\n1.2 Synthetic data set",
    "text": "1.2 Synthetic data set\nTo generate synthetic data, we defined the custom function data.sim_ex1(). This function takes as inputs the sample size n and generates synthetic data according to the DAG in Fig. 1.1.\n\ndata.sim_ex1 &lt;- function(n) {\n  b_hvac.prod = c(0, 5) #direct causal effect of H on P\n  b_temp.prod = -1 #direct causal effect of T on P\n  #Simulate HVAC\n  H &lt;- factor(sample(c('classic', 'alternative'), size = n, replace = TRUE))  \n  #Simulate indoor air temperature\n  T &lt;- rnorm(n = n, mean = ifelse(H == 'classic', 21, 22), sd = 0.7)\n  #Simulate productivity\n  P &lt;- rnorm(n = n, mean = ifelse(H == 'classic', 75 + b_hvac.prod[1] + b_temp.prod * T, 75 + b_hvac.prod[2] + b_temp.prod * T), sd = 2)  \n  #Return tibble with simulated values\n  return(tibble(H, T, P))\n  }\n\nFrom this data generation mechanism, we simulated the target population, which consists of one million observations.\n\nset.seed(2025) #set random number for reproducibility\n#Simulate the population\nex1_population &lt;- data.sim_ex1(n = 1e6)\n#Set HVAC reference category to 'classic'\nex1_population$H &lt;- relevel(ex1_population$H, ref = 'classic')\n#View the data frame\nex1_population\n\n# A tibble: 1,000,000 × 3\n   H               T     P\n   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 classic      20.6  51.1\n 2 alternative  21.6  56.4\n 3 alternative  22.3  58.2\n 4 alternative  21.1  56.8\n 5 classic      20.6  55.7\n 6 classic      20.0  53.0\n 7 alternative  21.8  56.4\n 8 classic      21.7  53.9\n 9 alternative  21.5  55.3\n10 classic      20.5  57.4\n# ℹ 999,990 more rows\n\n\nFrom this population, we obtained one data set of five thousand observations using simple random sampling.\n\nn_sims &lt;- 1e3 #number of data sets to simulate\n\n#Set random number for reproducibility\nset.seed(2025)  \n#Generate a vector of random numbers for reproducibility\nex1_random.seed &lt;- sample(1:1e5, size = n_sims, replace = FALSE)\n\n\nset.seed(ex1_random.seed[1])\n#Sample one data set of 5,000 observations\nex1_sample.random &lt;- \n  ex1_population %&gt;% \n  slice_sample(n = 5e3) #take a simple random sample of size 5,000 \n\n#View the data frame\nex1_sample.random\n\n# A tibble: 5,000 × 3\n   H               T     P\n   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 alternative  22.6  54.3\n 2 classic      21.3  53.3\n 3 classic      21.5  53.7\n 4 alternative  22.5  58.2\n 5 classic      22.5  55.6\n 6 classic      21.3  52.1\n 7 alternative  21.7  60.3\n 8 classic      22.9  55.3\n 9 alternative  22.7  58.6\n10 classic      19.9  55.8\n# ℹ 4,990 more rows",
    "crumbs": [
      "Simpson's paradox",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex1_simulation.html#data-analysis",
    "href": "ex1_simulation.html#data-analysis",
    "title": "1  Simulation example",
    "section": "\n1.3 Data analysis",
    "text": "1.3 Data analysis\nIn this example, the target of our analysis is the total average causal effect, ACE (also known as total average treatment effect, ATE) of indoor air temperature \\((\\text{T})\\) on productivity \\((\\text{P})\\), which stands for the expected increase of \\(\\text{P}\\) in response to a unit increase in \\(\\text{T}\\) due to an intervention. The causal effect of interest is visualized in Fig. 1.2.\n\nCodeggplot(data = DAG.ex1, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 0.7, label = 'indoor air temperature', \n           size = 4, hjust = 0.4, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 3.3, label = 'HVAC system', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'productivity', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  #causal effect number\n  annotate('text', x = 3.5, y = 1.2, label = '-1', \n           size = 4.5, hjust = 0.5, colour = 'black', parse = TRUE) +\n\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 1.2. Graphical representation via DAG of the data-generating process. The green line indicates the causal question of interest, and the number on the path indicates the total average causal effect.\n\n\n\n\n\n1.3.1 Identification\nThe first step to answer the causal question of interest is identification. Identification answers a ‘theoretical’ question by determining whether a causal effect can, in principle, be estimated from observed data. The backdoor criterion and its generalization, the adjustment criterion, allow us to understand whether our causal effect of interest can be identified and, if so, which variables we should (or should not) statistically adjust for (i.e., the adjustment set) to estimate the causal effect from the data.\nGiven its simplicity, we will first apply the backdoor criterion to identify valid adjustment sets to estimate the causal effect of interest. If the backdoor criterion is not applicable, we will apply its generalization, the adjustment criterion.\nBackdoor criterion\nApplying the backdoor criterion revealed the existence of a backdoor path (i.e., a non-causal path) from indoor air temperature \\((\\text{T})\\) to productivity \\((\\text{P})\\). Specifically, the backdoor path is \\(\\text{T} \\leftarrow \\text{H} \\rightarrow \\text{P}\\). Since HVAC system \\((\\text{H})\\) is a common cause of \\(\\text{T}\\) and \\(\\text{P}\\), association can flow from \\(\\text{T}\\) to \\(\\text{P}\\) through \\(\\text{H}\\). As a result, there is confounding. To close this backdoor path we need to adjust for \\(\\text{H}\\).\nGiven the DAG in Fig. 1.2, we can use the adjustmentSets() function to identify the adjustment set algorithmically. It is essential to note that this function applies the adjustment criterion and not the backdoor criterion. As such, the adjustmentSets() function can find adjustment sets even when the backdoor criterion is not applicable.\n\nadjustmentSets(DAG.ex1,\n               exposure = 'T', #indoor air temperature\n               outcome = 'P', #productivity\n               type = 'all', \n               effect = 'total', \n               max.results = Inf)\n\n{ H }\n\n\nAs expected, the resulting adjustment set includes HVAC system \\((\\text{H})\\). Therefore, to get the correct estimate of the total average causal effect, we need to adjust for \\(\\text{H}\\); failing to do so will lead to bias.\n\n1.3.2 Estimation\nFollowing the identification step is the estimation step. This step addresses a statistical question by determining how the causal effect identified in the previous step can be estimated. To perform this step, we used a parametric (model-based) estimator, specifically, linear regression. This was possible because we designed the illustrative examples to be simple and with a linear relationship between the variables. This way, we limited the complexity of the examples themselves and shifted the focus to the application of the backdoor criterion to define ‘correct’ adjustment sets.\nFor transparency and understanding, all (implicit) assumptions used for this illustrative example are (explicitly) provided in Table 1.1.\n\n\n\nTable 1.1. Summary description of the simulation example\n\n\n\n\n\n\n\n\n\n\n\nResearch question\nTotal average causal effect (ACE) of indoor air temperature (T) on productivity (P).\n\n\nAssumptions\nRandom sample (simple random sampling): everyone in the population has an equal chance of being selected into the sample.\n\n\n\nLimited random variability: large sample size.\n\n\n\nIndependence of observations: each observation represents independent bits of information.\n\n\n\nNo confounding: the DAG includes all shared causes among the variables.\n\n\n\nNo model error: perfect functional form specification.\n\n\n\nNo measurement error: all variables are measured perfectly.\n\n\nVariables\nIndoor air temperature (T): continuous variable [unit: °C]\n\n\n\nHVAC (H): categorical variable ['classic'; 'alternative']\n\n\n\nProductivity (P): continuous variable [unit: -]\n\n\n\n\n\n\n\n\n\nTo carry out the estimation step, we utilized linear regression within both the frequentist and Bayesian frameworks. Specifically, we will run two regression models:\n\n\nModel.1 will include only indoor air temperature \\((\\text{T})\\) as predictor;\n\nModel.2 will include both indoor air temperature \\((\\text{T})\\) and HVAC system \\((\\text{H})\\) as predictors.\n\nThe results of the fitted statistical models (i.e., Model.1 and Model.2) are presented here.\nFrequentist framework\n\n#Fit the linear regression model with T and P (Model 1)\nex1_Model.1 &lt;-\n  lm(formula = P ~ T,\n     data = ex1_sample.random)\n#View of the model summary\nsummary(ex1_Model.1)\n\n\nCall:\nlm(formula = P ~ T, data = ex1_sample.random)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7246 -2.0653  0.0144  1.9700 10.0478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 40.56735    1.00976   40.17   &lt;2e-16 ***\nT            0.71541    0.04693   15.24   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.869 on 4998 degrees of freedom\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.04423 \nF-statistic: 232.4 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with T , H and P (Model 2)\nex1_Model.2 &lt;-\n  lm(formula = P ~ T + H,\n     data = ex1_sample.random)\n#View of the model summary\nsummary(ex1_Model.2)\n\n\nCall:\nlm(formula = P ~ T + H, data = ex1_sample.random)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6805 -1.3390 -0.0401  1.3199  8.8070 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  75.15431    0.84857   88.56   &lt;2e-16 ***\nT            -1.00927    0.04036  -25.00   &lt;2e-16 ***\nHalternative  5.08393    0.06979   72.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.998 on 4997 degrees of freedom\nMultiple R-squared:  0.5365,    Adjusted R-squared:  0.5364 \nF-statistic:  2892 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated coefficients for the two models are then plotted in Fig. 1.3.\n\nCodeb_temp.prod = -1 \n\ndata.frame(model = c('Model.1', 'Model.2'),\n           estimate = c(coef(ex1_Model.1)['T'], coef(ex1_Model.2)['T']),\n           lower.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 1], confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 1]),\n           upper.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 2], confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 2])) %&gt;%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.25, 1.25)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 1.3. Estimates of the temperature coefficient for the two models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nFig. 1.3 shows the estimates (point estimate and 95% confidence interval) of the coefficient for temperature for Model.1 and Model.2.\nFor Model.1 we found a positive coefficient between indoor air temperature \\((\\text{T})\\) and productivity \\((\\text{P})\\) equal to 0.715 with 95% confidence interval (CI) [0.623, 0.807]. Since the 95% CI does not include zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 2.56e-51). Additionally, since the 95% CI does not include the data-generating parameter for indoor air temperature (i.e., b_temp.prod = -1), we can deduce that the estimated coefficient for indoor air temperature is statistically significantly different from -1 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for b_temp.prod = -1\ncar::linearHypothesis(ex1_Model.1, 'T = -1') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nT = - 1\n\nModel 1: restricted model\nModel 2: P ~ T\n\n  Res.Df   RSS Df Sum of Sq    F    Pr(&gt;F)    \n1   4999 52123                                \n2   4998 41130  1     10994 1336 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 1.97e-259, which indicates that we can reject the null hypothesis (i.e., T = -1) at the 0.05 level. This result suggests that the regression coefficient for indoor air temperature (i.e., 0.715) is statistically significantly different from -1. Since the estimated causal effect from Model.1 is biased, we would erroneously conclude that an increase of 1°C in indoor air temperature causes an increase in productivity by 0.715 units (95% CI [0.623, 0.807]).\nFor Model.2 we found a negative coefficient between indoor air temperature \\((\\text{T})\\) and productivity \\((\\text{P})\\) equal to -1.009 with 95% CI [-1.088, -0.930]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 3.99e-130). Additionally, since the 95% CI includes the data-generating parameter for indoor air temperature (i.e., b_temp.prod = -1), we can deduce that the estimated coefficient for indoor air temperature is not statistically significantly different from -1 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for T = -1\ncar::linearHypothesis(ex1_Model.2, 'T = -1') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nT = - 1\n\nModel 1: restricted model\nModel 2: P ~ T + H\n\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1   4998 19948                           \n2   4997 19948  1   0.21046 0.0527 0.8184\n\n\nThe resulting p-value is 0.818, which indicates that we fail to reject the null hypothesis (i.e., T = -1) at the 0.05 level. This result suggests that the regression coefficient for indoor air temperature (i.e., -1.009) is not statistically significantly different from -1. Since the estimated causal effect from Model.2 is unbiased, we would correctly conclude that an increase of 1°C in indoor air temperature causes a decrease in productivity by -1.009 units (95% CI [-1.088, -0.930]).\nImportantly, for both Model.1 and Model.2, the 95% confidence interval means that if we were to repeat the sampling process and calculate the interval many times, 95% of those calculated intervals would contain the true population parameter. To highlight this, we can repeat the analysis by fitting the two models to one thousand data sets randomly selected from our population.\nThe for-loop shown in the code below performs the following operations. First, sample (using simple random sampling) a data set of 5,000 observations from the target population. Subsequently, perform linear regression using Model.1 and Model.2 and store the estimated coefficients for temperature, its standard error and 95% confidence interval in the data frame coefs_ex1. This operation is repeated a thousand times, resulting in the data frame coefs_ex1 containing the estimates (point estimate, standard error and confidence interval) of a thousand random samples of size 5,000 using both Model.1 and Model.2.\n\nn_model &lt;- c('mod.1', 'mod.2')     \n\nn_row &lt;- n_sims*length(n_model)\n\n#Create an empty data frame\nempty.df &lt;- data.frame(matrix(NA, nrow = n_row, ncol = 7))\n#Rename the data frame columns\ncolnames(empty.df) &lt;- c('sim.id', 'estimate', 'se', 'CI_2.5', 'CI_97.5',\n                        'model', 'coverage')\n\n#Sample a thousand data sets of 5,000 observations and perform linear regression \ncoefs_ex1 &lt;- empty.df #assign the empty data frame\nk = 1\nfor (i in 1:n_sims){\n  set.seed(ex1_random.seed[i]) #set unique seed for each simulation \n#Sample data set from population   \n  sample.random &lt;- \n    ex1_population %&gt;% \n    slice_sample(n = 5e3) #take a simple random sample of size 5,000\n#Fit models\n  for (j in 1:length(n_model)){\n    if (n_model[j] == 'mod.1'){\n      fit &lt;- lm(formula = P ~ T,\n                data = sample.random)\n    } else {\n      fit &lt;- lm(formula = P ~ T + H,\n                data = sample.random)\n    }  \n#Compile matrix  \n  coefs_ex1[k, 1] &lt;- i #simulation ID\n  coefs_ex1[k, 2] &lt;- coef(fit)['T'] #point estimate\n  coefs_ex1[k, 3] &lt;- summary(fit)$coef['T','Std. Error'] #standard error\n  coefs_ex1[k, 4:5] &lt;- confint(fit, level = 0.95, type = 'Wald')['T', ] #confidence interval (Wald)\n  coefs_ex1[k, 6] &lt;- n_model[j] #sample size\n  k = k + 1\n  }\n}\ncoefs_ex1 &lt;- as_tibble(coefs_ex1)\n#View the data frame\ncoefs_ex1\n\n# A tibble: 2,000 × 7\n   sim.id estimate     se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;   \n 1      1    0.715 0.0469  0.623   0.807 mod.1 NA      \n 2      1   -1.01  0.0404 -1.09   -0.930 mod.2 NA      \n 3      2    0.706 0.0477  0.613   0.800 mod.1 NA      \n 4      2   -0.959 0.0415 -1.04   -0.878 mod.2 NA      \n 5      3    0.673 0.0472  0.581   0.766 mod.1 NA      \n 6      3   -1.02  0.0402 -1.10   -0.946 mod.2 NA      \n 7      4    0.668 0.0468  0.576   0.759 mod.1 NA      \n 8      4   -1.02  0.0405 -1.10   -0.937 mod.2 NA      \n 9      5    0.717 0.0475  0.624   0.811 mod.1 NA      \n10      5   -1.03  0.0418 -1.11   -0.949 mod.2 NA      \n# ℹ 1,990 more rows\n\n\nThe coverage is defined by setting its value to 1 if the confidence interval overlaps the data-generating parameter for temperature (i.e., b_temp.prod = -1) and 0 otherwise.\n\n#Calculate coverage\ncoefs_ex1 &lt;-\n  coefs_ex1 %&gt;%\n  mutate(coverage = case_when(CI_2.5 &gt; b_temp.prod | CI_97.5 &lt; b_temp.prod ~ 0,\n                              CI_2.5 &lt;= b_temp.prod & CI_97.5 &gt;= b_temp.prod ~ 1,\n                              .default = NA))\n#View the data frame\ncoefs_ex1\n\n# A tibble: 2,000 × 7\n   sim.id estimate     se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1      1    0.715 0.0469  0.623   0.807 mod.1        0\n 2      1   -1.01  0.0404 -1.09   -0.930 mod.2        1\n 3      2    0.706 0.0477  0.613   0.800 mod.1        0\n 4      2   -0.959 0.0415 -1.04   -0.878 mod.2        1\n 5      3    0.673 0.0472  0.581   0.766 mod.1        0\n 6      3   -1.02  0.0402 -1.10   -0.946 mod.2        1\n 7      4    0.668 0.0468  0.576   0.759 mod.1        0\n 8      4   -1.02  0.0405 -1.10   -0.937 mod.2        1\n 9      5    0.717 0.0475  0.624   0.811 mod.1        0\n10      5   -1.03  0.0418 -1.11   -0.949 mod.2        1\n# ℹ 1,990 more rows\n\n\nThe results are then plotted in Fig. 1.4.\n\nCodemodel_names &lt;- c('mod.1' = 'Model.1', \n                 'mod.2' = 'Model.2')\n\nggplot(data = subset(coefs_ex1, sim.id &lt;= 2e2), aes(x = sim.id, y = estimate, ymin = CI_2.5, ymax = CI_97.5, colour = as.factor(coverage))) + \n  geom_hline(yintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 1, fill = 'white', stroke = 0.5) +\n  scale_colour_manual('', values = c('black', '#FF4040'),\n                      breaks = c('1','0')) +\n  scale_y_continuous('Estimate') +\n  scale_x_continuous('Simulation ID') +\n  facet_wrap(model~., \n             labeller = labeller(model = model_names),\n             nrow = 2,\n             scales = 'fixed') +\n  theme(legend.position = 'none', \n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 1.4. Estimates of the temperature coefficient (only the first hundreds of the thousand simulations are shown). The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. In red are highlighted the confidence intervals that do not overlap the parameter used to generate the data (dashed blue line).\n\n\n\n\nFig. 1.4 shows the first 200 estimates (point estimate and confidence interval) of the coefficient for indoor air temperature for Model.1 and Model.2. If all the thousand simulations are considered, the frequency of the coverage of the calculated confidence intervals (i.e., how many times the confidence intervals overlap the data-generating parameter) is 0.0% and 94.9% for Model.1 and Model.2, respectively. Since the estimates of the causal effect for Model.1 are biased, the calculated confidence intervals for this model do not have the expected coverage. In fact, confidence intervals only quantify the uncertainty due to random error (i.e., sample variability), not systematic error (i.e., bias). Instead, the estimates from Model.2 are unbiased and the calculated 95% confidence intervals have the expected coverage (i.e., overlap the data-generating parameter 95% of the times).\nWe can also visualize all the 1,000 estimates of the coefficient for indoor air temperature for Model.1 and Model.2 (the estimates were shown as white dots in Fig. 1.4).\n\nCodeggplot(data = coefs_ex1, aes(x = estimate, y = ifelse(after_stat(count) &gt; 0, after_stat(count), NA))) +\n  geom_histogram(binwidth = 0.05, fill = 'white', colour = 'grey50') +\n  geom_vline(xintercept = -1, colour = 'blue', linetype = 'dashed') +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.5, 1.5)) +\n  facet_grid(model~., \n             labeller = labeller(model = model_names),\n             scales = 'fixed') +\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 1.5. Histogram of the 1,000 estimates of the temperature coefficient for the two models. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nIn Fig. 1.5, the estimated coefficients are visualized through a histogram. Here, we can see that the estimates for Model.1 are not clustered around the data-generating parameter (blue dashed line) having a mean estimate of 0.687 (with 0.043 standard deviation). In contrast, the estimates for Model.2 having a mean estimate of -1.005 (with 0.041 standard deviation) are centered around the data-generating parameter.\nAs expected, the inclusion of HVAC system \\((\\text{H})\\) in the regression model (i.e., using Model.2) leads to the correct estimate of the total average causal effect, while its exclusion (i.e., using Model.1) leads to bias. Specifically, the estimated effect is reversed. This phenomenon is known as Simpson’s Paradox. Generally, Simpson’s Paradox happens when a trend that appears in different groups of data reverses (or disappears) when the groups are combined.\nTo better explain this apparent paradox, we can plot the data and the fitted regression lines for only the data with HVAC = classic, the data with HVAC = alternative, and the data combined (i.e., without considering the groups).\n\nCodepred_ex1_Model.1_separete &lt;-\n  rbind(cbind(filter(ex1_sample.random, H == 'classic'), \n              predict(lm(formula = P ~ T, data = filter(ex1_sample.random, H == 'classic')), \n                      interval = 'confidence', level = 0.95), pred = 'classic'), \n        cbind(filter(ex1_sample.random, H == 'alternative'), \n              predict(lm(formula = P ~ T, data = filter(ex1_sample.random, H == 'alternative')),\n                      interval = 'confidence', level = 0.95), pred = 'alternative'))\n\npred_ex1_Model.1 &lt;- \n  cbind(ex1_sample.random, \n      predict(ex1_Model.1, interval = 'confidence', level = 0.95))\n\n#Plot \nex1_sample.random %&gt;%\n  ggplot(aes(x = T, y = P)) +\n  geom_point(aes(colour = H), shape = 19, size = 2, alpha = 0.10, stroke = NA) +\n  geom_ribbon(data = pred_ex1_Model.1_separete, aes(x = T , y = fit, ymin = lwr, ymax = upr, fill = H), alpha = 0.2, show.legend = NA) +\n  geom_line(data = pred_ex1_Model.1_separete, aes(x = T , y = fit, colour = H)) + \n  geom_ribbon(data = pred_ex1_Model.1, aes(x = T , y = fit, ymin = lwr, ymax = upr, fill = 'all.data'), alpha = 0.2, show.legend = NA) +\n  geom_line(data = pred_ex1_Model.1, aes(x = T , y = fit, colour = 'all.data')) + \n  scale_colour_manual('',\n                      breaks = c('classic', 'alternative', 'all.data'),\n                      labels = c('classic', 'alternative', 'all data'),\n                      values = c('#D55E00','#0072B2', 'black')) +\n  scale_fill_manual('',\n                    breaks = c('classic', 'alternative', 'all.data'),\n                    labels = c('classic', 'alternative', 'all data'),\n                    values = c('#D55E00','#0072B2', 'black')) +\n  scale_x_continuous('Indoor air temperature') + \n  scale_y_continuous('Productivity') +\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + \n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 1.6. Visualization of the reversed regression line direction in the total sample compared to the subgroups.\n\n\n\n\nIn Fig. 1.6 the three colored lines represent the regression lines for only the data with HVAC = classic (in orange), the data with HVAC = alternative (in blue), and the combined data (in black). The regression lines in orange and blue are obtained by fitting the following regression models:\n\nsubset of data with H == 'classic'\n\n\n\n#Filter data with H == 'classic'\nex1_sample.random_classic &lt;- filter(ex1_sample.random, H == 'classic')\n#Fit the linear regression model for only data with H == 'classic'\nex1_Model.1_classic &lt;- lm(formula = P ~ T, data = ex1_sample.random_classic)\n#View of the model summary\nex1_Model.1_classic\n\n\nCall:\nlm(formula = P ~ T, data = ex1_sample.random_classic)\n\nCoefficients:\n(Intercept)            T  \n    74.1501      -0.9614  \n\n\n\nsubset of data with H == 'alternative'\n\n\n\n#Filter data with H == 'alternative'\nex1_sample.random_alternative &lt;- filter(ex1_sample.random, H == 'alternative')\n#Fit the linear regression model for only data with H == 'alternative'\nex1_Model.1_alternative &lt;- lm(formula = P ~ T, data = ex1_sample.random_alternative)      \n#View of the model summary\nex1_Model.1_alternative        \n\n\nCall:\nlm(formula = P ~ T, data = ex1_sample.random_alternative)\n\nCoefficients:\n(Intercept)            T  \n      81.36        -1.06  \n\n\nWe can visualize the estimated coefficients for these new regression models and compare them with the ones estimated from Model.1 and Model.2.\n\nCode#Plot\ndata.frame(model = c('Model.1', 'Model.1_classic', 'Model.1_alternative', 'Model.2'),\n           estimate = c(coef(ex1_Model.1)['T'], \n                        coef(ex1_Model.1_classic)['T'], \n                        coef(ex1_Model.1_alternative)['T'], \n                        coef(ex1_Model.2)['T']),\n           lower.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 1],\n                           confint(ex1_Model.1_classic, level = 0.95, type = 'Wald')['T', 1],\n                           confint(ex1_Model.1_alternative, level = 0.95, type = 'Wald')['T', 1],\n                           confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 1]),\n           upper.95.CI = c(confint(ex1_Model.1, level = 0.95, type = 'Wald')['T', 2], \n                           confint(ex1_Model.1_classic, level = 0.95, type = 'Wald')['T', 2], \n                           confint(ex1_Model.1_alternative, level = 0.95, type = 'Wald')['T', 2], \n                           confint(ex1_Model.2, level = 0.95, type = 'Wald')['T', 2])) %&gt;%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.25, 1.25)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 1.7. Estimates of the temperature coefficient for the four models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nFig. 1.7, we can see that only the estimated coefficient for Model.1 is biased. As explained in the identification step, to identify the causal effect of indoor air temperature \\((\\text{T})\\) on productivity \\((\\text{P})\\), we need to block the backdoor path (i.e., the non-causal path) \\(\\text{T} \\leftarrow \\text{H} \\rightarrow \\text{P}\\). Fitting a separate regression model to the subset of data with H == 'classic' and one to H == 'alternative' is equivalent to conditioning for \\(\\text{H}\\), closing the backdoor path and allowing unbiased estimation of the causal effect of indoor air temperature \\((\\text{T})\\) on productivity \\((\\text{P})\\) (although this comes at the price of a larger standard error and consequently wider confidence intervals compared to Model.2, where \\(\\text{H}\\) is added as a predictor).\nBayesian framework\n\nClick to expand\n\n#Fit the linear regression model with T and P (Model 1)\nex1_Model.1 &lt;- stan_glm(formula = P ~ T,\n                        family = gaussian(),\n                        data = ex1_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = 0, scale = 2),\n                        #Prior intercept\n                        prior_intercept = normal(location = 56, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility   \n#View of the model\nex1_Model.1\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ T\n observations: 5000\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 40.6    1.0  \nT            0.7    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.9    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with T , H and P (Model 2)\nex1_Model.2 &lt;- stan_glm(formula = P ~ T + H,\n                        family = gaussian(),\n                        data = ex1_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(2, 5)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 56, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex1_Model.2\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ T + H\n observations: 5000\n predictors:   3\n------\n             Median MAD_SD\n(Intercept)  75.1    0.9  \nT            -1.0    0.0  \nHalternative  5.1    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.0    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nIn Bayesian analysis, there are important diagnostics that have to be carried out in order to assess the convergence and efficiency of the Markov Chains. This is done by using the monitor() function which computes summaries of MCMC (Markov Chain Monte Carlo) draws and monitor convergence. Specifically, we will look at Rhat, Bulk_ESS and Tail_ESS metrics.\n\n#Diagnostics for model 1\nmonitor(ex1_Model.1$stanfit)  \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       39.0     40.6     42.2     40.6 1.0     1    11517     8203\nT                  0.6      0.7      0.8      0.7 0.0     1    11445     8323\nsigma              2.8      2.9      2.9      2.9 0.0     1    12774     8565\nmean_PPD          55.9     55.9     56.0     55.9 0.1     1    12512    11016\nlog-posterior -12372.1 -12369.4 -12368.4 -12369.7 1.2     1     5351     7267\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 2\nmonitor(ex1_Model.2$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       73.7     75.1     76.5     75.1 0.9     1     8648     8845\nT                 -1.1     -1.0     -0.9     -1.0 0.0     1     8495     8786\nHalternative       5.0      5.1      5.2      5.1 0.1     1     8517     8685\nsigma              2.0      2.0      2.0      2.0 0.0     1    12597     8666\nmean_PPD          55.9     55.9     56.0     55.9 0.0     1    11453    11001\nlog-posterior -10565.3 -10562.3 -10561.0 -10562.6 1.4     1     5905     7324\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\nRhat is a metric used to assess the convergence of Markov Chain Monte Carlo (MCMC) simulations. It helps determine if the MCMC chains have adequately explored the target posterior distribution. Specifically, it compares the between- and within-chain estimates for model parameters: If chains have not mixed well (i.e., the between- and within-chain estimates do not agree), R-hat is larger than 1. A general rule of thumb is to use the sample only if R-hat is less than 1.05; a larger value suggests that the chains have not mixed well, and the results might not be reliable. In our two models, all Rhat are equal to 1 indicating that the chains have mixed well and have adequately explored the target posterior distribution.\nBulk_ESS and Tail_ESS stand for ‘Bulk Effective Sample Size’ and ‘Tail Effective Sample Size,’ respectively. Since MCMC samples are not truly independent (they are correlated), these metrics assess the sampling efficiencies, that is, they help evaluate how efficiently the MCMC sampler is exploring the parameter space.\n\n\nBulk_ESS is a useful measure for sampling efficiency in the bulk (center) of the distribution (e.g., efficiency of mean and median estimates);\n\nTail_ESS is a useful measure for sampling efficiency in the tails of the distribution (e.g., efficiency of variance and tail quantile estimates). A general rule of thumb is that both Bulk-ESS and Tail-ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable. In our two models, all Bulk-ESS and Tail-ESS are well above 400 (i.e., 100 multiplied by 4, the number of chains we used) indicating that estimates of posterior quantiles are reliable.\n\nSince we have established that the posteriors are reliable, we can now explore the model estimates.\nThe estimated coefficients for the two models are then plotted in Fig. 1.8.\n\nCode#Extract draws from model 1 \npost_ex1_Model.1 &lt;-\n  ex1_Model.1 %&gt;% \n  spread_draws(T) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex1_Model.2 &lt;-\n  ex1_Model.2 %&gt;% \n  spread_draws(T) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.2') #add a new column to specify that the model\n\n#Combine draws\nplot.post &lt;- rbind(post_ex1_Model.1, post_ex1_Model.2)\n\n# Plot\nplot.post  %&gt;%\n  ggplot(aes(y = model, x = T)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.25, 1.25)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 1.8. Posterior distribution of the temperature coefficient for the two models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.\n\n\n\n\nFig. 1.8 shows the estimates (mean and 95% HDI) of the coefficient for temperature for Model.1 and Model.2.\nFor Model.1 we found a positive coefficient between indoor air temperature \\((\\text{T})\\) and productivity \\((\\text{P})\\) (mean = 0.714, 95% HDI [0.624, 0.803]). The estimated causal effect is therefore biased, leading to the wrong conclusion that an increase of 1°C in indoor air temperature causes an increase in productivity by 0.714 units, on average. In contrast, for Model.2 we found a negative coefficient between indoor air temperature \\((\\text{T})\\) and productivity \\((\\text{P})\\) (mean = -1.008, 95% HDI [-1.087, -0.929]). The estimated causal effect is therefore unbiased, leading to the correct conclusion that an increase of 1°C in indoor air temperature causes a decrease in productivity by -1.008 units, on average.\nImportantly, for both Model.1 and Model.2, the 95% HDI is the range of parameter values within which the most credible 95% of the posterior distribution falls. Unlike a frequentist confidence interval, the Bayesian 95% HDI has a direct probabilistic meaning: every point inside the HDI has a higher probability density than any point outside the interval. Therefore, given the model, the prior and the data, we can say that there is a 95% probability that the data-generating parameter (i.e., b_temp.prod = -1) lies within the HDI. However, since Model.1 leads to a biased estimate, we will reach the wrong conclusion by stating that there is a 95% probability that the data-generating parameter lies within the [0.624, 0.803] interval. This probability is 0%.\nAs expected, the inclusion of HVAC system \\((\\text{H})\\) in the regression model (i.e., using Model.2) leads to the correct estimate of the total average causal effect, while its exclusion (i.e., using Model.1) leads to bias. Specifically, the estimated effect is reversed. This phenomenon is known as Simpson’s Paradox. Generally, Simpson’s Paradox happens when a trend that appears in different groups of data reverses (or disappears) when the groups are combined.\nTo better explain this apparent paradox, we can plot the data and the fitted regression lines for only the data with HVAC = classic, the data with HVAC = alternative, and the data combined (i.e., without considering the groups).\n\nCode#Expected value of the posterior predictive distribution (epred) for model 1\nepred_ex1_Model.1 &lt;- \n  ex1_Model.1 %&gt;%\n  epred_draws(newdata = ex1_sample.random, \n              seed = 2025) %&gt;%\n  group_by(T) %&gt;%\n  mean_hdi(.epred, .width = .95) %&gt;%\n  ungroup()\n\n#Expected value of the posterior predictive distribution (epred) for model 2\nepred_ex1_Model.2 &lt;- \n  ex1_Model.2 %&gt;%\n  epred_draws(newdata = ex1_sample.random, \n              seed = 2025) %&gt;%\n  group_by(T, H) %&gt;%\n  mean_hdi(.epred, .width = .95) %&gt;%\n  ungroup()\n\n#Plot \nex1_sample.random %&gt;%\n  ggplot(aes(x = T, y = P)) +\n  geom_point(aes(colour = H), shape = 19, size = 2, alpha = 0.15, stroke = NA) +\n  \n  geom_ribbon(data = epred_ex1_Model.2, aes(x = T , y = .epred, ymin = .lower, ymax = .upper, fill = H), alpha = 0.2, show.legend = NA) +\n  geom_line(data = epred_ex1_Model.2, aes(x = T , y = .epred, colour = H)) + \n  \n  geom_ribbon(data = epred_ex1_Model.1, aes(x = T , y = .epred, ymin = .lower, ymax = .upper, fill = 'all.data'), alpha = 0.2, show.legend = NA) +\n  geom_line(data = epred_ex1_Model.1, aes(x = T , y = .epred, colour = 'all.data')) + \n  scale_colour_manual('',\n                      breaks = c('classic', 'alternative', 'all.data'),\n                      labels = c('classic', 'alternative', 'all data'),\n                      values = c('#D55E00','#0072B2', 'black')) +\n  scale_fill_manual('',\n                    breaks = c('classic', 'alternative', 'all.data'),\n                    labels = c('classic', 'alternative', 'all data'),\n                    values = c('#D55E00','#0072B2', 'black')) +\n  scale_x_continuous('Indoor air temperature') + \n  scale_y_continuous('Productivity') +\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + \n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 1.9. Visualization of the reversed regression line direction in the total sample compared to the subgroups.\n\n\n\n\nIn Fig. 1.9 the three colored lines represent the regression lines for only the data with HVAC = classic (in orange), the data with HVAC = alternative (in blue), and the combined data (in black). The regression lines in orange and blue are obtained by fitting the following regression models:\n\nsubset of data with H == 'classic'\n\n\n\n#Filter data with H == 'classic'\nex1_sample.random_classic &lt;- filter(ex1_sample.random, H == 'classic')\n#Fit the linear regression model for only data with H == 'classic'\nex1_Model.1_classic &lt;- stan_glm(formula = P ~ T,\n                                    family = gaussian(),\n                                    data = ex1_sample.random_classic,\n                                    #Prior coefficients\n                                    prior = normal(location = 0, scale = 5),\n                                    #Prior intercept\n                                    prior_intercept = normal(location = 56, scale = 10),\n                                    #Prior sigma\n                                    prior_aux = exponential(rate = 0.5),\n                                    iter = 4000, warmup = 1000, \n                                    save_warmup = TRUE,\n                                    chains = 4, cores = 4,\n                                    seed = 2025) #for reproducibility\n#View of the model\nex1_Model.1_classic\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ T\n observations: 2551\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 74.1    1.2  \nT           -1.0    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.0    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsubset of data with H == 'alternative'\n\n\n\n#Filter data with H == 'alternative'\nex1_sample.random_alternative &lt;- filter(ex1_sample.random, H == 'alternative')\n#Fit the linear regression model for only data with H == 'alternative'\nex1_Model.1_alternative &lt;- stan_glm(formula = P ~ T,\n                                    family = gaussian(),\n                                    data = ex1_sample.random_alternative,\n                                    #Prior coefficients\n                                    prior = normal(location = 0, scale = 5),\n                                    #Prior intercept\n                                    prior_intercept = normal(location = 56, scale = 10),\n                                    #Prior sigma\n                                    prior_aux = exponential(rate = 0.5),\n                                    iter = 4000, warmup = 1000, \n                                    save_warmup = TRUE,\n                                    chains = 4, cores = 4,\n                                    seed = 2025) #for reproducibility\n#View of the model\nex1_Model.1_alternative\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ T\n observations: 2449\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 81.4    1.3  \nT           -1.1    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.0    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWe can visualize the estimated coefficients for these new regression models and compared them with the one estimated from Model.1 and Model.2.\n\nCode#Extract draws from model 1 \npost_ex1_Model.1_classic &lt;-\n  ex1_Model.1_classic %&gt;% \n  spread_draws(T) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1_classic') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex1_Model.1_alternative &lt;-\n  ex1_Model.1_alternative %&gt;% \n  spread_draws(T) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1_alternative') #add a new column to specify that the model\n\n#Combine draws\nplot.post1 &lt;- rbind(post_ex1_Model.1, post_ex1_Model.1_classic, post_ex1_Model.1_alternative, post_ex1_Model.2)\n\n#Plot\nplot.post1  %&gt;%\n  ggplot(aes(y = model, x = T)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = b_temp.prod, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(-1.5, 1.5)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 1.10. Posterior distribution of the temperature coefficient for the four models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.\n\n\n\n\nFig. 1.10, we can see that only the estimated coefficient for Model.1 is biased. As explained in the identification step, to identify the causal effect of indoor air temperature \\((\\text{T})\\) on productivity \\((\\text{P})\\), we need to block the backdoor path (i.e., the non-causal path) \\(\\text{T} \\leftarrow \\text{H} \\rightarrow \\text{P}\\). Fitting a separate regression model to the subset of data with H == 'classic' and one to H == 'alternative' is equivalent to conditioning for \\(\\text{H}\\), closing the backdoor path and allowing unbiased estimation of the causal effect of indoor air temperature \\((\\text{T})\\) on productivity \\((\\text{P})\\) (although this comes at the price of a larger standard error and consequently wider credible interval compared to Model.2, where \\(\\text{H}\\) is added as a predictor).",
    "crumbs": [
      "Simpson's paradox",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex1_simulation.html#session-info",
    "href": "ex1_simulation.html#session-info",
    "title": "1  Simulation example",
    "section": "Session info",
    "text": "Session info\nVersion information about R, the OS and attached or loaded packages.\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 26200)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rstan_2.32.7        StanHeaders_2.32.10 gt_1.1.0           \n [4] lubridate_1.9.4     forcats_1.0.1       stringr_1.6.0      \n [7] dplyr_1.1.4         purrr_1.2.0         readr_2.1.6        \n[10] tidyr_1.3.1         tibble_3.3.0        ggplot2_4.0.1      \n[13] tidyverse_2.0.0     ggdist_3.3.3        tidybayes_3.0.7    \n[16] rstanarm_2.32.2     Rcpp_1.1.0          dagitty_0.3-4      \n[19] ggdag_0.2.13       \n\nloaded via a namespace (and not attached):\n  [1] backports_1.5.0       plyr_1.8.9            igraph_2.2.1         \n  [4] splines_4.2.3         svUnit_1.0.8          crosstalk_1.2.2      \n  [7] rstantools_2.5.0      inline_0.3.21         digest_0.6.38        \n [10] htmltools_0.5.8.1     viridis_0.6.5         magrittr_2.0.4       \n [13] checkmate_2.3.3       memoise_2.0.1         tzdb_0.5.0           \n [16] graphlayouts_1.2.2    RcppParallel_5.1.11-1 matrixStats_1.5.0    \n [19] xts_0.14.1            timechange_0.3.0      colorspace_2.1-2     \n [22] ggrepel_0.9.6         rbibutils_2.4         xfun_0.54            \n [25] jsonlite_2.0.0        lme4_1.1-37           survival_3.8-3       \n [28] zoo_1.8-14            glue_1.8.0            reformulas_0.4.2     \n [31] polyclip_1.10-7       gtable_0.3.6          V8_8.0.1             \n [34] distributional_0.5.0  car_3.1-3             pkgbuild_1.4.8       \n [37] abind_1.4-8           scales_1.4.0          miniUI_0.1.2         \n [40] viridisLite_0.4.2     xtable_1.8-4          Formula_1.2-5        \n [43] stats4_4.2.3          DT_0.34.0             htmlwidgets_1.6.4    \n [46] threejs_0.3.4         arrayhelpers_1.1-0    RColorBrewer_1.1-3   \n [49] posterior_1.6.1       pkgconfig_2.0.3       loo_2.8.0            \n [52] farver_2.1.2          sass_0.4.10           utf8_1.2.6           \n [55] tidyselect_1.2.1      labeling_0.4.3        rlang_1.1.6          \n [58] reshape2_1.4.5        later_1.4.4           tools_4.2.3          \n [61] cachem_1.1.0          cli_3.6.5             generics_0.1.4       \n [64] evaluate_1.0.5        fastmap_1.2.0         yaml_2.3.10          \n [67] knitr_1.50            fs_1.6.6              tidygraph_1.3.1      \n [70] ggraph_2.2.2          nlme_3.1-168          mime_0.13            \n [73] xml2_1.5.0            compiler_4.2.3        bayesplot_1.14.0     \n [76] shinythemes_1.2.0     rstudioapi_0.17.1     curl_7.0.0           \n [79] tweenr_2.0.3          stringi_1.8.7         lattice_0.22-7       \n [82] Matrix_1.6-5          nloptr_2.2.1          markdown_2.0         \n [85] shinyjs_2.1.0         tensorA_0.36.2.1      vctrs_0.6.5          \n [88] pillar_1.11.1         lifecycle_1.0.4       Rdpack_2.6.4         \n [91] httpuv_1.6.16         QuickJSR_1.8.1        R6_2.6.1             \n [94] promises_1.5.0        gridExtra_2.3         codetools_0.2-20     \n [97] boot_1.3-32           colourpicker_1.3.0    MASS_7.3-58.2        \n[100] gtools_3.9.5          withr_3.0.2           shinystan_2.6.0      \n[103] parallel_4.2.3        hms_1.1.4             grid_4.2.3           \n[106] coda_0.19-4.1         minqa_1.2.8           rmarkdown_2.30       \n[109] S7_0.2.1              carData_3.0-5         otel_0.2.0           \n[112] ggforce_0.5.0         shiny_1.11.1          base64enc_0.1-3      \n[115] dygraphs_1.1.1.6     \n\n\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Simpson's paradox",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex2_simulation.html",
    "href": "ex2_simulation.html",
    "title": "2  Simulation example",
    "section": "",
    "text": "2.1 Data-generating process\nThe data-generating process is described via the directed acyclic graph (DAG) in Fig. 2.1. In this DAG, glare perception \\((\\text{G})\\) is directly influenced by social interaction \\((\\text{I})\\) and window state \\((\\text{W})\\), and indirectly by occupancy \\((\\text{O})\\) through social interaction \\((\\text{I})\\). Additionally, occupancy \\((\\text{O})\\) and window state \\((\\text{W})\\) directly influence CO2 concentration \\((\\text{C})\\).\nCodedag_coords.ex2 &lt;-\n  data.frame(name = c('I', 'O', 'C', 'G', 'W'),\n             x = c(1, 1, 3.5, 6, 6),\n             y = c(1, 3, 2, 1, 3))\n\nDAG.ex2 &lt;-\n  dagify(I ~ O,\n         C ~ O,\n         C ~ W,\n         G ~ W,\n         G ~ I + W,\n         coords = dag_coords.ex2)\n\nnode_labels &lt;- c(\n  I = 'bold(I)', \n  O = 'bold(O)', \n  C = 'bold(C)', \n  G = 'bold(G)', \n  W = 'bold(W)'\n)\n\nggplot(data = DAG.ex2, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 0.7, label = 'social interaction', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'CO2', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 3.3, label = 'occupancy', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n    annotate('text', x = 6, y = 3.3, label = 'window state', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n    annotate('text', x = 6, y = 0.7, label = 'glare perception', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 2.1. Graphical representation via DAG of the data-generating process.\nThe DAG in Fig. 2.1 can be written as:",
    "crumbs": [
      "M-bias",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex2_simulation.html#data-generating-process",
    "href": "ex2_simulation.html#data-generating-process",
    "title": "2  Simulation example",
    "section": "",
    "text": "\\(I \\sim f_{I}(O)\\), read as ‘social interaction \\((\\text{I})\\) is some function of occupancy \\((\\text{O})\\)’.\n\n\\(C \\sim f_{C}(O, W)\\), read as ‘CO2 concentration \\((\\text{C})\\) is some function of occupancy \\((\\text{O})\\) and window \\((\\text{W})\\)’.\n\n\\(G \\sim f_{G}(I, W)\\), read as ‘glare perception \\((\\text{G})\\) is some function of social interaction \\((\\text{I})\\) and window \\((\\text{W})\\)’.",
    "crumbs": [
      "M-bias",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex2_simulation.html#synthetic-data-set",
    "href": "ex2_simulation.html#synthetic-data-set",
    "title": "2  Simulation example",
    "section": "\n2.2 Synthetic data set",
    "text": "2.2 Synthetic data set\nTo generate synthetic data, we defined the custom function data.sim_ex2(). This function takes as inputs the sample size n and generates synthetic data according to the DAG in Fig. 2.1.\n\ndata.sim_ex2 &lt;- function(n) {\n  b_socint.glare = -2 #direct causal effect of I on G\n  b_win.glare = c(0, 20) #direct causal effect of W on G\n  #Simulate occupancy\n  O &lt;- factor(sample(c('low', 'high'), size = n, replace = TRUE))  \n  #Simulate window state\n  W &lt;- factor(sample(c('closed', 'open'), size = n, replace = TRUE))  \n  #Simulate CO2\n  C &lt;- rnorm(n = n, mean = 400 + ifelse(O == 'low', 0, 50) + ifelse(W == 'closed', 0, 30), sd = 10)\n  #Simulate social interaction\n  I &lt;- rpois(n = n, lambda = 7 + ifelse(O == 'low', 0, 3))  \n  #Simulate glare perception \n  G &lt;- rnorm(n = n, mean = 50 + b_socint.glare * I + ifelse(W == 'closed', b_win.glare[1], b_win.glare[2]), sd = 5)\n  #Return tibble with simulated values\n  return(tibble(O, I, C, W, G))\n  }\n\nFrom this data generation mechanism, we simulated the target population, which consists of one million observations.\n\nset.seed(2025) #set random number for reproducibility\n#Simulate the population\nex2_population &lt;- data.sim_ex2(n = 1e6)\n#Set occupancy reference category to 'low'\nex2_population$O &lt;- relevel(ex2_population$O, ref = 'low')\n#Set window state reference category to 'closed'\nex2_population$W &lt;- relevel(ex2_population$W, ref = 'closed')\n#View the data frame\nex2_population\n\n# A tibble: 1,000,000 × 5\n   O         I     C W          G\n   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 low       5  422. closed  41.3\n 2 high      6  474. closed  42.5\n 3 high     13  473. open    35.9\n 4 high     11  461. open    42.1\n 5 low       2  391. closed  43.8\n 6 low      12  404. closed  18.4\n 7 high     16  452. closed  24.0\n 8 low      10  416. open    54.2\n 9 high     15  496. open    33.4\n10 low       5  415. open    50.4\n# ℹ 999,990 more rows\n\n\nFrom this population, we obtained one data set of five thousand observations using simple random sampling.\n\nn_sims &lt;- 1e3 #number of data sets to simulate\n\n#Set random number for reproducibility\nset.seed(2025)  \n#Generate a vector of random numbers for reproducibility\nex2_random.seed &lt;- sample(1:1e5, size = n_sims, replace = FALSE)\n\n\nset.seed(ex2_random.seed[1])\n#Sample one data set of 5,000 observations\nex2_sample.random &lt;- \n  ex2_population %&gt;% \n  slice_sample(n = 5e3) #take a simple random sample of size 5,000 \n\n#View the data frame\nex2_sample.random\n\n# A tibble: 5,000 × 5\n   O         I     C W          G\n   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 high     11  463. closed  32.9\n 2 low      11  404. closed  27.3\n 3 low       2  393. closed  40.5\n 4 high     10  481. open    45.5\n 5 low       7  398. closed  38.2\n 6 low       6  422. open    62.4\n 7 high     11  472. open    47.9\n 8 low       5  424. open    58.3\n 9 high     13  465. open    47.2\n10 low       6  417. open    64.5\n# ℹ 4,990 more rows",
    "crumbs": [
      "M-bias",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex2_simulation.html#data-analysis",
    "href": "ex2_simulation.html#data-analysis",
    "title": "2  Simulation example",
    "section": "\n2.3 Data analysis",
    "text": "2.3 Data analysis\nIn this example, the target of our analysis is the total average causal effect, ACE (also known as total average treatment effect, ATE) of social interaction \\((\\text{I})\\) on glare perception \\((\\text{G})\\), which stands for the expected increase of \\(\\text{G}\\) in response to a unit increase in \\(\\text{I}\\) due to an intervention. The causal effect of interest is visualized in Fig. 2.2.\n\nCodeggplot(data = DAG.ex2, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualise causal effect path\n  geom_segment(x = 1, xend = 6, y = 1, yend = 1,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\ngeom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') + \n  annotate('text', x = 1, y = 0.7, label = 'social interaction', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'CO2', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 3.3, label = 'occupancy', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 3.3, label = 'window state', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'glare perception', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  #causal effect number\n  annotate('text', x = 3.5, y = 1.2, label = '-2', \n           size = 4.5, hjust = 0.5, colour = 'black', parse = TRUE) +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 2.2. Graphical representation via DAG of the data-generating process. The green line indicates the causal question of interest, and the number on the path indicates the total average causal effect.\n\n\n\n\n\n2.3.1 Identification\nThe first step to answer the causal question of interest is identification. Identification answers a ‘theoretical’ question by determining whether a causal effect can, in principle, be estimated from observed data. The backdoor criterion and its generalization, the adjustment criterion, allow us to understand whether our causal effect of interest can be identified and, if so, which variables we should (or should not) statistically adjust for (i.e., the adjustment set) to estimate the causal effect from the data.\nGiven its simplicity, we will first apply the backdoor criterion to identify valid adjustment sets to estimate the causal effect of interest. If the backdoor criterion is not applicable, we will apply its generalization, the adjustment criterion.\nBackdoor criterion\nApplying the backdoor criterion revealed the absence of any backdoor path (i.e., a non-causal path) from social interaction \\((\\text{I})\\) to glare perception \\((\\text{G})\\). As a result, there is no confounding and no adjustment is required.\nGiven the DAG in Fig. 2.2, we can use the adjustmentSets() function to identify the adjustment set algorithmically. It is essential to note that this function applies the adjustment criterion and not the backdoor criterion. As such, the adjustmentSets() function can find adjustment sets even when the backdoor criterion is not applicable.\n\nadjustmentSets(DAG.ex2,\n               exposure = 'I', #social interaction\n               outcome = 'G', #glare perception\n               type = 'all', \n               effect = 'total', \n               max.results = Inf)\n\n {}\n{ O }\n{ C, O }\n{ W }\n{ C, W }\n{ O, W }\n{ C, O, W }\n\n\nAs expected, the resulting adjustment set includes the empty set (i.e., no adjustment is required). However, the backdoor criterion reveals that there are six other possible adjustment sets to get the correct estimate of the total average causal effect of \\(\\text{I}\\) on \\(\\text{G}\\). Specifically:\n\nadjust for occupancy \\((\\text{O})\\)\n\nadjust for CO2 concentration \\((\\text{C})\\) and occupancy \\((\\text{O})\\)\n\nadjust for window state \\((\\text{W})\\)\n\nadjust for CO2 concentration \\((\\text{C})\\) and window state \\((\\text{W})\\)\n\nadjust for occupancy \\((\\text{O})\\) and window state \\((\\text{W})\\)\n\nadjust for CO2 concentration \\((\\text{C})\\), occupancy \\((\\text{O})\\) and window state \\((\\text{W})\\)\n\n\nTherefore, to get the correct estimate of the total average causal effect, any of the aforementioned adjustment sets will work; failing to do so will lead to bias.\n\n2.3.2 Estimation\nFollowing the identification step is the estimation step. This step addresses a statistical question by determining how the causal effect identified in the previous step can be estimated. To perform this step, we used a parametric (model-based) estimator, specifically, linear regression. This was possible because we designed the illustrative examples to be simple and with a linear relationship between the variables. This way, we limited the complexity of the examples themselves and shifted the focus to the application of the backdoor criterion to define ‘correct’ adjustment sets.\nFor transparency and understanding, all (implicit) assumptions used for this illustrative example are (explicitly) provided in Table 2.1.\n\n\n\nTable 2.1. Summary description of the simulation example\n\n\n\n\n\n\n\n\n\n\n\nResearch question\nTotal average causal effect (ACE) of social interaction (I) on glare perception (G).\n\n\nAssumptions\nRandom sample (simple random sampling): everyone in the population has an equal chance of being selected into the sample.\n\n\n\nLimited random variability: large sample size.\n\n\n\nIndependence of observations: each observation represents independent bits of information.\n\n\n\nNo confounding: the DAG includes all shared causes among the variables.\n\n\n\nNo model error: perfect functional form specification.\n\n\n\nNo measurement error: all variables are measured perfectly.\n\n\nVariables\nSocial interaction (I): discrete (count) variable [unit: -]\n\n\n\nOccupancy (O): categorical variable ['low'; 'high']\n\n\n\nCO2 concentration (C): continuous variable [unit: ppm]\n\n\n\nWindow state (W): categorical variable ['closed'; 'open']\n\n\n\nGlare perception (G): continuous variable [unit: -]\n\n\n\n\n\n\n\n\n\nTo carry out the estimation step, we utilized linear regression within both the frequentist and Bayesian frameworks. Specifically, we will run three regression models:\n\n\nModel.1 will include only social interaction \\((\\text{I})\\) as predictor;\n\nModel.2 will include social interaction \\((\\text{I})\\) and CO2 concentration \\((\\text{C})\\) as predictors.\n\nModel.3 will include social interaction \\((\\text{I})\\), CO2 concentration \\((\\text{C})\\) and window state \\((\\text{W})\\) as predictors.\n\nThe results of the fitted statistical models (i.e., Model.1, Model.2 and Model.3) are presented here.\nFrequentist framework\n\nClick to expand\n\n#Fit the linear regression model with I and G (Model 1)\nex2_Model.1 &lt;-\n  lm(formula = G ~ I,\n     data = ex2_sample.random)\n#View of the model summary\nsummary(ex2_Model.1)\n\n\nCall:\nlm(formula = G ~ I, data = ex2_sample.random)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.8784  -9.9755  -0.2005  10.0889  25.5743 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 59.29801    0.43856   135.2   &lt;2e-16 ***\nI           -1.92618    0.04756   -40.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.21 on 4998 degrees of freedom\nMultiple R-squared:  0.2471,    Adjusted R-squared:  0.2469 \nF-statistic:  1640 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with I, C and G (Model 2)\nex2_Model.2 &lt;-\n  lm(formula = G ~ I + C,\n     data = ex2_sample.random)\n#View of the model summary\nsummary(ex2_Model.2)\n\n\nCall:\nlm(formula = G ~ I + C, data = ex2_sample.random)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.1718  -7.1808  -0.1675   7.4538  28.7898 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.57969    2.04883   -8.58   &lt;2e-16 ***\nI            -2.62628    0.04568  -57.49   &lt;2e-16 ***\nC             0.18872    0.00494   38.21   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.862 on 4997 degrees of freedom\nMultiple R-squared:  0.4173,    Adjusted R-squared:  0.4171 \nF-statistic:  1789 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with I, C, W and G (Model 3)\nex2_Model.3 &lt;-\n  lm(formula = G ~ I + C + W,\n     data = ex2_sample.random)\n#View of the model summary\nsummary(ex2_Model.3)\n\n\nCall:\nlm(formula = G ~ I + C + W, data = ex2_sample.random)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1535  -3.4904   0.0251   3.4565  18.6750 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.759671   1.198330  42.359   &lt;2e-16 ***\nI           -1.975151   0.024053 -82.115   &lt;2e-16 ***\nC           -0.002258   0.003002  -0.752    0.452    \nWopen       20.079323   0.169566 118.416   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.055 on 4996 degrees of freedom\nMultiple R-squared:  0.8469,    Adjusted R-squared:  0.8468 \nF-statistic:  9214 on 3 and 4996 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated coefficients for the three models are then plotted in Fig. 2.3.\n\nCodeb_socint.glare = -2\n\ndata.frame(model = c('Model.1', 'Model.2', 'Model.3'),\n           estimate = c(coef(ex2_Model.1)['I'], \n                        coef(ex2_Model.2)['I'], \n                        coef(ex2_Model.3)['I']),\n           lower.95.CI = c(confint(ex2_Model.1, level = 0.95, type = 'Wald')['I', 1],\n                           confint(ex2_Model.2, level = 0.95, type = 'Wald')['I', 1],\n                           confint(ex2_Model.3, level = 0.95, type = 'Wald')['I', 1]),\n           upper.95.CI = c(confint(ex2_Model.1, level = 0.95, type = 'Wald')['I', 2],\n                           confint(ex2_Model.2, level = 0.95, type = 'Wald')['I', 2],\n                           confint(ex2_Model.3, level = 0.95, type = 'Wald')['I', 2])) %&gt;%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_socint.glare, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-3, -1.5)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 2.3. Estimates of the social interaction coefficient for the three models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nFig. 2.3 shows the estimates (point estimate and 95% confidence interval) of the coefficient for social interaction for Model.1, Model.2 and Model.3.\nFor Model.1 we found a negative coefficient between social interaction \\((\\text{I})\\) and glare perception \\((\\text{G})\\) equal to -1.926 with 95% confidence interval (CI) [-2.019, -1.833]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 2.35e-310). Additionally, since the 95% CI includes the data-generating parameter for social interaction (i.e., b_socint.glare = -2), we can deduce that the estimated coefficient for social interaction is not statistically significantly different from -2 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for I = -2\ncar::linearHypothesis(ex2_Model.1, 'I = -2') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nI = - 2\n\nModel 1: restricted model\nModel 2: G ~ I\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   4999 628314                           \n2   4998 628012  1    302.67 2.4088 0.1207\n\n\nThe resulting p-value is 0.121, which indicates that we fail to reject the null hypothesis (i.e., I = -2) at the 0.05 level. This result suggests that the regression coefficient for social interaction (i.e., -1.926) is not statistically significantly different from -2. Since the estimated causal effect from Model.1 is unbiased, we would correctly conclude that an increase of unit in social interaction causes a decrease in glare perception by -1.926 units (95% CI [-2.019, -1.833]).\nFor Model.2 we found a negative coefficient between coefficient between social interaction \\((\\text{I})\\) and glare perception \\((\\text{G})\\) equal to -2.626 with 95% CI [-2.716, -2.537]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 0). Additionally, since the 95% CI does not include the data-generating parameter for social interaction (i.e., b_socint.glare = -2), we can deduce that the estimated coefficient for social interaction is statistically significantly different from -2 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for I = -2\ncar::linearHypothesis(ex2_Model.2, 'I = -2') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nI = - 2\n\nModel 1: restricted model\nModel 2: G ~ I + C\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   4998 504312                                  \n2   4997 486032  1     18280 187.94 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 5.11e-42, which indicates that we can reject the null hypothesis (i.e., I = -2) at the 0.05 level. This result suggests that the regression coefficient for social interaction (i.e., -2.626) is statistically significantly different from -2. Since the estimated causal effect from Model.2 is biased, we would erroneously conclude that an increase of unit in social interaction causes a decrease in glare perception by -2.626 units (95% CI [-2.716, -2.537]).\nFor Model.3 we found a negative coefficient between social interaction \\((\\text{I})\\) and glare perception \\((\\text{G})\\) equal to -1.975 with 95% CI [-2.022, -1.928]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 0). Additionally, since the 95% CI includes the data-generating parameter for social interaction (i.e., b_socint.glare = -2), we can deduce that the estimated coefficient for social interaction is not statistically significantly different from -2 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for I = -2\ncar::linearHypothesis(ex2_Model.3, 'I = -2') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nI = - 2\n\nModel 1: restricted model\nModel 2: G ~ I + C + W\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   4997 127705                           \n2   4996 127678  1    27.275 1.0673 0.3016\n\n\nThe resulting p-value is 0.302, which indicates that we fail to reject the null hypothesis (i.e., I = -2) at the 0.05 level. This result suggests that the regression coefficient for social interaction (i.e., -1.975) is not statistically significantly different from -2. Since the estimated causal effect from Model.3 is unbiased, we would correctly conclude that an increase of unit in social interaction causes a decrease in glare perception by -1.975 units (95% CI [-2.022, -1.928]).\nImportantly, for Model.1, Model.2, and Model.3, the 95% confidence interval means that if we were to repeat the sampling process and calculate the interval many times, 95% of those calculated intervals would contain the true population parameter. To highlight this, we can repeat the analysis by fitting the three models to one thousand data sets randomly selected from our population.\nThe for-loop shown in the code below performs the following operations. First, sample (using simple random sampling) a data set of 5,000 observations from the target population. Subsequently, perform linear regression using Model.1, Model.2 and Model.3 and store the estimated coefficients for interaction, its standard error and 95% confidence interval in the data frame coefs_ex2. This operation is repeated a thousand times, resulting in the data frame coefs_ex2 containing the estimates (point estimate, standard error and confidence interval) of a thousand random samples of size 5,000 using Model.1, Model.2 and Model.3.\n\nn_model &lt;- c('mod.1', 'mod.2', 'mod.3')     \n\nn_row &lt;- n_sims*length(n_model)\n\n#Create an empty data frame\nempty.df &lt;- data.frame(matrix(NA, nrow = n_row, ncol = 7))\n#Rename the data frame columns\ncolnames(empty.df) &lt;- c('sim.id', 'estimate', 'se', 'CI_2.5', 'CI_97.5',\n                        'model', 'coverage')\n\n#Sample a thousand data sets of 5,000 observations and perform linear regression \ncoefs_ex2 &lt;- empty.df #assign the empty data frame\nk = 1\nfor (i in 1:n_sims){\n  set.seed(ex2_random.seed[i]) #set unique seed for each simulation \n#Sample data set from population   \n  sample.random &lt;- \n    ex2_population %&gt;% \n    slice_sample(n = 5e3) #take a simple random sample of size 5,000\n#Fit models\n  for (j in 1:length(n_model)){\n    if (n_model[j] == 'mod.1'){\n      fit &lt;- lm(formula = G ~ I,\n                data = sample.random)\n    } else if (n_model[j] == 'mod.2'){\n      fit &lt;- lm(formula = G ~ I + C,\n                data = sample.random)  \n    } else {\n      fit &lt;- lm(formula = G ~ I + C + W,\n                data = sample.random)\n    }  \n#Compile matrix  \n  coefs_ex2[k, 1] &lt;- i #simulation ID\n  coefs_ex2[k, 2] &lt;- coef(fit)['I'] #point estimate\n  coefs_ex2[k, 3] &lt;- summary(fit)$coef['I','Std. Error'] #standard error\n  coefs_ex2[k, 4:5] &lt;- confint(fit, level = 0.95, type = 'Wald')['I', ] #confidence interval (Wald)\n  coefs_ex2[k, 6] &lt;- n_model[j] #sample size\n  k = k + 1\n  }\n}\ncoefs_ex2 &lt;- as_tibble(coefs_ex2)\n#View the data frame\ncoefs_ex2\n\n# A tibble: 3,000 × 7\n   sim.id estimate     se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;   \n 1      1    -1.93 0.0476  -2.02   -1.83 mod.1 NA      \n 2      1    -2.63 0.0457  -2.72   -2.54 mod.2 NA      \n 3      1    -1.98 0.0241  -2.02   -1.93 mod.3 NA      \n 4      2    -2.08 0.0483  -2.17   -1.98 mod.1 NA      \n 5      2    -2.71 0.0452  -2.80   -2.62 mod.2 NA      \n 6      2    -2.04 0.0242  -2.09   -1.99 mod.3 NA      \n 7      3    -2.07 0.0483  -2.16   -1.97 mod.1 NA      \n 8      3    -2.65 0.0457  -2.74   -2.56 mod.2 NA      \n 9      3    -1.99 0.0238  -2.03   -1.94 mod.3 NA      \n10      4    -2.01 0.0481  -2.10   -1.92 mod.1 NA      \n# ℹ 2,990 more rows\n\n\nThe coverage is defined by setting its value to 1 if the confidence interval overlaps the data-generating parameter for interaction (i.e., b_socint.glare = -2) and 0 otherwise.\n\n#Calculate coverage\ncoefs_ex2 &lt;-\n  coefs_ex2 %&gt;%\n  mutate(coverage = case_when(CI_2.5 &gt; b_socint.glare | CI_97.5 &lt; b_socint.glare ~ 0,\n                              CI_2.5 &lt;= b_socint.glare & CI_97.5 &gt;= b_socint.glare ~ 1,\n                              .default = NA))\n#View the data frame\ncoefs_ex2\n\n# A tibble: 3,000 × 7\n   sim.id estimate     se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1      1    -1.93 0.0476  -2.02   -1.83 mod.1        1\n 2      1    -2.63 0.0457  -2.72   -2.54 mod.2        0\n 3      1    -1.98 0.0241  -2.02   -1.93 mod.3        1\n 4      2    -2.08 0.0483  -2.17   -1.98 mod.1        1\n 5      2    -2.71 0.0452  -2.80   -2.62 mod.2        0\n 6      2    -2.04 0.0242  -2.09   -1.99 mod.3        1\n 7      3    -2.07 0.0483  -2.16   -1.97 mod.1        1\n 8      3    -2.65 0.0457  -2.74   -2.56 mod.2        0\n 9      3    -1.99 0.0238  -2.03   -1.94 mod.3        1\n10      4    -2.01 0.0481  -2.10   -1.92 mod.1        1\n# ℹ 2,990 more rows\n\n\nThe results are then plotted in Fig. 2.4.\n\nCodemodel_names &lt;- c('mod.1' = 'Model.1', \n                 'mod.2' = 'Model.2',\n                 'mod.3' = 'Model.3')\n\nggplot(data = subset(coefs_ex2, sim.id &lt;= 2e2), aes(x = sim.id, y = estimate, ymin = CI_2.5, ymax = CI_97.5, colour = as.factor(coverage))) + \n  geom_hline(yintercept = b_socint.glare, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 1, fill = 'white', stroke = 0.5) +\n  scale_colour_manual('', values = c('black', '#FF4040'),\n                      breaks = c('1','0')) +\n  scale_y_continuous('Estimate') +\n  scale_x_continuous('Simulation ID') +\n  facet_wrap(model~., \n             labeller = labeller(model = model_names),\n             nrow = 3,\n             scales = 'fixed') +\n  theme(legend.position = 'none', \n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 2.4. Estimates of the social interaction coefficient (only the first hundreds of the thousand simulations are shown). The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. In red are highlighted the confidence intervals that do not overlap the parameter used to generate the data (dashed blue line).\n\n\n\n\nFig. 2.4 shows the first 200 estimates (point estimate and confidence interval) of the coefficient for social interaction for Model.1, Model.2 and Model.3. If all the thousand simulations are considered, the frequency of the coverage of the calculated confidence intervals (i.e., how many times the confidence intervals overlap the data-generating parameter) is 94.6%, 0.0% and 94.4% for Model.1, Model.2 and Model.3, respectively. Since the estimate of the causal effect for Model.2 are biased, the calculated confidence intervals for this model do not have the expected coverage. In fact, confidence intervals only quantify the uncertainty due to random error (i.e., sample variability), not systematic error (i.e., bias). Instead, the estimates from Model.1 and Model.3 are unbiased and the calculated 95% confidence intervals have the expected coverage (i.e., overlap the data-generating parameter 95% of the times).\nWe can also visualize all the 1,000 estimates of the coefficient for social interaction for Model.1, Model.2 and Model.3 (the estimates were shown as white dots in Fig. 2.4).\n\nCodeggplot(data = coefs_ex2, aes(x = estimate, y = ifelse(after_stat(count) &gt; 0, after_stat(count), NA))) +\n  geom_histogram(binwidth = 0.02, fill = 'white', colour = 'grey50') +\n  geom_vline(xintercept = b_socint.glare, colour = 'blue', linetype = 'dashed') +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-3, -1.5)) +\n  facet_grid(model~., \n             labeller = labeller(model = model_names),\n             scales = 'fixed') +\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 2.5. Histogram of the 1,000 estimates of the social interaction coefficient for the three models. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nIn Fig. 2.5, the estimated coefficients are visualized through a histogram. Here, we can see that the estimates for Model.2 are not clustered around the data-generating parameter (blue dashed line) having a mean estimate of -2.634 (with 0.044 standard deviation). In contrast, the estimates for Model.1 and Model.3 having a mean estimate of -1.994 (with 0.048 standard deviation) and -2.001 (with 0.025 standard deviation), respectively, are centered around the data-generating parameter.\nAs expected, since there is no backdoor path open, regressing glare perception \\((\\text{G})\\) on social interaction \\((\\text{I})\\) (i.e., using Model.1) leads to the correct estimate of the total average causal effect. However, when CO2 is included as a predictor (i.e., using Model.2) the estimates are biased. Adjusting for CO2 alone opens the backdoor path \\(\\text{I} \\leftarrow \\text{O} \\rightarrow \\text{C} \\leftarrow \\text{W} \\rightarrow \\text{G}\\) that was previously closed. As a result, association can flow from \\(\\text{I}\\) to \\(\\text{G}\\) through \\(\\text{O}\\), \\(\\text{C}\\) and \\(\\text{W}\\). This happens because CO2 is a collider. However, when we also include window state \\((\\text{W})\\) as a predictor (i.e., using Model.3) the backdoor path is closed, leading to the correct estimate of the total average causal effect of social interaction \\((\\text{I})\\) on glare perception \\((\\text{G})\\).\nThis bias is known as M-bias. Generally, adjusting for an inappropriate variable (a collider on a non-causal path) opens an M-shaped backdoor path and creates spurious association.\nBayesian framework\n\n#Fit the linear regression model with I and G (Model 1)\nex2_Model.1 &lt;- stan_glm(formula = G ~ I,\n                        family = gaussian(),\n                        data = ex2_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = 0, scale = 4),\n                        #Prior intercept\n                        prior_intercept = normal(location = 50, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility   \n#View of the model\nex2_Model.1\n\nstan_glm\n family:       gaussian [identity]\n formula:      G ~ I\n observations: 5000\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 59.3    0.4  \nI           -1.9    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 11.2    0.1  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with I, C and G (Model 2)\nex2_Model.2 &lt;- stan_glm(formula = G ~ I + C,\n                        family = gaussian(),\n                        data = ex2_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(4, 1)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 50, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex2_Model.2\n\nstan_glm\n family:       gaussian [identity]\n formula:      G ~ I + C\n observations: 5000\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) -17.5    2.0 \nI            -2.6    0.0 \nC             0.2    0.0 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 9.9    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with I, C, W and G (Model 3)\nex2_Model.3 &lt;- stan_glm(formula = G ~ I + C + W,\n                        family = gaussian(),\n                        data = ex2_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0, 0), scale = c(4, 1, 40)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 50, scale = 10),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex2_Model.3\n\nstan_glm\n family:       gaussian [identity]\n formula:      G ~ I + C + W\n observations: 5000\n predictors:   4\n------\n            Median MAD_SD\n(Intercept) 50.8    1.2  \nI           -2.0    0.0  \nC            0.0    0.0  \nWopen       20.1    0.2  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 5.1    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nIn Bayesian analysis, there are important diagnostics that have to be carried out in order to assess the convergence and efficiency of the Markov Chains. This is done by using the monitor() function which computes summaries of MCMC (Markov Chain Monte Carlo) draws and monitor convergence. Specifically, we will look at Rhat, Bulk_ESS and Tail_ESS metrics.\n\n#Diagnostics for model 1\nmonitor(ex2_Model.1$stanfit)  \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       58.6     59.3     60.0     59.3 0.4     1    11109     7597\nI                 -2.0     -1.9     -1.8     -1.9 0.0     1    11198     7423\nsigma             11.0     11.2     11.4     11.2 0.1     1    12198     8926\nmean_PPD          42.4     42.7     43.1     42.7 0.2     1    11230    10778\nlog-posterior -19189.8 -19187.1 -19186.1 -19187.4 1.2     1     5159     7821\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 2\nmonitor(ex2_Model.2$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)      -20.9    -17.5    -14.3    -17.6 2.0     1    14345     9537\nI                 -2.7     -2.6     -2.6     -2.6 0.0     1    11779     8439\nC                  0.2      0.2      0.2      0.2 0.0     1    13380     9184\nsigma              9.7      9.9     10.0      9.9 0.1     1    12884     9520\nmean_PPD          42.4     42.7     43.1     42.7 0.2     1    11998    10505\nlog-posterior -18550.3 -18547.3 -18546.0 -18547.6 1.4     1     5374     6558\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 3\nmonitor(ex2_Model.3$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       48.8     50.8     52.8     50.8 1.2     1    10783     9224\nI                 -2.0     -2.0     -1.9     -2.0 0.0     1     9359     8154\nC                  0.0      0.0      0.0      0.0 0.0     1     9643     8274\nWopen             19.8     20.1     20.4     20.1 0.2     1     9978     8954\nsigma              5.0      5.1      5.1      5.1 0.1     1     9898     8482\nmean_PPD          42.6     42.7     42.9     42.7 0.1     1     9558     9741\nlog-posterior -15208.6 -15205.1 -15203.5 -15205.5 1.6     1     5197     7655\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\nRhat is a metric used to assess the convergence of Markov Chain Monte Carlo (MCMC) simulations. It helps determine if the MCMC chains have adequately explored the target posterior distribution. Specifically, it compares the between- and within-chain estimates for model parameters: If chains have not mixed well (i.e., the between- and within-chain estimates do not agree), R-hat is larger than 1. A general rule of thumb is to use the sample only if R-hat is less than 1.05; a larger value suggests that the chains have not mixed well, and the results might not be reliable. In our three models, all Rhat are equal to 1 indicating that the chains have mixed well and have adequately explored the target posterior distribution.\nBulk_ESS and Tail_ESS stand for ‘Bulk Effective Sample Size’ and ‘Tail Effective Sample Size,’ respectively. Since MCMC samples are not truly independent (they are correlated), these metrics assess the sampling efficiencies, that is, they help evaluate how efficiently the MCMC sampler is exploring the parameter space.\n\n\nBulk_ESS is a useful measure for sampling efficiency in the bulk (center) of the distribution (e.g., efficiency of mean and median estimates);\n\nTail_ESS is a useful measure for sampling efficiency in the tails of the distribution (e.g., efficiency of variance and tail quantile estimates). A general rule of thumb is that both Bulk-ESS and Tail-ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable. In our three models, all Bulk-ESS and Tail-ESS are well above 400 (i.e., 100 multiplied by 4, the number of chains we used) indicating that estimates of posterior quantiles are reliable.\n\nSince we have established that the posteriors are reliable, we can now explore the model estimates.\nThe estimated coefficients for the three models are then plotted in Fig. 2.6.\n\nCode#Extract draws from model 1 \npost_ex2_Model.1 &lt;-\n  ex2_Model.1 %&gt;% \n  spread_draws(I) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex2_Model.2 &lt;-\n  ex2_Model.2 %&gt;% \n  spread_draws(I) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.2') #add a new column to specify that the model\n\n#Extract draws from model 3\npost_ex2_Model.3 &lt;-\n  ex2_Model.3 %&gt;% \n  spread_draws(I) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.3') #add a new column to specify that the model\n\n#Combine draws\nplot.post &lt;- rbind(post_ex2_Model.1, post_ex2_Model.2, post_ex2_Model.3)\n\n# Plot\nplot.post  %&gt;%\n  ggplot(aes(y = model, x = I)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = b_socint.glare, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-3, -1.5)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 2.6. Posterior distribution of the social interaction coefficient for the three models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.\n\n\n\n\nFig. 2.6 shows the estimates (mean and 95% HDI) of the coefficient for social interaction for Model.1, Model.2 and Model.3.\nFor Model.1 we found a negative coefficient between social interaction \\((\\text{I})\\) and glare perception \\((\\text{G})\\) (mean = -1.927, 95% HDI [-2.022, -1.835]). The estimated causal effect is unbiased, leading to the correct conclusion that an increase of unit in social interaction causes a decrease in glare perception by -1.927 units, on average.\nFor Model.2 we found a negative coefficient between social interaction \\((\\text{I})\\) and glare perception \\((\\text{G})\\) (mean = -2.625, 95% HDI [-2.714, -2.537]). However, the estimated causal effect is biased, leading to the wrong conclusion that an increase of unit in social interaction causes a decrease in glare perception by -2.625 units, on average.\nFor Model.3 we found a negative coefficient between social interaction \\((\\text{I})\\) and glare perception \\((\\text{G})\\) (mean = -1.975, 95% HDI [-2.023, -1.928]). However, now the estimated causal effect is unbiased, leading to the correct conclusion that an increase of unit in social interaction causes a decrease in glare perception by -1.975 units, on average.\nImportantly, for Model.1, Model.2 and Model.3, the 95% HDI is the range of parameter values within which the most credible 95% of the posterior distribution falls. Unlike a frequentist confidence interval, the Bayesian 95% HDI has a direct probabilistic meaning: every point inside the HDI has a higher probability density than any point outside the interval. Therefore, given the model, the prior and the data, we can say that there is a 95% probability that the data-generating parameter (i.e., b_socint.glare = -2) lies within the HDI. However, since Model.2 leads to a biased estimate, we will reach the wrong conclusion by stating that there is a 95% probability that the data-generating parameter lies within the [-2.714, -2.537] interval. This probability is 0%.\nAs expected, since there is no backdoor path open, regressing glare perception \\((\\text{G})\\) on social interaction \\((\\text{I})\\) (i.e., using Model.1) leads to the correct estimate of the total average causal effect. However, when CO2 is included as a predictor (i.e., using Model.2) the estimates are biased. Adjusting for CO2 alone opens the backdoor path \\(\\text{I} \\leftarrow \\text{O} \\rightarrow \\text{C} \\leftarrow \\text{W} \\rightarrow \\text{G}\\) that was previously closed. As a result, association can flow from \\(\\text{I}\\) to \\(\\text{G}\\) through \\(\\text{O}\\), \\(\\text{C}\\) and \\(\\text{W}\\). This happens because CO2 is a collider. However, when we also include window state \\((\\text{W})\\) as a predictor (i.e., using Model.3) the backdoor path is closed, leading to the correct estimate of the total average causal effect of social interaction \\((\\text{I})\\) on glare perception \\((\\text{G})\\).\nThis bias is known as M-bias. Generally, adjusting for an inappropriate variable (a collider on a non-causal path) opens an M-shaped backdoor path and creates spurious association.",
    "crumbs": [
      "M-bias",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex2_simulation.html#session-info",
    "href": "ex2_simulation.html#session-info",
    "title": "2  Simulation example",
    "section": "Session info",
    "text": "Session info\nVersion information about R, the OS and attached or loaded packages.\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 26200)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rstan_2.32.7        StanHeaders_2.32.10 gt_1.1.0           \n [4] lubridate_1.9.4     forcats_1.0.1       stringr_1.6.0      \n [7] dplyr_1.1.4         purrr_1.2.0         readr_2.1.6        \n[10] tidyr_1.3.1         tibble_3.3.0        ggplot2_4.0.1      \n[13] tidyverse_2.0.0     ggdist_3.3.3        tidybayes_3.0.7    \n[16] rstanarm_2.32.2     Rcpp_1.1.0          dagitty_0.3-4      \n[19] ggdag_0.2.13       \n\nloaded via a namespace (and not attached):\n  [1] backports_1.5.0       plyr_1.8.9            igraph_2.2.1         \n  [4] splines_4.2.3         svUnit_1.0.8          crosstalk_1.2.2      \n  [7] rstantools_2.5.0      inline_0.3.21         digest_0.6.38        \n [10] htmltools_0.5.8.1     viridis_0.6.5         magrittr_2.0.4       \n [13] checkmate_2.3.3       memoise_2.0.1         tzdb_0.5.0           \n [16] graphlayouts_1.2.2    RcppParallel_5.1.11-1 matrixStats_1.5.0    \n [19] xts_0.14.1            timechange_0.3.0      colorspace_2.1-2     \n [22] ggrepel_0.9.6         rbibutils_2.4         xfun_0.54            \n [25] jsonlite_2.0.0        lme4_1.1-37           survival_3.8-3       \n [28] zoo_1.8-14            glue_1.8.0            reformulas_0.4.2     \n [31] polyclip_1.10-7       gtable_0.3.6          V8_8.0.1             \n [34] distributional_0.5.0  car_3.1-3             pkgbuild_1.4.8       \n [37] abind_1.4-8           scales_1.4.0          miniUI_0.1.2         \n [40] viridisLite_0.4.2     xtable_1.8-4          Formula_1.2-5        \n [43] stats4_4.2.3          DT_0.34.0             htmlwidgets_1.6.4    \n [46] threejs_0.3.4         arrayhelpers_1.1-0    RColorBrewer_1.1-3   \n [49] posterior_1.6.1       pkgconfig_2.0.3       loo_2.8.0            \n [52] farver_2.1.2          sass_0.4.10           utf8_1.2.6           \n [55] tidyselect_1.2.1      labeling_0.4.3        rlang_1.1.6          \n [58] reshape2_1.4.5        later_1.4.4           tools_4.2.3          \n [61] cachem_1.1.0          cli_3.6.5             generics_0.1.4       \n [64] evaluate_1.0.5        fastmap_1.2.0         yaml_2.3.10          \n [67] knitr_1.50            fs_1.6.6              tidygraph_1.3.1      \n [70] ggraph_2.2.2          nlme_3.1-168          mime_0.13            \n [73] xml2_1.5.0            compiler_4.2.3        bayesplot_1.14.0     \n [76] shinythemes_1.2.0     rstudioapi_0.17.1     curl_7.0.0           \n [79] tweenr_2.0.3          stringi_1.8.7         lattice_0.22-7       \n [82] Matrix_1.6-5          nloptr_2.2.1          markdown_2.0         \n [85] shinyjs_2.1.0         tensorA_0.36.2.1      vctrs_0.6.5          \n [88] pillar_1.11.1         lifecycle_1.0.4       Rdpack_2.6.4         \n [91] httpuv_1.6.16         QuickJSR_1.8.1        R6_2.6.1             \n [94] promises_1.5.0        gridExtra_2.3         codetools_0.2-20     \n [97] boot_1.3-32           colourpicker_1.3.0    MASS_7.3-58.2        \n[100] gtools_3.9.5          withr_3.0.2           shinystan_2.6.0      \n[103] parallel_4.2.3        hms_1.1.4             grid_4.2.3           \n[106] coda_0.19-4.1         minqa_1.2.8           rmarkdown_2.30       \n[109] S7_0.2.1              carData_3.0-5         otel_0.2.0           \n[112] ggforce_0.5.0         shiny_1.11.1          base64enc_0.1-3      \n[115] dygraphs_1.1.1.6     \n\n\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "M-bias",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex3_simulation.html",
    "href": "ex3_simulation.html",
    "title": "3  Simulation example",
    "section": "",
    "text": "3.1 Data-generating process\nThe data-generating process is described via the directed acyclic graph (DAG) in Fig. 3.1. In this DAG, window shading \\((\\text{W})\\) influences perceived warmth \\((\\text{P})\\) both directly and indirectly, passing through daylight \\((\\text{D})\\). Additionally, window shading \\((\\text{W})\\) also indirectly influences visual comfort \\((\\text{V})\\) through daylight \\((\\text{D})\\).\nCodedag_coords.ex3 &lt;-\n  data.frame(name = c('W', 'D', 'V', 'P'),\n             x = c(1, 3.5, 6, 6),\n             y = c(3, 2, 1, 3))\n\nDAG.ex3 &lt;-\n  dagify(D ~ W,\n         P ~ W + D,\n         V ~ D,\n         coords = dag_coords.ex3)\n\nnode_labels &lt;- c(\n  W = 'bold(W)', \n  D = 'bold(D)', \n  V = 'bold(V)', \n  P = 'bold(P)'\n)\n\nggplot(data = DAG.ex3, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 3.3, label = 'window shading', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'daylight', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  annotate('text', x = 6, y = 3.3, label = 'perceived warmth', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'visual comfort', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 3.1. Graphical representation via DAG of the data-generating process.\nThe DAG in Fig. 3.1 can be written as:",
    "crumbs": [
      "Post-treatment bias",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex3_simulation.html#data-generating-process",
    "href": "ex3_simulation.html#data-generating-process",
    "title": "3  Simulation example",
    "section": "",
    "text": "\\(D \\sim f_{D}(W)\\), read as ‘daylight \\((\\text{D})\\) is some function of window shading \\((\\text{W})\\)’.\n\n\\(P \\sim f_{P}(W, D)\\), read as ‘perceived warmth \\((\\text{P})\\) is some function of window shading \\((\\text{W})\\) and daylight \\((\\text{D})\\)’.\n\n\\(V \\sim f_{V}(D)\\), read as ‘visual comfort \\((\\text{V})\\) is some function of daylight \\((\\text{D})\\)’.",
    "crumbs": [
      "Post-treatment bias",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex3_simulation.html#synthetic-data-set",
    "href": "ex3_simulation.html#synthetic-data-set",
    "title": "3  Simulation example",
    "section": "\n3.2 Synthetic data set",
    "text": "3.2 Synthetic data set\nTo generate synthetic data, we defined the custom function data.sim_ex3(). This function takes as inputs the sample size n and generates synthetic data according to the DAG in Fig. 3.1.\n\ndata.sim_ex3 &lt;- function(n) {\n  b_win.day = 1.5 #direct causal effect of W on D\n  b_win.per = -0.7 #direct causal effect of W on P\n  b_day.per = 0.8 #direct causal effect of D on P\n  b_win = b_win.per + (b_win.day * b_day.per) #total causal effect of W on P\n  #Simulate window \n  W &lt;- rnorm(n = n, mean = 0, sd = 1) \n  #Simulate daylight\n  D &lt;- rnorm(n = n, mean = b_win.day * W, sd = 0.5)\n  #Simulate visual comfort \n  V &lt;- rnorm(n = n, mean = 1 * D, sd = 0.2)\n  #Simulate perceived warmth \n  P &lt;- rnorm(n = n, mean = b_win.per * W  + b_day.per * D, sd = 0.5)\n  #Return tibble with simulated values\n  return(tibble(W, D, P, V))\n  }\n\nFrom this data generation mechanism, we simulated the target population, which consists of one million observations.\n\nset.seed(2025) #set random number for reproducibility\n#Simulate the population\nex3_population &lt;- data.sim_ex3(n = 1e6)\n#View the data frame\nex3_population\n\n# A tibble: 1,000,000 × 4\n         W       D       P      V\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.621   2.01    1.14    1.85 \n 2  0.0356  1.24    1.62    0.875\n 3  0.773   0.826  -0.716   1.06 \n 4  1.27    0.978   0.241   1.05 \n 5  0.371   0.0871 -0.0980 -0.384\n 6 -0.163  -0.0222 -0.364   0.387\n 7  0.397   0.672   0.821   0.867\n 8 -0.0800 -0.801  -1.27   -0.553\n 9 -0.345   0.282   1.40    0.280\n10  0.702   0.306  -0.193   0.538\n# ℹ 999,990 more rows\n\n\nFrom this population, we obtained one data set of five thousand observations using simple random sampling.\n\nn_sims &lt;- 1e3 #number of data sets to simulate\n\n#Set random number for reproducibility\nset.seed(2025)  \n#Generate a vector of random numbers for reproducibility\nex3_random.seed &lt;- sample(1:1e5, size = n_sims, replace = FALSE)\n\n\nset.seed(ex3_random.seed[1])\n#Sample one data set of 5,000 observations\nex3_sample.random &lt;- \n  ex3_population %&gt;% \n  slice_sample(n = 5e3) #take a simple random sample of size 5,000 \n\n#View the data frame\nex3_sample.random\n\n# A tibble: 5,000 × 4\n          W      D       P      V\n      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.148    0.878  0.767   0.889\n 2  0.820    1.45   0.348   1.21 \n 3 -1.28    -2.24  -0.939  -2.46 \n 4 -0.179   -0.226 -0.331  -0.352\n 5 -0.131   -0.320 -0.189  -0.284\n 6 -0.990   -1.88  -0.432  -1.78 \n 7 -0.522   -1.16  -0.688  -1.53 \n 8 -0.00594 -0.293  0.398  -0.335\n 9 -0.146   -0.950  0.0766 -0.574\n10 -0.991   -2.15  -1.50   -2.12 \n# ℹ 4,990 more rows",
    "crumbs": [
      "Post-treatment bias",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex3_simulation.html#data-analysis",
    "href": "ex3_simulation.html#data-analysis",
    "title": "3  Simulation example",
    "section": "\n3.3 Data analysis",
    "text": "3.3 Data analysis\nIn this example, the target of our analysis is the total average causal effect, ACE (also known as total average treatment effect, ATE) of window shading \\((\\text{W})\\) on perceived warmth \\((\\text{P})\\), which stands for the expected increase of \\(\\text{P}\\) in response to a unit increase in \\(\\text{W}\\) due to an intervention. The causal effect of interest is visualized in Fig. 3.2.\n\nCodeggplot(data = DAG.ex3, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 6, y = 3, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_segment(x = 1, xend = 3.5, y = 3, yend = 2,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_segment(x = 3.5, xend = 6, y = 2, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 3.3, label = 'window shading', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'daylight', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  annotate('text', x = 6, y = 3.3, label = 'perceived warmth', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'visual comfort', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  \n  #causal effect number\n  annotate('text', x = 3.5, y = 3.2, label = '(-0.7)', \n           size = 4.5, hjust = 0.5, colour = 'grey50', parse = TRUE) +\n  annotate('text', x = 1.8, y = 2.4, label = '(1.5)', \n           size = 4.5, hjust = 0.5, colour = 'grey50', parse = TRUE) +\n  annotate('text', x = 5.2, y = 2.4, label = '(0.8)', \n           size = 4.5, hjust = 0.5, colour = 'grey50', parse = TRUE) +\n  \n  #causal effect total number b_win = b_win.per + (b_win.day * b_day.per)\n  annotate('text', x = 3.5, y = 2.8, label = '-0.7 + (1.5 %*% 0.8) == 0.5', # %-&gt;% \n           size = 4.5, hjust = 0.5, colour = 'black', parse = TRUE) +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 3.2. Graphical representation via DAG of the data-generating process. The green lines indicate the causal question of interest, and the black number indicates the total average causal effect (the grey numbers with parentheses indicate the causal effect between the two variables connected by the corresponding path).\n\n\n\n\n\n3.3.1 Identification\nThe first step to answer the causal question of interest is identification. Identification answers a ‘theoretical’ question by determining whether a causal effect can, in principle, be estimated from observed data. The backdoor criterion and its generalization, the adjustment criterion, allow us to understand whether our causal effect of interest can be identified and, if so, which variables we should (or should not) statistically adjust for (i.e., the adjustment set) to estimate the causal effect from the data.\nGiven its simplicity, we will first apply the backdoor criterion to identify valid adjustment sets to estimate the causal effect of interest. If the backdoor criterion is not applicable, we will apply its generalization, the adjustment criterion.\nBackdoor criterion\nApplying the backdoor criterion revealed the absence of any backdoor path (i.e., a non-causal path) from window shading \\((\\text{W})\\) to perceived warmth \\((\\text{P})\\). As a result, there is no confounding and no adjustment is required.\nGiven the DAG in Fig. 3.2, we can use the adjustmentSets() function to identify the adjustment set algorithmically. It is essential to note that this function applies the adjustment criterion and not the backdoor criterion. As such, the adjustmentSets() function can find adjustment sets even when the backdoor criterion is not applicable.\n\nadjustmentSets(DAG.ex3,\n               exposure = 'W', #window shading\n               outcome = 'P', #perceived warmth\n               type = 'all', \n               effect = 'total', \n               max.results = Inf)\n\n {}\n\n\nAs expected, the resulting adjustment set includes the empty set. Therefore, to get the correct estimate of the total average causal effect, no adjustment is required; failing to do so will lead to bias.\n\n3.3.2 Estimation\nFollowing the identification step is the estimation step. This step addresses a statistical question by determining how the causal effect identified in the previous step can be estimated. To perform this step, we used a parametric (model-based) estimator, specifically, linear regression. This was possible because we designed the illustrative examples to be simple and with a linear relationship between the variables. This way, we limited the complexity of the examples themselves and shifted the focus to the application of the backdoor criterion to define ‘correct’ adjustment sets.\nFor transparency and understanding, all (implicit) assumptions used for this illustrative example are (explicitly) provided in Table 3.1.\n\n\n\nTable 3.1. Summary description of the simulation example\n\n\n\n\n\n\n\n\n\n\n\nResearch question\nTotal average causal effect (ACE) of window shading (W) on perceived warmth (P).\n\n\nAssumptions\nRandom sample (simple random sampling): everyone in the population has an equal chance of being selected into the sample.\n\n\n\nLimited random variability: large sample size.\n\n\n\nIndependence of observations: each observation represents independent bits of information.\n\n\n\nNo confounding: the DAG includes all shared causes among the variables.\n\n\n\nNo model error: perfect functional form specification.\n\n\n\nNo measurement error: all variables are measured perfectly.\n\n\nVariables\nwindow shading (W): continuous variable [unit: -]\n\n\n\nDaylight (D): continuous variable [unit: lux]\n\n\n\nPerceived warmth (P): continuous variable [unit: -]\n\n\n\nVisual comfort (V): continuous variable [unit: -]\n\n\n\n\n\n\n\n\n\nTo carry out the estimation step, we utilized linear regression within both the frequentist and Bayesian frameworks. Specifically, we will run three regression models:\n\n\nModel.1 will include only window shading \\((\\text{W})\\) as predictor;\n\nModel.2 will include window shading \\((\\text{W})\\) and daylight \\((\\text{D})\\) as predictors.\n\nModel.3 will include window shading \\((\\text{W})\\) and visual comfort \\((\\text{V})\\) as predictors.\n\nThe results of the fitted statistical models (i.e., Model.1, Model.2 and Model.3) are presented here.\nFrequentist framework\n\n#Fit the linear regression model with W and P (Model 1)\nex3_Model.1 &lt;-\n  lm(formula = P ~ W,\n     data = ex3_sample.random)\n#View of the model summary\nsummary(ex3_Model.1)\n\n\nCall:\nlm(formula = P ~ W, data = ex3_sample.random)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.73804 -0.42710  0.00262  0.42714  2.35619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.001521   0.009025  -0.169    0.866    \nW            0.510180   0.008920  57.192   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6382 on 4998 degrees of freedom\nMultiple R-squared:  0.3956,    Adjusted R-squared:  0.3954 \nF-statistic:  3271 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with W, D and P (Model 2)\nex3_Model.2 &lt;-\n  lm(formula = P ~ W + D,\n     data = ex3_sample.random)\n#View of the model summary\nsummary(ex3_Model.2)\n\n\nCall:\nlm(formula = P ~ W + D, data = ex3_sample.random)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92355 -0.33391 -0.00018  0.33940  1.68471 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.003004   0.007111   0.422    0.673    \nW           -0.698074   0.022967 -30.394   &lt;2e-16 ***\nD            0.800956   0.014495  55.259   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5028 on 4997 degrees of freedom\nMultiple R-squared:  0.6248,    Adjusted R-squared:  0.6247 \nF-statistic:  4161 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with W, V and P (Model 3)\nex3_Model.3 &lt;-\n  lm(formula = P ~ W + V,\n     data = ex3_sample.random)\n#View of the model summary\nsummary(ex3_Model.3)\n\n\nCall:\nlm(formula = P ~ W + V, data = ex3_sample.random)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9264 -0.3466 -0.0032  0.3518  1.8682 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.005649   0.007426   0.761    0.447    \nW           -0.522315   0.022364 -23.355   &lt;2e-16 ***\nV            0.683934   0.013994  48.873   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.525 on 4997 degrees of freedom\nMultiple R-squared:  0.591, Adjusted R-squared:  0.5909 \nF-statistic:  3611 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated coefficients for the three models are then plotted in Fig. 3.3.\n\nCode  b_win.day = 1.5 #direct causal effect of W on D\n  b_win.per = -0.7 #direct causal effect of W on P\n  b_day.per = 0.8 #direct causal effect of D on P\n  \n  b_win = b_win.per + (b_win.day * b_day.per) #total causal effect of W on P\n\ndata.frame(model = c('Model.1', 'Model.2', 'Model.3'),\n           estimate = c(coef(ex3_Model.1)['W'], \n                        coef(ex3_Model.2)['W'], \n                        coef(ex3_Model.3)['W']),\n           lower.95.CI = c(confint(ex3_Model.1, level = 0.95, type = 'Wald')['W', 1],\n                           confint(ex3_Model.2, level = 0.95, type = 'Wald')['W', 1],\n                           confint(ex3_Model.3, level = 0.95, type = 'Wald')['W', 1]),\n           upper.95.CI = c(confint(ex3_Model.1, level = 0.95, type = 'Wald')['W', 2],\n                           confint(ex3_Model.2, level = 0.95, type = 'Wald')['W', 2],\n                           confint(ex3_Model.3, level = 0.95, type = 'Wald')['W', 2])) %&gt;%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_win, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-0.85, 0.55)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 3.3. Estimates of the window shading coefficient for the three models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nFig. 3.3 shows the estimates (point estimate and 95% confidence interval) of the coefficient for window shading for Model.1, Model.2 and Model.3.\nFor Model.1 we found a positive coefficient between window shading \\((\\text{W})\\) and perceived warmth \\((\\text{P})\\) equal to 0.510 with 95% confidence interval (CI) [0.493, 0.528]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 0). Additionally, since the 95% CI includes the data-generating parameter for window shading (i.e., b_win = 0.5), we can deduce that the estimated coefficient for window shading is not statistically significantly different from 0.5 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for W = 0.5\ncar::linearHypothesis(ex3_Model.1, 'W = 0.5') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nW = 0.5\n\nModel 1: restricted model\nModel 2: P ~ W\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   4999 2035.9                           \n2   4998 2035.4  1   0.53037 1.3024 0.2538\n\n\nThe resulting p-value is 0.254, which indicates that we fail to reject the null hypothesis (i.e., W = 0.5) at the 0.05 level. This result suggests that the regression coefficient for window shading (i.e., 0.510) is not statistically significantly different from 0.5. Since the estimated causal effect from Model.1 is unbiased, we would correctly conclude that an increase of unit in the opening of window shading causes an increase in perceived warmth by 0.510 units (95% CI [0.493, 0.528]).\nFor Model.2 we found a negative coefficient between window shading \\((\\text{W})\\) and perceived warmth \\((\\text{P})\\) equal to -0.698 with 95% CI [-0.743, -0.653]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 2.43e-186). Additionally, since the 95% CI does not include the data-generating parameter for window shading (i.e., b_win = 0.5), we can deduce that the estimated coefficient for window shading is statistically significantly different from 0.5 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for W = 0.5\ncar::linearHypothesis(ex3_Model.2, 'W = 0.5') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nW = 0.5\n\nModel 1: restricted model\nModel 2: P ~ W + D\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   4998 1951.3                                  \n2   4997 1263.4  1    687.97 2721.1 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 0, which indicates that we can reject the null hypothesis (i.e., W = 0.5) at the 0.05 level. This result suggests that the regression coefficient for window shading (i.e., -0.698) is statistically significantly different from 0.5. Since the estimated causal effect from Model.2 is biased, we would erroneously conclude that an increase of unit in the opening of window shading causes a decrease in perceived warmth by -0.698 units (95% CI [-0.743, -0.653]).\nFor Model.3 we found a negative coefficient between window shading \\((\\text{W})\\) and perceived warmth \\((\\text{P})\\) equal to -0.522 with 95% CI [-0.566, -0.478]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 1.39e-114). Additionally, since the 95% CI does not include the data-generating parameter for window shading (i.e., b_win = 0.5), we can deduce that the estimated coefficient for window shading is statistically significantly different from 0.5 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for W = 0.5\ncar::linearHypothesis(ex3_Model.3, 'W = 0.5') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nW = 0.5\n\nModel 1: restricted model\nModel 2: P ~ W + V\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   4998 1953.0                                  \n2   4997 1377.1  1    575.86 2089.6 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 0, which indicates that we can reject the null hypothesis (i.e., W = 0.5) at the 0.05 level. This result suggests that the regression coefficient for window shading (i.e., -0.522) is statistically significantly different from 0.5. Since the estimated causal effect from Model.3 is biased, we would erroneously conclude that an increase of unit in the opening of window shading causes a decrease in perceived warmth by -0.522 units (95% CI [-0.566, -0.478]).\nImportantly, for Model.1, Model.2, and Model.3, the 95% confidence interval means that if we were to repeat the sampling process and calculate the interval many times, 95% of those calculated intervals would contain the true population parameter. To highlight this, we can repeat the analysis by fitting the three models to one thousand data sets randomly selected from our population.\nThe for-loop shown in the code below performs the following operations. First, sample (using simple random sampling) a data set of 5,000 observations from the target population. Subsequently, perform linear regression using Model.1, Model.2 and Model.3 and store the estimated coefficients for window shading, its standard error and 95% confidence interval in the data frame coefs_ex3. This operation is repeated a thousand times, resulting in the data frame coefs_ex3 containing the estimates (point estimate, standard error and confidence interval) of a thousand random samples of size 5,000 using Model.1, Model.2 and Model.3.\n\nn_model &lt;- c('mod.1', 'mod.2', 'mod.3')     \n\nn_row &lt;- n_sims*length(n_model)\n\n#Create an empty data frame\nempty.df &lt;- data.frame(matrix(NA, nrow = n_row, ncol = 7))\n#Rename the data frame columns\ncolnames(empty.df) &lt;- c('sim.id', 'estimate', 'se', 'CI_2.5', 'CI_97.5',\n                        'model', 'coverage')\n\n#Sample a thousand data sets of 5,000 observations and perform linear regression \ncoefs_ex3 &lt;- empty.df #assign the empty data frame\nk = 1\nfor (i in 1:n_sims){\n  set.seed(ex3_random.seed[i]) #set unique seed for each simulation \n#Sample data set from population   \n  sample.random &lt;- \n    ex3_population %&gt;% \n    slice_sample(n = 5e3) #take a simple random sample of size 5,000\n#Fit models\n  for (j in 1:length(n_model)){\n    if (n_model[j] == 'mod.1'){\n      fit &lt;- lm(formula = P ~ W,\n                data = sample.random)\n    } else if (n_model[j] == 'mod.2'){\n      fit &lt;- lm(formula = P ~ W + D,\n                data = sample.random)  \n    } else {\n      fit &lt;- lm(formula = P ~ W + V,\n                data = sample.random)\n    }  \n#Compile matrix  \n  coefs_ex3[k, 1] &lt;- i #simulation ID\n  coefs_ex3[k, 2] &lt;- coef(fit)['W'] #point estimate\n  coefs_ex3[k, 3] &lt;- summary(fit)$coef['W','Std. Error'] #standard error\n  coefs_ex3[k, 4:5] &lt;- confint(fit, level = 0.95, type = 'Wald')['W', ] #confidence interval (Wald)\n  coefs_ex3[k, 6] &lt;- n_model[j] #sample size\n  k = k + 1\n  }\n}\ncoefs_ex3 &lt;- as_tibble(coefs_ex3)\n#View the data frame\ncoefs_ex3\n\n# A tibble: 3,000 × 7\n   sim.id estimate      se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;   \n 1      1    0.510 0.00892  0.493   0.528 mod.1 NA      \n 2      1   -0.698 0.0230  -0.743  -0.653 mod.2 NA      \n 3      1   -0.522 0.0224  -0.566  -0.478 mod.3 NA      \n 4      2    0.488 0.00913  0.470   0.506 mod.1 NA      \n 5      2   -0.710 0.0221  -0.753  -0.666 mod.2 NA      \n 6      2   -0.555 0.0216  -0.597  -0.512 mod.3 NA      \n 7      3    0.495 0.00903  0.478   0.513 mod.1 NA      \n 8      3   -0.707 0.0220  -0.750  -0.664 mod.2 NA      \n 9      3   -0.544 0.0216  -0.586  -0.502 mod.3 NA      \n10      4    0.511 0.00910  0.494   0.529 mod.1 NA      \n# ℹ 2,990 more rows\n\n\nThe coverage is defined by setting its value to 1 if the confidence interval overlaps the data-generating parameter for window shading (i.e., b_win = 0.5) and 0 otherwise.\n\n#Calculate coverage\ncoefs_ex3 &lt;-\n  coefs_ex3 %&gt;%\n  mutate(coverage = case_when(CI_2.5 &gt; b_win | CI_97.5 &lt; b_win ~ 0,\n                              CI_2.5 &lt;= b_win & CI_97.5 &gt;= b_win ~ 1,\n                              .default = NA))\n#View the data frame\ncoefs_ex3\n\n# A tibble: 3,000 × 7\n   sim.id estimate      se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1      1    0.510 0.00892  0.493   0.528 mod.1        1\n 2      1   -0.698 0.0230  -0.743  -0.653 mod.2        0\n 3      1   -0.522 0.0224  -0.566  -0.478 mod.3        0\n 4      2    0.488 0.00913  0.470   0.506 mod.1        1\n 5      2   -0.710 0.0221  -0.753  -0.666 mod.2        0\n 6      2   -0.555 0.0216  -0.597  -0.512 mod.3        0\n 7      3    0.495 0.00903  0.478   0.513 mod.1        1\n 8      3   -0.707 0.0220  -0.750  -0.664 mod.2        0\n 9      3   -0.544 0.0216  -0.586  -0.502 mod.3        0\n10      4    0.511 0.00910  0.494   0.529 mod.1        1\n# ℹ 2,990 more rows\n\n\nThe results are then plotted in Fig. 3.4.\n\nCodemodel_names &lt;- c('mod.1' = 'Model.1', \n                 'mod.2' = 'Model.2',\n                 'mod.3' = 'Model.3')\n\nggplot(data = subset(coefs_ex3, sim.id &lt;= 2e2), aes(x = sim.id, y = estimate, ymin = CI_2.5, ymax = CI_97.5, colour = as.factor(coverage))) + \n  geom_hline(yintercept = b_win, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 1, fill = 'white', stroke = 0.5) +\n  scale_colour_manual('', values = c('black', '#FF4040'),\n                      breaks = c('1','0')) +\n  scale_y_continuous('Estimate') +\n  scale_x_continuous('Simulation ID') +\n  facet_wrap(model~., \n             labeller = labeller(model = model_names),\n             nrow = 3,\n             scales = 'fixed') +\n  theme(legend.position = 'none', \n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 3.4. Estimates of the window shading coefficient (only the first hundreds of the thousand simulations are shown). The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. In red are highlighted the confidence intervals that do not overlap the parameter used to generate the data (dashed blue line).\n\n\n\n\nFig. 3.4 shows the first 200 estimates (point estimate and confidence interval) of the causal effect of window shading for Model.1, Model.2 and Model.3. If all the thousand simulations are considered, the frequency of the coverage of the calculated confidence intervals (i.e., how many times the confidence intervals overlap the data-generating parameter) is 94.8%, 0.0% and 0.0% for Model.1, Model.2 and Model.3, respectively. Since the estimate of the causal effect for Model.2 and Model.3 are biased, the calculated confidence intervals for these models do not have the expected coverage. In fact, confidence intervals only quantify the uncertainty due to random error (i.e., sample variability), not systematic error (i.e., bias). Instead, the estimates from Model.1 are unbiased and the calculated 95% confidence intervals have the expected coverage (i.e., overlap the data-generating parameter 95% of the times).\nWe can also visualize all the 1,000 estimates of the coefficient for window shading for Model.1, Model.2 and Model.3 (the estimates were shown as white dots in Fig. 3.4).\n\nCodeggplot(data = coefs_ex3, aes(x = estimate, y = ifelse(after_stat(count) &gt; 0, after_stat(count), NA))) +\n  geom_histogram(binwidth = 0.015, fill = 'white', colour = 'grey50') +\n  geom_vline(xintercept = b_win, colour = 'blue', linetype = 'dashed') +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-0.85, 0.55)) +\n  facet_grid(model~., \n             labeller = labeller(model = model_names),\n             scales = 'fixed') +\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 3.5. Histogram of the 1,000 estimates of the window shading coefficient for the three models. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nIn Fig. 3.5, the estimated coefficients are visualized through a histogram. Here, we can see that the estimates for Model.2 and Model.3 are not clustered around the data-generating parameter (blue dashed line) having a mean estimate of -0.701 (with 0.022 standard deviation) and -0.535 (with 0.021 standard deviation), respectively. In contrast, the estimates for Model.1 having a mean estimate of 0.499 (with 0.009 standard deviation) are centered around the data-generating parameter.\nAs expected, since there is no backdoor path open, regressing perceived warmth \\((\\text{P})\\) on window shading \\((\\text{W})\\) (i.e., using Model.1) leads to the correct estimate of the total average causal effect. However, when daylight is included as a predictor (i.e., using Model.2), the estimates are biased. Adjusting for daylight blocks a part of the causal effect. This happens because daylight is a mediator on the path from window shading \\((\\text{W})\\) to perceived warmth \\((\\text{P})\\). This bias is known as post-treatment bias. Additionally, we have a biased estimate of the total average causal effect of window shading \\((\\text{W})\\) on perceived warmth \\((\\text{P})\\) when we include visual comfort \\((\\text{V})\\) as a predictor (i.e., using Model.3). This happens because visual comfort \\((\\text{V})\\) is a descendant of daylight \\((\\text{D})\\); consequently, adjusting for visual comfort \\((\\text{V})\\) is akin to adjusting for daylight \\((\\text{D})\\) since the two variables share information (daylight is the cause of visual comfort).\nBayesian framework\n\nClick to expand\n\n#Fit the linear regression model with W and P (Model 1)\nex3_Model.1 &lt;- stan_glm(formula = P ~ W,\n                        family = gaussian(),\n                        data = ex3_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = 0, scale = 1),\n                        #Prior intercept\n                        prior_intercept = normal(location = 0, scale = 1),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 1),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility   \n#View of the model\nex3_Model.1\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ W\n observations: 5000\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.0    0.0   \nW           0.5    0.0   \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.6    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with W, D and P (Model 2)\nex3_Model.2 &lt;- stan_glm(formula = P ~ W + D,\n                        family = gaussian(),\n                        data = ex3_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(1, 1)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 0, scale = 1),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 1),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex3_Model.2\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ W + D\n observations: 5000\n predictors:   3\n------\n            Median MAD_SD\n(Intercept)  0.0    0.0  \nW           -0.7    0.0  \nD            0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.5    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with W, V and P (Model 3)\nex3_Model.3 &lt;- stan_glm(formula = P ~ W + V,\n                        family = gaussian(),\n                        data = ex3_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(1, 1)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 0, scale = 1),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 1),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex3_Model.3\n\nstan_glm\n family:       gaussian [identity]\n formula:      P ~ W + V\n observations: 5000\n predictors:   3\n------\n            Median MAD_SD\n(Intercept)  0.0    0.0  \nW           -0.5    0.0  \nV            0.7    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.5    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nIn Bayesian analysis, there are important diagnostics that have to be carried out in order to assess the convergence and efficiency of the Markov Chains. This is done by using the monitor() function which computes summaries of MCMC (Markov Chain Monte Carlo) draws and monitor convergence. Specifically, we will look at Rhat, Bulk_ESS and Tail_ESS metrics.\n\n#Diagnostics for model 1\nmonitor(ex3_Model.1$stanfit)  \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                   Q5     Q50     Q95    Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       0.0     0.0     0.0     0.0 0.0     1    12088     8622\nW                 0.5     0.5     0.5     0.5 0.0     1    11533     8670\nsigma             0.6     0.6     0.6     0.6 0.0     1    11793     8110\nmean_PPD          0.0     0.0     0.0     0.0 0.0     1    12191    10616\nlog-posterior -4854.8 -4852.0 -4851.0 -4852.4 1.2     1     6031     7127\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 2\nmonitor(ex3_Model.2$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                   Q5     Q50     Q95    Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       0.0     0.0     0.0     0.0 0.0     1     7636     6818\nW                -0.7    -0.7    -0.7    -0.7 0.0     1     4770     5567\nD                 0.8     0.8     0.8     0.8 0.0     1     4796     5487\nsigma             0.5     0.5     0.5     0.5 0.0     1     8255     6236\nmean_PPD          0.0     0.0     0.0     0.0 0.0     1     9358    10287\nlog-posterior -3664.7 -3661.7 -3660.4 -3662.0 1.4     1     4692     6853\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 3\nmonitor(ex3_Model.3$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                   Q5     Q50     Q95    Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       0.0     0.0     0.0     0.0 0.0     1     8298     7180\nW                -0.6    -0.5    -0.5    -0.5 0.0     1     4298     5085\nV                 0.7     0.7     0.7     0.7 0.0     1     4334     5399\nsigma             0.5     0.5     0.5     0.5 0.0     1     9060     7011\nmean_PPD          0.0     0.0     0.0     0.0 0.0     1     9705    10021\nlog-posterior -3880.2 -3877.1 -3875.7 -3877.4 1.4     1     4575     6066\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\nRhat is a metric used to assess the convergence of Markov Chain Monte Carlo (MCMC) simulations. It helps determine if the MCMC chains have adequately explored the target posterior distribution. Specifically, it compares the between- and within-chain estimates for model parameters: If chains have not mixed well (i.e., the between- and within-chain estimates do not agree), R-hat is larger than 1. A general rule of thumb is to use the sample only if R-hat is less than 1.05; a larger value suggests that the chains have not mixed well, and the results might not be reliable. In our three models, all Rhat are equal to 1 indicating that the chains have mixed well and have adequately explored the target posterior distribution.\nBulk_ESS and Tail_ESS stand for ‘Bulk Effective Sample Size’ and ‘Tail Effective Sample Size,’ respectively. Since MCMC samples are not truly independent (they are correlated), these metrics assess the sampling efficiencies, that is, they help evaluate how efficiently the MCMC sampler is exploring the parameter space.\n\n\nBulk_ESS is a useful measure for sampling efficiency in the bulk (center) of the distribution (e.g., efficiency of mean and median estimates);\n\nTail_ESS is a useful measure for sampling efficiency in the tails of the distribution (e.g., efficiency of variance and tail quantile estimates). A general rule of thumb is that both Bulk-ESS and Tail-ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable. In our three models, all Bulk-ESS and Tail-ESS are well above 400 (i.e., 100 multiplied by 4, the number of chains we used) indicating that estimates of posterior quantiles are reliable.\n\nSince we have established that the posteriors are reliable, we can now explore the model estimates.\nThe estimated coefficients for the three models are then plotted in Fig. 3.6.\n\nCode#Extract draws from model 1 \npost_ex3_Model.1 &lt;-\n  ex3_Model.1 %&gt;% \n  spread_draws(W) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex3_Model.2 &lt;-\n  ex3_Model.2 %&gt;% \n  spread_draws(W) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.2') #add a new column to specify that the model\n\n#Extract draws from model 3\npost_ex3_Model.3 &lt;-\n  ex3_Model.3 %&gt;% \n  spread_draws(W) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.3') #add a new column to specify that the model\n\n#Combine draws\nplot.post &lt;- rbind(post_ex3_Model.1, post_ex3_Model.2, post_ex3_Model.3)\n\n# Plot\nplot.post  %&gt;%\n  ggplot(aes(y = model, x = W)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = b_win, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-0.85, 0.55)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 3.6. Posterior distribution of the window shading coefficient for the three models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.\n\n\n\n\nFig. 3.6 shows the estimates (mean and 95% HDI) of the coefficient for window shading for Model.1, Model.2 and Model.3.\nFor Model.1 we found a positive coefficient between window shading \\((\\text{W})\\) and perceived warmth \\((\\text{P})\\) (mean = 0.510, 95% HDI [0.493, 0.527]). The estimated causal effect is unbiased, leading to the correct conclusion that an increase of unit in the opening of window shading causes an increase in perceived warmth by 0.510 units, on average.\nFor Model.2 we found a negative coefficient between window shading \\((\\text{W})\\) and perceived warmth \\((\\text{P})\\) (mean = -0.698, 95% HDI [-0.745, -0.654]). The estimated causal effect is therefore biased, leading to the wrong conclusion that an increase of unit in the opening of window shading causes a decrease in perceived warmth by -0.698 units, on average.\nFor Model.3 we found a negative coefficient between window shading \\((\\text{W})\\) and perceived warmth \\((\\text{P})\\) (mean = -0.522, 95% HDI [-0.567, -0.478]). The estimated causal effect is therefore biased, leading to the wrong conclusion that an increase of unit in the opening of window shading causes a decrease in perceived warmth by -0.522 units, on average.\nImportantly, for both Model.1, Model.2 and Model.3, the 95% HDI is the range of parameter values within which the most credible 95% of the posterior distribution falls. Unlike a frequentist confidence interval, the Bayesian 95% HDI has a direct probabilistic meaning: every point inside the HDI has a higher probability density than any point outside the interval. Therefore, given the model, the prior and the data, we can say that there is a 95% probability that the data-generating parameter (i.e., b_win = 0.5) lies within the HDI. However, since Model.2 and Model.3 lead to biased estimates, we will reach the wrong conclusion by stating that there is a 95% probability that the data-generating parameter lies within the [-0.745, -0.654] interval and [-0.567, -0.478], respectively. This probability is 0%.\nAs expected, since there is no backdoor path open, regressing perceived warmth \\((\\text{P})\\) on window shading \\((\\text{W})\\) (i.e., using Model.1) leads to the correct estimate of the total average causal effect. However, when daylight is included as a predictor (i.e., using Model.2) the estimates are biased. Adjusting for daylight blocks a part of the causal effect. This happens because daylight is a mediator on the path from window shading \\((\\text{W})\\) to perceived warmth \\((\\text{P})\\). This bias is known as post-treatment bias. Additionally, we have a biased estimate of the total average causal effect of window shading \\((\\text{W})\\) on perceived warmth \\((\\text{P})\\) when we include visual comfort \\((\\text{V})\\) as a predictor (i.e., using Model.3). This happens because visual comfort \\((\\text{V})\\) is a descendant of daylight \\((\\text{D})\\); consequently, adjusting for visual comfort \\((\\text{V})\\) is akin to adjusting for daylight \\((\\text{D})\\) since the two variables share information (daylight is the cause of visual comfort).",
    "crumbs": [
      "Post-treatment bias",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex3_simulation.html#session-info",
    "href": "ex3_simulation.html#session-info",
    "title": "3  Simulation example",
    "section": "Session info",
    "text": "Session info\nVersion information about R, the OS and attached or loaded packages.\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 26200)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rstan_2.32.7        StanHeaders_2.32.10 gt_1.1.0           \n [4] lubridate_1.9.4     forcats_1.0.1       stringr_1.6.0      \n [7] dplyr_1.1.4         purrr_1.2.0         readr_2.1.6        \n[10] tidyr_1.3.1         tibble_3.3.0        ggplot2_4.0.1      \n[13] tidyverse_2.0.0     ggdist_3.3.3        tidybayes_3.0.7    \n[16] rstanarm_2.32.2     Rcpp_1.1.0          dagitty_0.3-4      \n[19] ggdag_0.2.13       \n\nloaded via a namespace (and not attached):\n  [1] backports_1.5.0       plyr_1.8.9            igraph_2.2.1         \n  [4] splines_4.2.3         svUnit_1.0.8          crosstalk_1.2.2      \n  [7] rstantools_2.5.0      inline_0.3.21         digest_0.6.38        \n [10] htmltools_0.5.8.1     viridis_0.6.5         magrittr_2.0.4       \n [13] checkmate_2.3.3       memoise_2.0.1         tzdb_0.5.0           \n [16] graphlayouts_1.2.2    RcppParallel_5.1.11-1 matrixStats_1.5.0    \n [19] xts_0.14.1            timechange_0.3.0      colorspace_2.1-2     \n [22] ggrepel_0.9.6         rbibutils_2.4         xfun_0.54            \n [25] jsonlite_2.0.0        lme4_1.1-37           survival_3.8-3       \n [28] zoo_1.8-14            glue_1.8.0            reformulas_0.4.2     \n [31] polyclip_1.10-7       gtable_0.3.6          V8_8.0.1             \n [34] distributional_0.5.0  car_3.1-3             pkgbuild_1.4.8       \n [37] abind_1.4-8           scales_1.4.0          miniUI_0.1.2         \n [40] viridisLite_0.4.2     xtable_1.8-4          Formula_1.2-5        \n [43] stats4_4.2.3          DT_0.34.0             htmlwidgets_1.6.4    \n [46] threejs_0.3.4         arrayhelpers_1.1-0    RColorBrewer_1.1-3   \n [49] posterior_1.6.1       pkgconfig_2.0.3       loo_2.8.0            \n [52] farver_2.1.2          sass_0.4.10           utf8_1.2.6           \n [55] tidyselect_1.2.1      labeling_0.4.3        rlang_1.1.6          \n [58] reshape2_1.4.5        later_1.4.4           tools_4.2.3          \n [61] cachem_1.1.0          cli_3.6.5             generics_0.1.4       \n [64] evaluate_1.0.5        fastmap_1.2.0         yaml_2.3.10          \n [67] knitr_1.50            fs_1.6.6              tidygraph_1.3.1      \n [70] ggraph_2.2.2          nlme_3.1-168          mime_0.13            \n [73] xml2_1.5.0            compiler_4.2.3        bayesplot_1.14.0     \n [76] shinythemes_1.2.0     rstudioapi_0.17.1     curl_7.0.0           \n [79] tweenr_2.0.3          stringi_1.8.7         lattice_0.22-7       \n [82] Matrix_1.6-5          nloptr_2.2.1          markdown_2.0         \n [85] shinyjs_2.1.0         tensorA_0.36.2.1      vctrs_0.6.5          \n [88] pillar_1.11.1         lifecycle_1.0.4       Rdpack_2.6.4         \n [91] httpuv_1.6.16         QuickJSR_1.8.1        R6_2.6.1             \n [94] promises_1.5.0        gridExtra_2.3         codetools_0.2-20     \n [97] boot_1.3-32           colourpicker_1.3.0    MASS_7.3-58.2        \n[100] gtools_3.9.5          withr_3.0.2           shinystan_2.6.0      \n[103] parallel_4.2.3        hms_1.1.4             grid_4.2.3           \n[106] coda_0.19-4.1         minqa_1.2.8           rmarkdown_2.30       \n[109] S7_0.2.1              carData_3.0-5         otel_0.2.0           \n[112] ggforce_0.5.0         shiny_1.11.1          base64enc_0.1-3      \n[115] dygraphs_1.1.1.6     \n\n\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Post-treatment bias",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex4_simulation.html",
    "href": "ex4_simulation.html",
    "title": "4  Simulation example",
    "section": "",
    "text": "4.1 Data-generating process\nThe data-generating process is described via the directed acyclic graph (DAG) in Fig. 4.1. In this DAG, relative humidity \\((\\text{H})\\) influences respiratory irritation \\((\\text{R})\\) both directly and indirectly, passing through microbial aerosol \\((\\text{M})\\). Additionally, respiratory irritation \\((\\text{R})\\) influences absence \\((\\text{A})\\) directly.\nCodedag_coords.ex4 &lt;-\n  data.frame(name = c('M', 'H', 'A', 'R'),\n             x = c(1, 3.5, 6, 6),\n             y = c(3, 2, 1, 3))\n\nDAG.ex4 &lt;-\n  dagify(M ~ H,\n         R ~ H + M,\n         A ~ R,\n         coords = dag_coords.ex4)\n\nnode_labels &lt;- c(\n  M = 'bold(M)', \n  H = 'bold(H)', \n  A = 'bold(A)', \n  R = 'bold(R)'\n)\n\nggplot(data = DAG.ex4, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 3.3, label = 'microbial aerosol', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'relative humidity', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 3.3, label = 'respiratory irritation', \n           size = 4, hjust = 0.7, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'absence', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 4.1. Graphical representation via DAG of the data-generating process.\nThe DAG in Fig. 4.1 can be written as:",
    "crumbs": [
      "Case-control bias",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex4_simulation.html#data-generating-process",
    "href": "ex4_simulation.html#data-generating-process",
    "title": "4  Simulation example",
    "section": "",
    "text": "\\(M \\sim f_{M}(H)\\), read as ‘microbial aerosol \\((\\text{M})\\) is some function of relative humidity \\((\\text{H})\\)’.\n\n\\(R \\sim f_{R}(H, M)\\), read as ‘respiratory irritation \\((\\text{R})\\) is some function of relative humidity \\((\\text{H})\\) and microbial aerosol \\((\\text{M})\\)’.\n\n\\(A \\sim f_{A}(R)\\), read as ‘absence \\((\\text{A})\\) is some function of respiratory irritation \\((\\text{R})\\)’.",
    "crumbs": [
      "Case-control bias",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex4_simulation.html#synthetic-data-set",
    "href": "ex4_simulation.html#synthetic-data-set",
    "title": "4  Simulation example",
    "section": "\n4.2 Synthetic data set",
    "text": "4.2 Synthetic data set\nTo generate synthetic data, we defined the custom function data.sim_ex4(). This function takes as inputs the sample size n and generates synthetic data according to the DAG in Fig. 4.1.\n\ndata.sim_ex4 &lt;- function(n) {\n  b_hum.resp = c(0, 0.5) #direct causal effect of H on R\n  b_micr.resp = c(0, 3) #direct causal effect of M on R\n  #Simulate relative humidity\n  H &lt;- factor(sample(c('low', 'high'), size = n, replace = TRUE))\n  #Simulate microbial aerosol\n  M &lt;- factor(ifelse(rbinom(n = n, size = 1, prob = plogis(ifelse(H == 'low', -1.5, 2))) == 0, 'low', 'high'))\n  #Simulate respiratory irritation\n  R &lt;- rnorm(n = n, mean = -1 + ifelse(H == 'high', b_hum.resp[1], b_hum.resp[2]) + ifelse(M == 'low',  b_micr.resp[1],  b_micr.resp[2]), sd = 3)\n  #Simulate absence \n  A &lt;- factor(ifelse(rbinom(n = n, size = 1, prob = plogis(0.3 + 2 * R)) == 0, 'no', 'yes'))\n  #Return tibble with simulated values\n  return(tibble(H, A, M, R))\n}\n\nFrom this data generation mechanism, we simulated the target population, which consists of one million observations.\n\nset.seed(2025) #set random number for reproducibility\n#Simulate the population\nex4_population &lt;- data.sim_ex4(n = 1e6)\n#Set relative humidity reference category to 'high'\nex4_population$H &lt;- relevel(ex4_population$H, ref = 'high')\n#Set microbial aerosol reference category to 'low'\nex4_population$M &lt;- relevel(ex4_population$M, ref = 'low')\n#Set absence reference category to 'no'\nex4_population$A &lt;- relevel(ex4_population$A, ref = 'no')\n#View the data frame\nex4_population\n\n# A tibble: 1,000,000 × 4\n   H     A     M            R\n   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n 1 low   yes   low    5.95   \n 2 high  yes   high   9.13   \n 3 high  yes   high  -0.00136\n 4 high  no    high  -3.59   \n 5 low   no    low   -3.32   \n 6 low   yes   low    0.832  \n 7 high  yes   high   2.46   \n 8 low   no    low   -4.59   \n 9 high  yes   high   6.80   \n10 low   no    low   -4.98   \n# ℹ 999,990 more rows\n\n\nFrom this population, we obtained one data set of five thousand observations using simple random sampling.\n\nn_sims &lt;- 1e3 #number of data sets to simulate\n\n#Set random number for reproducibility\nset.seed(2025)  \n#Generate a vector of random numbers for reproducibility\nex4_random.seed &lt;- sample(1:1e5, size = n_sims, replace = FALSE)\n\n\nset.seed(ex4_random.seed[1])\n#Sample one data set of 5,000 observations\nex4_sample.random &lt;- \n  ex4_population %&gt;% \n  slice_sample(n = 5e3) #take a simple random sample of size 5,000 \n\n#View the data frame\nex4_sample.random\n\n# A tibble: 5,000 × 4\n   H     A     M          R\n   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 high  yes   high   5.94 \n 2 low   yes   low    0.799\n 3 low   no    low   -2.45 \n 4 high  yes   high   2.25 \n 5 low   no    low   -1.24 \n 6 low   no    low   -2.90 \n 7 high  no    high  -0.279\n 8 low   no    low   -2.21 \n 9 high  no    high  -2.38 \n10 low   no    low   -4.50 \n# ℹ 4,990 more rows",
    "crumbs": [
      "Case-control bias",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex4_simulation.html#data-analysis",
    "href": "ex4_simulation.html#data-analysis",
    "title": "4  Simulation example",
    "section": "\n4.3 Data analysis",
    "text": "4.3 Data analysis\nIn this example, the target of our analysis is the total average causal effect, ACE (also known as total average treatment effect, ATE) of microbial aerosol \\((\\text{M})\\) on respiratory irritation \\((\\text{R})\\), which stands for the expected increase of \\(\\text{R}\\) in response to a change in \\(\\text{M}\\) from low to high due to an intervention. The causal effect of interest is visualized in Fig. 4.2.\n\nCodeggplot(data = DAG.ex4, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 6, y = 3, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 1, y = 3.3, label = 'microbial aerosol', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 1.7, label = 'relative humidity', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 3.3, label = 'respiratory irritation', \n           size = 4, hjust = 0.7, colour = 'grey50') +\n  annotate('text', x = 6, y = 0.7, label = 'absence', \n           size = 4, hjust = 0.6, colour = 'grey50') +\n  #causal effect number\n  annotate('text', x = 3.5, y = 3.2, label = '3', \n           size = 4.5, hjust = 0.5, colour = 'black', parse = TRUE) +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 4.2. Graphical representation via DAG of the data-generating process. The green line indicates the causal question of interest, and the number on the path indicates the total average causal effect.\n\n\n\n\n\n4.3.1 Identification\nThe first step to answer the causal question of interest is identification. Identification answers a ‘theoretical’ question by determining whether a causal effect can, in principle, be estimated from observed data. The backdoor criterion and its generalization, the adjustment criterion, allow us to understand whether our causal effect of interest can be identified and, if so, which variables we should (or should not) statistically adjust for (i.e., the adjustment set) to estimate the causal effect from the data.\nGiven its simplicity, we will first apply the backdoor criterion to identify valid adjustment sets to estimate the causal effect of interest. If the backdoor criterion is not applicable, we will apply its generalization, the adjustment criterion.\nBackdoor criterion\nApplying the adjustment criterion revealed the existence of a backdoor path (i.e., a non-causal path) from microbial aerosol \\((\\text{M})\\) to respiratory irritation \\((\\text{R})\\). Specifically, the backdoor path is \\(\\text{M} \\leftarrow \\text{H} \\rightarrow \\text{R}\\). Since relative humidity \\((\\text{H})\\) is a common cause of \\(\\text{M}\\) and \\(\\text{R}\\), association can flow from \\(\\text{M}\\) to \\(\\text{R}\\) through \\(\\text{H}\\). As a result, there is confounding. To close this backdoor path we need to adjust for \\(\\text{H}\\).\nGiven the DAG in Fig. 4.2, we can use the adjustmentSets() function to identify the adjustment set algorithmically. It is essential to note that this function applies the adjustment criterion and not the backdoor criterion. As such, the adjustmentSets() function can find adjustment sets even when the backdoor criterion is not applicable.\n\nadjustmentSets(DAG.ex4,\n               exposure = 'M', #microbial aerosol\n               outcome = 'R', #respiratory irritation\n               type = 'all', \n               effect = 'total', \n               max.results = Inf)\n\n{ H }\n\n\nAs expected, the resulting adjustment set includes \\(\\text{H}\\). Therefore, to get the correct estimate of the total average causal effect, we need to adjust for \\(\\text{H}\\); failing to do so will lead to bias.\n\n4.3.2 Estimation\nFollowing the identification step is the estimation step. This step addresses a statistical question by determining how the causal effect identified in the previous step can be estimated. To perform this step, we used a parametric (model-based) estimator, specifically, linear regression. This was possible because we designed the illustrative examples to be simple and with a linear relationship between the variables. This way, we limited the complexity of the examples themselves and shifted the focus to the application of the backdoor criterion to define ‘correct’ adjustment sets.\nFor transparency and understanding, all (implicit) assumptions used for this illustrative example are (explicitly) provided in Table 4.1.\n\n\n\nTable 4.1. Summary description of the simulation example\n\n\n\n\n\n\n\n\n\n\n\nResearch question\nTotal average causal effect (ACE) of microbial aerosol (M) on respiratory irritation (R).\n\n\nAssumptions\nRandom sample (simple random sampling): everyone in the population has an equal chance of being selected into the sample.\n\n\n\nLimited random variability: large sample size.\n\n\n\nIndependence of observations: each observation represents independent bits of information.\n\n\n\nNo confounding: the DAG includes all shared causes among the variables.\n\n\n\nNo model error: perfect functional form specification.\n\n\n\nNo measurement error: all variables are measured perfectly.\n\n\nVariables\nRelative humidity (H): categorical variable ['low'; 'high']\n\n\n\nMicrobial aerosol (M): categorical variable ['low'; 'high']\n\n\n\nAbsence (A): categorical variable ['no'; 'yes']\n\n\n\nRespiratory irritation (R): continuous variable [unit: -]\n\n\n\n\n\n\n\n\n\nTo carry out the estimation step, we utilized linear regression within both the frequentist and Bayesian frameworks. Specifically, we will run three regression models:\n\n\nModel.1 will include microbial aerosol \\((\\text{M})\\), relative humidity \\((\\text{H})\\) and absence \\((\\text{A})\\) as predictor;\n\nModel.2 will include microbial aerosol \\((\\text{M})\\) and relative humidity \\((\\text{H})\\) as predictors.\n\nThe results of the fitted statistical models (i.e., Model.1 and Model.2) are presented here.\nFrequentist framework\n\nClick to expand\n\n#Fit the linear regression model with M, H, A and R (Model 1)\nex4_Model.1 &lt;-\n  lm(formula = R ~ M + H + A,\n     data = ex4_sample.random)\n#View of the model summary\nsummary(ex4_Model.1)\n\n\nCall:\nlm(formula = R ~ M + H + A, data = ex4_sample.random)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.725 -1.386 -0.043  1.365  7.905 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.95007    0.08678  -33.99  &lt; 2e-16 ***\nMhigh        1.37308    0.08360   16.42  &lt; 2e-16 ***\nHlow         0.32211    0.08114    3.97 7.29e-05 ***\nAyes         4.63997    0.06148   75.47  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.013 on 4996 degrees of freedom\nMultiple R-squared:  0.606, Adjusted R-squared:  0.6058 \nF-statistic:  2562 on 3 and 4996 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with M, H and R (Model 2)\nex4_Model.2 &lt;-\n  lm(formula = R ~ M + H,\n     data = ex4_sample.random)\n#View of the model summary\nsummary(ex4_Model.2)\n\n\nCall:\nlm(formula = R ~ M + H, data = ex4_sample.random)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8867  -1.9861   0.0083   2.0066   9.2993 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.9769     0.1210  -8.071 8.69e-16 ***\nMhigh         2.8811     0.1187  24.262  &lt; 2e-16 ***\nHlow          0.5111     0.1186   4.309 1.67e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.944 on 4997 degrees of freedom\nMultiple R-squared:  0.1569,    Adjusted R-squared:  0.1565 \nF-statistic: 464.9 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated coefficients for the two models are then plotted in Fig. 4.3.\n\nCodeb_micr.resp = 3\n\ndata.frame(model = c('Model.1', 'Model.2'),\n           estimate = c(coef(ex4_Model.1)['Mhigh'], \n                        coef(ex4_Model.2)['Mhigh']),\n           lower.95.CI = c(confint(ex4_Model.1, level = 0.95, type = 'Wald')['Mhigh', 1],\n                           confint(ex4_Model.2, level = 0.95, type = 'Wald')['Mhigh', 1]),\n           upper.95.CI = c(confint(ex4_Model.1, level = 0.95, type = 'Wald')['Mhigh', 2],\n                           confint(ex4_Model.2, level = 0.95, type = 'Wald')['Mhigh', 2])) %&gt;%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_micr.resp, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(0.5, 3.5)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 4.3. Estimates of the microbial aerosol coefficient for the two models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nFig. 4.3 shows the estimates (point estimate and 95% confidence interval) of the coefficient for microbial aerosol for Model.1 and Model.2.\nFor Model.1 we found a positive coefficient between microbial aerosol \\((\\text{M})\\) and respiratory irritation \\((\\text{R})\\) equal to 1.373 with 95% confidence interval (CI) [1.209, 1.537]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 4.47e-59). Moreover, since the 95% CI does not include the data-generating parameter for microbial aerosol (i.e., b_micr.resp = 3), we can deduce that the estimated coefficient for microbial aerosol is statistically significantly different from 3 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for M = 3\ncar::linearHypothesis(ex4_Model.1, 'Mhigh = 3') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nMhigh = 3\n\nModel 1: restricted model\nModel 2: R ~ M + H + A\n\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   4997 21775                                  \n2   4996 20240  1    1534.2 378.69 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 2.31e-81, which indicates that we can reject the null hypothesis (i.e., M = 3) at the 0.05 level. This result suggests that the regression coefficient for microbial aerosol (i.e., 1.373) is statistically significantly different from 3. Since the estimated causal effect from Model.1 is biased, we would erroneously conclude that changing microbial aerosol from low to high causes an increase in lung infection by 1.373 units (95% CI [1.209, 1.537]).\nFor Model.2 we found a positive coefficient between microbial aerosol \\((\\text{M})\\) and respiratory irritation \\((\\text{R})\\) equal to 2.881 with 95% CI [2.648, 3.114]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 5.01e-123). Additionally, since the 95% CI includes the data-generating parameter for microbial aerosol (i.e., b_micr.resp = 3), we can deduce that the estimated coefficient for microbial aerosol is not statistically significantly different from 3 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for M = 3\ncar::linearHypothesis(ex4_Model.2, 'Mhigh = 3') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nMhigh = 3\n\nModel 1: restricted model\nModel 2: R ~ M + H\n\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1   4998 43324                           \n2   4997 43315  1    8.6974 1.0034 0.3165\n\n\nThe resulting p-value is 0.317, which indicates that we fail to reject the null hypothesis (i.e., M = 3) at the 0.05 level. This result suggests that the regression coefficient for microbial aerosol (i.e., 2.881) is not statistically significantly different from 3. Since the estimated causal effect from Model.2 is unbiased, we would correctly conclude that changing microbial aerosol from low to high causes an increase in lung infection by 2.881 units (95% CI [2.648, 3.114]).\nImportantly, for Model.1 and Model.2, the 95% confidence interval means that if we were to repeat the sampling process and calculate the interval many times, 95% of those calculated intervals would contain the true population parameter. To highlight this, we can repeat the analysis by fitting the two models to one thousand data sets randomly selected from our population.\nThe for-loop shown in the code below performs the following operations. First, sample (using simple random sampling) a data set of 5,000 observations from the target population. Subsequently, perform linear regression using Model.1 and Model.2 and store the estimated coefficients for microbial aerosol, its standard error and 95% confidence interval in the data frame coefs_ex4. This operation is repeated a thousand times, resulting in the data frame coefs_ex4 containing the estimates (point estimate, standard error and confidence interval) of a thousand random samples of size 5,000 using Model.1 and Model.2.\n\nn_model &lt;- c('mod.1', 'mod.2')     \n\nn_row &lt;- n_sims*length(n_model)\n\n#Create an empty data frame\nempty.df &lt;- data.frame(matrix(NA, nrow = n_row, ncol = 7))\n#Rename the data frame columns\ncolnames(empty.df) &lt;- c('sim.id', 'estimate', 'se', 'CI_2.5', 'CI_97.5',\n                        'model', 'coverage')\n\n#Sample a thousand data sets of 5,000 observations and perform linear regression \ncoefs_ex4 &lt;- empty.df #assign the empty data frame\nk = 1\nfor (i in 1:n_sims){\n  set.seed(ex4_random.seed[i]) #set unique seed for each simulation \n#Sample data set from population   \n  sample.random &lt;- \n    ex4_population %&gt;% \n    slice_sample(n = 5e3) #take a simple random sample of size 5,000\n#Fit models\n  for (j in 1:length(n_model)){\n    if (n_model[j] == 'mod.1'){\n      fit &lt;- lm(formula = R ~ M + H + A,\n                data = sample.random)\n    } else {\n      fit &lt;- lm(formula = R ~ M + H,\n                data = sample.random)\n    }  \n#Compile matrix  \n  coefs_ex4[k, 1] &lt;- i #simulation ID\n  coefs_ex4[k, 2] &lt;- coef(fit)['Mhigh'] #point estimate\n  coefs_ex4[k, 3] &lt;- summary(fit)$coef['Mhigh','Std. Error'] #standard error\n  coefs_ex4[k, 4:5] &lt;- confint(fit, level = 0.95, type = 'Wald')['Mhigh', ] #confidence interval (Wald)\n  coefs_ex4[k, 6] &lt;- n_model[j] #sample size\n  k = k + 1\n  }\n}\ncoefs_ex4 &lt;- as_tibble(coefs_ex4)\n#View the data frame\ncoefs_ex4\n\n# A tibble: 2,000 × 7\n   sim.id estimate     se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;   \n 1      1     1.37 0.0836   1.21    1.54 mod.1 NA      \n 2      1     2.88 0.119    2.65    3.11 mod.2 NA      \n 3      2     1.30 0.0874   1.13    1.47 mod.1 NA      \n 4      2     2.89 0.122    2.65    3.13 mod.2 NA      \n 5      3     1.38 0.0881   1.21    1.56 mod.1 NA      \n 6      3     3.11 0.123    2.87    3.35 mod.2 NA      \n 7      4     1.47 0.0844   1.31    1.64 mod.1 NA      \n 8      4     3.12 0.118    2.89    3.35 mod.2 NA      \n 9      5     1.33 0.0841   1.17    1.50 mod.1 NA      \n10      5     2.99 0.118    2.76    3.22 mod.2 NA      \n# ℹ 1,990 more rows\n\n\nThe coverage is defined by setting its value to 1 if the confidence interval overlaps the data-generating parameter for microbial aerosol (i.e., b_micr.resp = 3) and 0 otherwise.\n\n#Calculate coverage\ncoefs_ex4 &lt;-\n  coefs_ex4 %&gt;%\n  mutate(coverage = case_when(CI_2.5 &gt; b_micr.resp | CI_97.5 &lt; b_micr.resp ~ 0,\n                              CI_2.5 &lt;= b_micr.resp & CI_97.5 &gt;= b_micr.resp ~ 1,\n                              .default = NA))\n#View the data frame\ncoefs_ex4\n\n# A tibble: 2,000 × 7\n   sim.id estimate     se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1      1     1.37 0.0836   1.21    1.54 mod.1        0\n 2      1     2.88 0.119    2.65    3.11 mod.2        1\n 3      2     1.30 0.0874   1.13    1.47 mod.1        0\n 4      2     2.89 0.122    2.65    3.13 mod.2        1\n 5      3     1.38 0.0881   1.21    1.56 mod.1        0\n 6      3     3.11 0.123    2.87    3.35 mod.2        1\n 7      4     1.47 0.0844   1.31    1.64 mod.1        0\n 8      4     3.12 0.118    2.89    3.35 mod.2        1\n 9      5     1.33 0.0841   1.17    1.50 mod.1        0\n10      5     2.99 0.118    2.76    3.22 mod.2        1\n# ℹ 1,990 more rows\n\n\nThe results are then plotted in Fig. 4.4.\n\nCodemodel_names &lt;- c('mod.1' = 'Model.1', \n                 'mod.2' = 'Model.2')\n\nggplot(data = subset(coefs_ex4, sim.id &lt;= 2e2), aes(x = sim.id, y = estimate, ymin = CI_2.5, ymax = CI_97.5, colour = as.factor(coverage))) + \n  geom_hline(yintercept = b_micr.resp, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 1, fill = 'white', stroke = 0.5) +\n  scale_colour_manual('', values = c('black', '#FF4040'),\n                      breaks = c('1','0')) +\n  scale_y_continuous('Estimate',\n                     breaks = seq(from = -3, to = 3, by = 0.5)) +\n  scale_x_continuous('Simulation ID') +\n  facet_wrap(model~., \n             labeller = labeller(model = model_names),\n             nrow = 3,\n             scales = 'fixed') +\n  theme(legend.position = 'none', \n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 4.4. Estimates of the microbial aerosol coefficient (only the first hundreds of the thousand simulations are shown). The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. In red are highlighted the confidence intervals that do not overlap the parameter used to generate the data (dashed blue line).\n\n\n\n\nFig. 4.4 shows the first 200 estimates (point estimate and confidence interval) of the coefficient for microbial aerosol for Model.1 and Model.2. If all the thousand simulations are considered, the frequency of the coverage of the calculated confidence intervals (i.e., how many times the confidence intervals overlap the data-generating parameter) is 0.0% and 95.3% for Model.1 and Model.2, respectively. Since the estimate of the causal effect for Model.1 are biased, the calculated confidence intervals for this model do not have the expected coverage. In fact, confidence intervals only quantify the uncertainty due to random error (i.e., sample variability), not systematic error (i.e., bias). Instead, the estimate from Model.2 are unbiased and the calculated 95% confidence intervals have the expected coverage (i.e., overlap the data-generating parameter 95% of the times).\nWe can also visualize all the 1,000 estimates of the coefficient for microbial aerosol for Model.1 and Model.2 (the estimates were shown as white dots in Fig. 4.4).\n\nCodeggplot(data = coefs_ex4, aes(x = estimate, y = ifelse(after_stat(count) &gt; 0, after_stat(count), NA))) +\n  geom_histogram(binwidth = 0.05, fill = 'white', colour = 'grey50') +\n  geom_vline(xintercept = b_micr.resp, colour = 'blue', linetype = 'dashed') +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(0.9, 3.5)) +\n  facet_grid(model~., \n             labeller = labeller(model = model_names),\n             scales = 'fixed') +\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 4.5. Histogram of the 1,000 estimates of the microbial aerosol coefficient for the two models. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nIn Fig. 4.5, the estimated coefficients are visualized through a histogram. Here, we can see that the estimates for Model.1 are not clustered around the data-generating parameter (blue dashed line) having a mean estimate of 1.334 (with 0.086 standard deviation). In contrast, the estimates for Model.2 having a mean estimate of 2.999 (with 0.120 standard deviation) are centered around the data-generating parameter.\nAs expected, adjusting for relative humidity \\((\\text{H})\\) (i.e., using Model.2) leads to the correct estimate of the total average causal effect of microbial aerosol \\((\\text{M})\\) on respiratory irritation \\((\\text{R})\\). However, when adjusting for both relative humidity \\((\\text{H})\\) and absence \\((\\text{A})\\) (i.e., using Model.1) the estimates are biased. This happened because absence \\((\\text{A})\\) is a descendant of respiratory irritation \\((\\text{R})\\), which is collider. Adjusting for a descendant of a collider is akin to adjusting for the collider itself, and, therefore, can introduce a similar bias.\nTo better explain this bias, we can plot the magnified DAG that shows that causes of all variables.\n\nCodedag_coords.ex4.3 &lt;-\n  data.frame(name = c('M', 'H', 'A', 'R', 'UA', 'UH', 'UM', 'UR'),\n             x = c(1, 3.5, 6, 6, 8, 3.5, 1, 8),\n             y = c(3, 2, 1, 3, 1, 1, 2, 3))\n\nDAG.ex4.3 &lt;-\n  dagify(M ~ H + UM,\n         H ~ UH,\n         R ~ H + M + UR,\n         A ~ R + UA,\n         coords = dag_coords.ex4.3)\n\nnode_labels &lt;- c(\n  M = 'bold(M)', \n  H = 'bold(H)', \n  A = 'bold(A)', \n  R = 'bold(R)', \n  UA = 'bold(UA)', \n  UH = 'bold(UH)', \n  UM = 'bold(UM)', \n  UR = 'bold(UR)'\n)\n\nggplot(data = DAG.ex4.3, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 8, y = 3, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05, linetype = 'solid') +\n  geom_point(x = 8, y = 3, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 8, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 3.5, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 1, y = 2, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  coord_cartesian(xlim = c(0.5, 8.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\nggplot(data = DAG.ex4.3, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 8.2, y = 3, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05, linetype = '12') +\n  #latent variable\n  geom_point(x = 8, y = 3, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 8, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 3.5, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 1, y = 2, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  #adjusted variable\n  geom_point(x = 6, y = 1, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  coord_cartesian(xlim = c(0.5, 8.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 4.6. Magnified graphical representation via a DAG of the causal structure. The circle represents unobserved variables; the grey-filled box indicates statistical adjustment; the solid light blue line indicates a closed path, and the dashed orange line indicates a partially open path.\n\n\n\nFig. 4.6 shows the magnified DAG with the causes for all variables. Here the causes only affect one variable (i.e., they are not shared) and are circled because they are considered unobserved. Consequently, they are generally omitted in a DAG. This structure implies that, for example, respiratory irritation \\((\\text{R})\\) is a function of microbial aerosol \\((\\text{M})\\), relative humidity \\((\\text{H})\\), and some unobserved influence \\((\\text{u}_\\text{R})\\). This unobserved influence (or set of influences) can be thought of as variations around the expected value of \\(\\text{R}\\) for each \\(\\text{M}\\) and \\(\\text{H}\\).\nFrom Fig. 4.6, it is clear that respiratory irritation \\((\\text{R})\\) is a virtual collider on the path \\(\\text{M} \\rightarrow \\text{R} \\leftarrow \\text{u}_\\text{R}\\). This path is closed (Fig. 4.6 (a)), but conditioning on absence \\(\\text{A}\\) partially opens it (Fig. 4.6 (b)). This happens because \\(\\text{A}\\) is a descendant of \\(\\text{R}\\), and adjusting for \\(\\text{A}\\) is similar to adjusting for \\(\\text{R}\\). Consequently, the non-causal path \\(\\text{M} \\rightarrow \\text{R} \\leftarrow \\text{u}_\\text{R}\\) is now partially open, creating a spurious association between \\(\\text{M}\\) and \\(\\text{u}_\\text{R}\\) that distorts the estimate of the causal effect of \\(\\text{M}\\) on \\(\\text{R}\\). This is why the estimates from Model.1 are biased. Following the backdoor criterion, we should not adjust for any descendant of the cause of interest. Since Model.2 does not include absence, which is a descendant of microbial aerosol (i.e., \\(\\text{M} \\rightarrow \\text{R} \\rightarrow \\text{A}\\)), the estimates from this model are unbiased.\nBayesian framework\n\n#Fit the linear regression model with M, H, A and R (Model 1)\nex4_Model.1 &lt;- stan_glm(formula = R ~ M + H + A,\n                        family = gaussian(),\n                        data = ex4_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0, 0), scale = c(6, 1, 4)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 0, scale = 2),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex4_Model.1\n\nstan_glm\n family:       gaussian [identity]\n formula:      R ~ M + H + A\n observations: 5000\n predictors:   4\n------\n            Median MAD_SD\n(Intercept) -2.9    0.1  \nMhigh        1.4    0.1  \nHlow         0.3    0.1  \nAyes         4.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.0    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with M, H and R (Model 2)\nex4_Model.2 &lt;- stan_glm(formula = R ~ M + H,\n                        family = gaussian(),\n                        data = ex4_sample.random,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(6, 1)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 0, scale = 2),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility \n#View of the model\nex4_Model.2\n\nstan_glm\n family:       gaussian [identity]\n formula:      R ~ M + H\n observations: 5000\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) -1.0    0.1  \nMhigh        2.9    0.1  \nHlow         0.5    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.9    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nIn Bayesian analysis, there are important diagnostics that have to be carried out in order to assess the convergence and efficiency of the Markov Chains. This is done by using the monitor() function which computes summaries of MCMC (Markov Chain Monte Carlo) draws and monitor convergence. Specifically, we will look at Rhat, Bulk_ESS and Tail_ESS metrics.\n\n#Diagnostics for model 1\nmonitor(ex4_Model.1$stanfit)  \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       -3.1     -2.9     -2.8     -2.9 0.1     1     8175     7548\nMhigh              1.2      1.4      1.5      1.4 0.1     1     7316     7632\nHlow               0.2      0.3      0.5      0.3 0.1     1     7483     8012\nAyes               4.5      4.6      4.7      4.6 0.1     1    11050     7932\nsigma              2.0      2.0      2.0      2.0 0.0     1    12676     9141\nmean_PPD           0.7      0.8      0.9      0.8 0.0     1    12106    11301\nlog-posterior -10602.1 -10598.7 -10597.1 -10599.0 1.6     1     5391     7209\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 2\nmonitor(ex4_Model.2$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       -1.2     -1.0     -0.8     -1.0 0.1     1     6384     7117\nMhigh              2.7      2.9      3.1      2.9 0.1     1     6823     7134\nHlow               0.3      0.5      0.7      0.5 0.1     1     7009     7386\nsigma              2.9      2.9      3.0      2.9 0.0     1     9718     8191\nmean_PPD           0.7      0.8      0.9      0.8 0.1     1    10639    10331\nlog-posterior -12501.9 -12498.9 -12497.6 -12499.2 1.4     1     5257     6747\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\nRhat is a metric used to assess the convergence of Markov Chain Monte Carlo (MCMC) simulations. It helps determine if the MCMC chains have adequately explored the target posterior distribution. Specifically, it compares the between- and within-chain estimates for model parameters: If chains have not mixed well (i.e., the between- and within-chain estimates do not agree), R-hat is larger than 1. A general rule of thumb is to use the sample only if R-hat is less than 1.05; a larger value suggests that the chains have not mixed well, and the results might not be reliable. In our two models, all Rhat are equal to 1 indicating that the chains have mixed well and have adequately explored the target posterior distribution.\nBulk_ESS and Tail_ESS stand for ‘Bulk Effective Sample Size’ and ‘Tail Effective Sample Size,’ respectively. Since MCMC samples are not truly independent (they are correlated), these metrics assess the sampling efficiencies, that is, they help evaluate how efficiently the MCMC sampler is exploring the parameter space.\n\n\nBulk_ESS is a useful measure for sampling efficiency in the bulk (center) of the distribution (e.g., efficiency of mean and median estimates);\n\nTail_ESS is a useful measure for sampling efficiency in the tails of the distribution (e.g., efficiency of variance and tail quantile estimates). A general rule of thumb is that both Bulk-ESS and Tail-ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable. In our two models, all Bulk-ESS and Tail-ESS are well above 400 (i.e., 100 multiplied by 4, the number of chains we used) indicating that estimates of posterior quantiles are reliable.\n\nSince we have established that the posteriors are reliable, we can now explore the model estimates. The estimated coefficients for the two models are then plotted in Fig. 4.7.\n\nCode#Extract draws from model 1 \npost_ex4_Model.1 &lt;-\n  ex4_Model.1 %&gt;% \n  spread_draws(Mhigh) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex4_Model.2 &lt;-\n  ex4_Model.2 %&gt;% \n  spread_draws(Mhigh) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.2') #add a new column to specify that the model\n\n#Combine draws\nplot.post &lt;- rbind(post_ex4_Model.1, post_ex4_Model.2)\n\n# Plot\nplot.post  %&gt;%\n  ggplot(aes(y = model, x = Mhigh)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = b_micr.resp, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     limits = c(0.5, 3.5)) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 4.7. Posterior distribution of the microbial aerosol coefficient for the two models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.\n\n\n\n\nFig. 4.7 shows the estimates (mean and 95% HDI) of the coefficient for microbial aerosol for Model.1, and Model.2.\nFor Model.1 we found a positive coefficient between microbial aerosol \\((\\text{M})\\) and respiratory irritation \\((\\text{R})\\) (mean = 1.372, 95% HDI [1.203, 1.533]). The estimated causal effect is biased, leading to the wrong conclusion that changing microbial aerosol from low to high causes an increase in lung infection by 1.372 units, on average.\nFor Model.2 we found a positive coefficient between microbial aerosol \\((\\text{M})\\) and respiratory irritation \\((\\text{R})\\) (mean = 2.875, 95% HDI [2.640, 3.101]). The estimated causal effect is unbiased, leading to the correct conclusion that changing microbial aerosol from low to high causes an increase in lung infection by 2.875 units, on average.\nImportantly, for both Model.1 and Model.1, the 95% HDI is the range of parameter values within which the most credible 95% of the posterior distribution falls. Unlike a frequentist confidence interval, the Bayesian 95% HDI has a direct probabilistic meaning: every point inside the HDI has a higher probability density than any point outside the interval. Therefore, given the model, the prior and the data, we can say that there is a 95% probability that the data-generating parameter (i.e., b_micr.resp = 3) lies within the HDI. However, since Model.1 leads to a biased estimate, we will reach the wrong conclusion by stating that there is a 95% probability that the data-generating parameter lies within the [1.203, 1.533] interval. This probability is 0%.\nAs expected, adjusting for relative humidity \\((\\text{H})\\) (i.e., using Model.2) leads to the correct estimate of the total average causal effect of microbial aerosol \\((\\text{M})\\) on respiratory irritation \\((\\text{R})\\). However, when adjusting for both relative humidity \\((\\text{H})\\) and absence \\((\\text{A})\\) (i.e., using Model.1) the estimates are biased. This happened because absence \\((\\text{A})\\) is a descendant of respiratory irritation \\((\\text{R})\\), which is a collider. Adjusting for a descendant of a collider is akin to adjusting for the collider itself, and, therefore, can introduce a similar bias.\nTo better explain this bias, we can plot the magnified DAG that shows that causes of all variables.\n\nCodedag_coords.ex4.3 &lt;-\n  data.frame(name = c('M', 'H', 'A', 'R', 'UA', 'UH', 'UM', 'UR'),\n             x = c(1, 3.5, 6, 6, 8, 3.5, 1, 8),\n             y = c(3, 2, 1, 3, 1, 1, 2, 3))\n\nDAG.ex4.3 &lt;-\n  dagify(M ~ H + UM,\n         H ~ UH,\n         R ~ H + M + UR,\n         A ~ R + UA,\n         coords = dag_coords.ex4.3)\n\nnode_labels &lt;- c(\n  M = 'bold(M)', \n  H = 'bold(H)', \n  A = 'bold(A)', \n  R = 'bold(R)', \n  UA = 'bold(UA)', \n  UH = 'bold(UH)', \n  UM = 'bold(UM)', \n  UR = 'bold(UR)'\n)\n\nggplot(data = DAG.ex4.3, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 8, y = 3, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#0072B2', alpha = 0.05, linetype = 'solid') +\n  geom_point(x = 8, y = 3, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 8, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 3.5, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 1, y = 2, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  coord_cartesian(xlim = c(0.5, 8.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\nggplot(data = DAG.ex4.3, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 8.2, y = 3, yend = 3,\n               linewidth = 8, lineend = 'square', colour = '#D55E00', alpha = 0.05, linetype = '12') +\n  #latent variable\n  geom_point(x = 8, y = 3, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 8, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 3.5, y = 1, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  geom_point(x = 1, y = 2, shape = 1, size = 18, stroke = 0.9, color = 'black') +\n  #adjusted variable\n  geom_point(x = 6, y = 1, shape = 22, size = 14, stroke = 0.9, fill = 'grey80', colour = 'black') +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  coord_cartesian(xlim = c(0.5, 8.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 4.8. Magnified graphical representation via a DAG of the causal structure. The circle represents unobserved variables; the grey-filled box indicates statistical adjustment; the solid light blue line indicates a closed path, and the dashed orange line indicates a partially open path.\n\n\n\nFig. 4.8 shows the magnified DAG with the causes for all variables. Here the causes only affect one variable (i.e., they are not shared) and are circled because they are considered unobserved. Consequently, they are generally omitted in a DAG. This structure implies that, for example, respiratory irritation \\((\\text{R})\\) is a function of microbial aerosol \\((\\text{M})\\), relative humidity \\((\\text{H})\\), and some unobserved influence \\((\\text{u}_\\text{R})\\). This unobserved influence (or set of influences) can be thought of as variations around the expected value of \\(\\text{R}\\) for each \\(\\text{M}\\) and \\(\\text{H}\\).\nFrom Fig. 4.6, it is clear that respiratory irritation \\((\\text{R})\\) is a virtual collider on the path \\(\\text{M} \\rightarrow \\text{R} \\leftarrow \\text{u}_\\text{R}\\). This path is closed (Fig. 4.8 (a)), but conditioning on absence \\(\\text{A}\\) partially opens it (Fig. 4.8 (b)). This happens because \\(\\text{A}\\) is a descendant of \\(\\text{R}\\), and adjusting for \\(\\text{A}\\) is similar to adjusting for \\(\\text{R}\\). Consequently, the non-causal path \\(\\text{M} \\rightarrow \\text{R} \\leftarrow \\text{u}_\\text{R}\\) is now partially open, creating a spurious association between \\(\\text{M}\\) and \\(\\text{u}_\\text{R}\\) that distorts the estimate of the causal effect of \\(\\text{M}\\) on \\(\\text{R}\\). This is why the estimates from Model.1 are biased. Following the backdoor criterion, we should not adjust for any descendant of the cause of interest. Since Model.2 does not include absence, which is a descendant of microbial aerosol (i.e., \\(\\text{M} \\rightarrow \\text{R} \\rightarrow \\text{A}\\)), the estimates from this model are unbiased.",
    "crumbs": [
      "Case-control bias",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex4_simulation.html#session-info",
    "href": "ex4_simulation.html#session-info",
    "title": "4  Simulation example",
    "section": "Session info",
    "text": "Session info\nVersion information about R, the OS and attached or loaded packages.\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 26200)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rstan_2.32.7        StanHeaders_2.32.10 gt_1.1.0           \n [4] lubridate_1.9.4     forcats_1.0.1       stringr_1.6.0      \n [7] dplyr_1.1.4         purrr_1.2.0         readr_2.1.6        \n[10] tidyr_1.3.1         tibble_3.3.0        ggplot2_4.0.1      \n[13] tidyverse_2.0.0     ggdist_3.3.3        tidybayes_3.0.7    \n[16] rstanarm_2.32.2     Rcpp_1.1.0          dagitty_0.3-4      \n[19] ggdag_0.2.13       \n\nloaded via a namespace (and not attached):\n  [1] backports_1.5.0       plyr_1.8.9            igraph_2.2.1         \n  [4] splines_4.2.3         svUnit_1.0.8          crosstalk_1.2.2      \n  [7] rstantools_2.5.0      inline_0.3.21         digest_0.6.38        \n [10] htmltools_0.5.8.1     viridis_0.6.5         magrittr_2.0.4       \n [13] checkmate_2.3.3       memoise_2.0.1         tzdb_0.5.0           \n [16] graphlayouts_1.2.2    RcppParallel_5.1.11-1 matrixStats_1.5.0    \n [19] xts_0.14.1            timechange_0.3.0      colorspace_2.1-2     \n [22] ggrepel_0.9.6         rbibutils_2.4         xfun_0.54            \n [25] jsonlite_2.0.0        lme4_1.1-37           survival_3.8-3       \n [28] zoo_1.8-14            glue_1.8.0            reformulas_0.4.2     \n [31] polyclip_1.10-7       gtable_0.3.6          V8_8.0.1             \n [34] distributional_0.5.0  car_3.1-3             pkgbuild_1.4.8       \n [37] abind_1.4-8           scales_1.4.0          miniUI_0.1.2         \n [40] viridisLite_0.4.2     xtable_1.8-4          Formula_1.2-5        \n [43] stats4_4.2.3          DT_0.34.0             htmlwidgets_1.6.4    \n [46] threejs_0.3.4         arrayhelpers_1.1-0    RColorBrewer_1.1-3   \n [49] posterior_1.6.1       pkgconfig_2.0.3       loo_2.8.0            \n [52] farver_2.1.2          sass_0.4.10           utf8_1.2.6           \n [55] tidyselect_1.2.1      labeling_0.4.3        rlang_1.1.6          \n [58] reshape2_1.4.5        later_1.4.4           tools_4.2.3          \n [61] cachem_1.1.0          cli_3.6.5             generics_0.1.4       \n [64] evaluate_1.0.5        fastmap_1.2.0         yaml_2.3.10          \n [67] knitr_1.50            fs_1.6.6              tidygraph_1.3.1      \n [70] ggraph_2.2.2          nlme_3.1-168          mime_0.13            \n [73] xml2_1.5.0            compiler_4.2.3        bayesplot_1.14.0     \n [76] shinythemes_1.2.0     rstudioapi_0.17.1     curl_7.0.0           \n [79] tweenr_2.0.3          stringi_1.8.7         lattice_0.22-7       \n [82] Matrix_1.6-5          nloptr_2.2.1          markdown_2.0         \n [85] shinyjs_2.1.0         tensorA_0.36.2.1      vctrs_0.6.5          \n [88] pillar_1.11.1         lifecycle_1.0.4       Rdpack_2.6.4         \n [91] httpuv_1.6.16         QuickJSR_1.8.1        R6_2.6.1             \n [94] promises_1.5.0        gridExtra_2.3         codetools_0.2-20     \n [97] boot_1.3-32           colourpicker_1.3.0    MASS_7.3-58.2        \n[100] gtools_3.9.5          withr_3.0.2           shinystan_2.6.0      \n[103] parallel_4.2.3        hms_1.1.4             grid_4.2.3           \n[106] coda_0.19-4.1         minqa_1.2.8           rmarkdown_2.30       \n[109] S7_0.2.1              carData_3.0-5         otel_0.2.0           \n[112] ggforce_0.5.0         shiny_1.11.1          base64enc_0.1-3      \n[115] dygraphs_1.1.1.6     \n\n\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Case-control bias",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex5_simulation.html",
    "href": "ex5_simulation.html",
    "title": "5  Simulation example",
    "section": "",
    "text": "5.1 Data-generating process\nThe data-generating process is described via the directed acyclic graph (DAG) in Fig. 5.1. In this DAG, acoustic sensation \\((\\text{C})\\) is influenced directly by noise \\((\\text{N})\\), biological sex \\((\\text{S})\\) and age \\((\\text{A})\\). Participation \\((\\text{P})\\) in the survey response is affected by biological sex \\((\\text{S})\\) and age \\((\\text{A})\\).\nCodedag_coords.ex5 &lt;-\n  data.frame(name = c('S', 'N', 'P', 'C', 'A'),\n             x = c(1, 1, 3.5, 3.5, 6),\n             y = c(2, 3, 1, 3, 2))\n\nDAG.ex5 &lt;-\n  dagify(C ~ N + S + A,\n         P ~ S + A,\n         coords = dag_coords.ex5)\n\nnode_labels &lt;- c(\n  S = 'bold(S)', \n  N = 'bold(N)', \n  P = 'bold(P)', \n  C = 'bold(C)', \n  A = 'bold(A)'\n)\n\nggplot(data = DAG.ex5, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_point(x = 3.5, y = 1, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 3.5, y = 0.68, label = 'participation', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 3.3, label = 'acoustic sensation', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 3.3, label = 'noise', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 1.7, label = 'age', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 1.7, label = 'sex', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 5.1. Graphical representation via DAG of the data-generating process. The box around P indicates that the sample is conditioned on participation.\nThe DAG in Fig. 5.1 can be written as:",
    "crumbs": [
      "Selection bias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex5_simulation.html#data-generating-process",
    "href": "ex5_simulation.html#data-generating-process",
    "title": "5  Simulation example",
    "section": "",
    "text": "\\(C \\sim f_{C}(N, S, A)\\), read as ‘acoustic sensation \\((\\text{C})\\) is some function of noise \\((\\text{N})\\), biological sex \\((\\text{S})\\) and age \\((\\text{A})\\)’.\n\n\\(P \\sim f_{P}(S, A)\\), read as ‘participation \\((\\text{P})\\) is some function of biological sex \\((\\text{S})\\) and age \\((\\text{A})\\)’.",
    "crumbs": [
      "Selection bias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex5_simulation.html#synthetic-data-set",
    "href": "ex5_simulation.html#synthetic-data-set",
    "title": "5  Simulation example",
    "section": "\n5.2 Synthetic data set",
    "text": "5.2 Synthetic data set\nTo generate synthetic data, we defined the custom function data.sim_ex5(). This function takes as inputs the sample size n and generates synthetic data according to the DAG in Fig. 5.1.\n\ndata.sim_ex5 &lt;- function(n) {\n  b_age.conf = 0.3 #direct causal effect of A on C 1 or 0.2\n  b_sex.conf = c(0, -1) #direct causal effect of S on C\n  b_noise.conf = 0.7 #direct causal effect of N on C\n  #Simulate age\n  A &lt;- sample(18:59, size = n, replace = TRUE)\n  #Simulate biological sex\n  S &lt;- factor(sample(c('male', 'female'), size = n, replace = TRUE)) \n  #Simulate noise\n  N &lt;- rnorm(n = n, mean = 45, sd = 4)\n  #Simulate acoustic sensation\n  C &lt;- rnorm(n = n, mean = ifelse(S == 'male', 10 + b_sex.conf[1] + b_noise.conf * N + b_age.conf * A, 10 + b_sex.conf[2] + b_noise.conf * N + b_age.conf * A), sd = 3)\n  #Simulate participation\n  P &lt;- rbinom(n = n, size = 1, prob = plogis(ifelse(S == 'male', +23 -15.6 - 0.25 * A, +23 -13.5 - 0.25 * A)))\n  #Return tibble with simulated values\n  return(tibble(A, S, N, C, P))\n  }\n\nFrom this data generation mechanism, we simulated the target population, which consists of one million observations.\n\nset.seed(2025) #set random number for reproducibility\n#Simulate the population\nex5_population &lt;- data.sim_ex5(n = 1e6)\n#Set biological sex reference category to 'male'\nex5_population$S &lt;- relevel(ex5_population$S, ref = 'male')\n#View the data frame\nex5_population\n\n# A tibble: 1,000,000 × 5\n       A S          N     C     P\n   &lt;int&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1    30 male    46.2  53.3     1\n 2    29 female  47.5  47.4     1\n 3    53 male    44.3  57.7     0\n 4    43 female  40.3  52.4     0\n 5    18 male    37.4  39.9     1\n 6    40 female  36.2  48.1     0\n 7    27 female  46.2  55.9     1\n 8    30 female  48.2  52.7     1\n 9    29 female  47.6  51.3     1\n10    21 female  36.2  42.8     1\n# ℹ 999,990 more rows\n\n\nFrom this population, we obtained two data sets of five thousand observations each: one using simple random sampling and the other using non-random sampling.\nThe first sample (i.e., ex5_sample.random) will use simple random sampling, whereas the second one (i.e., ex5_sample.bias) will use non-random sampling. In this case, we will first select only the observations with P == 1 and subsequently sampling randomly from them. As a result, the sample obtained is biased.\n\nn_sims &lt;- 1e3 #number of data sets to simulate\n\n#Set random number for reproducibility\nset.seed(2025)  \n#Generate a vector of random numbers for reproducibility\nex5_random.seed &lt;- sample(1:1e5, size = n_sims, replace = FALSE)\n\n\nset.seed(ex5_random.seed[1])\n#Sample one data set of 5,000 observations\nex5_sample.random &lt;- \n  ex5_population %&gt;% \n  slice_sample(n = 5e3) #take a simple random sample of size 5,000\n\nset.seed(ex5_random.seed[1])\n#Sample one data set of 5,000 observations\nex5_sample.bias &lt;- \n  ex5_population %&gt;% \n  filter(P == 1) %&gt;% slice_sample(n = 5e3) #take a biased sample of size 5,000 \n\nA summary of descriptive statistics for all variables in the data sets is given below.\n\nsummary(ex5_sample.random)\n\n       A              S              N               C               P        \n Min.   :18.00   male  :2544   Min.   :31.27   Min.   :34.81   Min.   :0.000  \n 1st Qu.:28.00   female:2456   1st Qu.:42.25   1st Qu.:48.85   1st Qu.:0.000  \n Median :39.00                 Median :44.90   Median :52.72   Median :0.000  \n Mean   :38.75                 Mean   :44.92   Mean   :52.62   Mean   :0.373  \n 3rd Qu.:49.00                 3rd Qu.:47.66   3rd Qu.:56.58   3rd Qu.:1.000  \n Max.   :59.00                 Max.   :60.65   Max.   :69.52   Max.   :1.000  \n\nsummary(ex5_sample.bias)\n\n       A              S              N               C               P    \n Min.   :18.00   male  :1869   Min.   :30.84   Min.   :33.79   Min.   :1  \n 1st Qu.:22.00   female:3131   1st Qu.:42.41   1st Qu.:46.10   1st Qu.:1  \n Median :26.00                 Median :45.04   Median :49.16   Median :1  \n Mean   :27.48                 Mean   :45.04   Mean   :49.17   Mean   :1  \n 3rd Qu.:32.00                 3rd Qu.:47.79   3rd Qu.:52.28   3rd Qu.:1  \n Max.   :59.00                 Max.   :62.38   Max.   :65.06   Max.   :1",
    "crumbs": [
      "Selection bias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex5_simulation.html#data-analysis",
    "href": "ex5_simulation.html#data-analysis",
    "title": "5  Simulation example",
    "section": "\n5.3 Data analysis",
    "text": "5.3 Data analysis\nIn this example, the target of our analysis is the total average causal effect, ACE (also known as total average treatment effect, ATE) of biological sex \\((\\text{S})\\) on acoustic sensation \\((\\text{C})\\), which stands for the expected increase of \\(\\text{C}\\) in response to a change from male to female in \\(\\text{S}\\) due to an intervention. The causal effect of interest is visualized in Fig. 5.2.\n\nCodeggplot(data = DAG.ex5, aes(x = x, y = y, xend = xend, yend = yend)) +\n  #visualize causal effect path\n  geom_segment(x = 1, xend = 3.5, y = 2, yend = 3,\n               linewidth = 14, lineend = 'round', colour = '#009E73', alpha = 0.05) +\n  geom_point(x = 3.5, y = 1, shape = 0, size = 12, stroke = 0.9, color = 'black') +\n  geom_dag_text(aes(label = node_labels[name]),\n                parse = TRUE,\n                colour = 'black',\n                size = 10,\n                family = 'mono') +\n  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, 'pt'), type = 'open'),\n                 edge_colour = 'black',\n                 family = 'mono',\n                 fontface = 'bold') +\n  annotate('text', x = 3.5, y = 0.68, label = 'participation', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 3.5, y = 3.3, label = 'acoustic sensation', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 3.3, label = 'noise', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 6, y = 1.7, label = 'age', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  annotate('text', x = 1, y = 1.7, label = 'sex', \n           size = 4, hjust = 0.5, colour = 'grey50') +\n  #causal effect number\n  annotate('text', x = 1.9, y = 2.6, label = '-1', \n           size = 4.5, hjust = 0.5, colour = 'black', parse = TRUE) +\n  coord_cartesian(xlim = c(0.5, 6.5), ylim = c(0.8, 3.2))  +\n  theme_dag()\n\n\n\n\n\n\nFig. 5.2. Graphical representation via DAG of the data-generating process. The box around P indicates that the sample is conditioned on participation. The green line indicates the causal question of interest, and the number on the path indicates the total average causal effect.\n\n\n\n\n\n5.3.1 Identification\nThe first step to answer the causal question of interest is identification. Identification answers a ‘theoretical’ question by determining whether a causal effect can, in principle, be estimated from observed data. The backdoor criterion and its generalization, the adjustment criterion, allow us to understand whether our causal effect of interest can be identified and, if so, which variables we should (or should not) statistically adjust for (i.e., the adjustment set) to estimate the causal effect from the data.\nGiven its simplicity, we will first apply the backdoor criterion to identify valid adjustment sets to estimate the causal effect of interest. If the backdoor criterion is not applicable, we will apply its generalization, the adjustment criterion.\nBackdoor criterion\nApplying the backdoor criterion, we found no backdoor paths since no arrows enter into biological sex. However, we noticed that participation \\((\\text{P})\\) is a descendant of biological sex \\((\\text{S})\\) and is selected. Although selection and adjustment are conceptually distinct, both are forms of conditioning. Since the sample is implicitly conditioned on \\(\\text{P}\\), a descendant of \\(\\text{S}\\), the conditions required to apply the backdoor criterion are not met. We then apply the adjustment criterion.\nAdjustment criterion\nWith the adjustment criterion, conditioning on \\(\\text{P}\\) is allowed because \\(\\text{P}\\) is not on the causal path from \\(\\text{S}\\) to \\(\\text{C}\\), but we also need to check that all the non-causal paths from \\(\\text{S}\\) to \\(\\text{C}\\) are closed. Since participation \\((\\text{P})\\) is a collider (i.e., common effect) of biological sex \\((\\text{S})\\) and age \\((\\text{A})\\), as females and older individuals are more likely to participate in the experiment, the non-causal path \\(\\text{S} \\rightarrow \\text{P} \\leftarrow \\text{A} \\rightarrow \\text{C}\\) is open. As a result, there is a spurious association between sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\).\nGiven the DAG in Fig. 5.1, we can use the adjustmentSets() function to identify the adjustment set algorithmically. Unfortunately, adjustmentSets() does not allow finding the adjustment set(s) when there is selection bias (i.e., a variable is conditioned on in the DAG). Nevertheless, for this example there is a workaround. We can still use adjustmentSets() to get the adjustment set(s) but then only select the one(s) that includes \\(\\text{P}\\).\n\nadjustmentSets(DAG.ex5,\n               exposure = 'S', #biological sex\n               outcome = 'C', #acoustic sensation\n               type = 'all', \n               effect = 'total', \n               max.results = Inf)\n\n {}\n{ A }\n{ N }\n{ A, N }\n{ A, P }\n{ A, N, P }\n\n\nThe resulting adjustment sets that include participation \\((\\text{P})\\) are only two: adjust for age \\((\\text{A})\\) or adjust for age \\((\\text{A})\\) and noise \\((\\text{N})\\); failing to do so will lead to bias.\n\n5.3.2 Estimation\nFollowing the identification step is the estimation step. This step addresses a statistical question by determining how the causal effect identified in the previous step can be estimated. To perform this step, we used a parametric (model-based) estimator, specifically, linear regression. This was possible because we designed the illustrative examples to be simple and with a linear relationship between the variables. This way, we limited the complexity of the examples themselves and shifted the focus to the application of the backdoor criterion to define ‘correct’ adjustment sets.\nFor transparency and understanding, all (implicit) assumptions used for this illustrative example are (explicitly) provided in Table 5.1.\n\n\n\nTable 5.1. Summary description of the simulation example\n\n\n\n\n\n\n\n\n\n\n\nResearch question\nTotal average causal effect (ACE) of biological sex (S) on acoustic sensation (C).\n\n\nAssumptions\nLimited random variability: large sample size.\n\n\n\nIndependence of observations: each observation represents independent bits of information.\n\n\n\nNo confounding: the DAG includes all shared causes among the variables.\n\n\n\nNo model error: perfect functional form specification.\n\n\n\nNo measurement error: all variables are measured perfectly.\n\n\nVariables\nNoise (N): continuous variable [unit: dB(A)]\n\n\n\nBiological sex (S): categorical variable ['male'; 'female']\n\n\n\nAge (A): discrete (count) variable [unit: years]\n\n\n\nAcoustic sensation (C): continuous variable [unit: -]\n\n\n\n\n\n\n\n\n\nTo carry out the estimation step, we utilized linear regression within both the frequentist and Bayesian frameworks. Specifically, we will run three regression models:\n\n\nModel.1 will include only biological sex \\((\\text{S})\\) as predictor;\n\nModel.2 will include biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) as predictors.\n\nModel.3 will include biological sex \\((\\text{S})\\) and noise \\((\\text{N})\\) as predictors.\n\nThe results of the fitted statistical models (i.e., Model.1, Model.2 and Model.3) are presented here.\nFrequentist framework\n\n#Fit the linear regression model with S and C (Model 1)\nex5_Model.1 &lt;-\n  lm(formula = C ~ S,\n     data = ex5_sample.bias)\n#View of the model summary\nsummary(ex5_Model.1)\n\n\nCall:\nlm(formula = C ~ S, data = ex5_sample.bias)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4275  -3.0739  -0.0302   3.1096  15.9110 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.21609    0.10604 464.114   &lt;2e-16 ***\nSfemale     -0.07153    0.13401  -0.534    0.594    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.584 on 4998 degrees of freedom\nMultiple R-squared:  5.701e-05, Adjusted R-squared:  -0.0001431 \nF-statistic: 0.2849 on 1 and 4998 DF,  p-value: 0.5935\n\n\n\n#Fit the linear regression model with S, A and C (Model 2)\nex5_Model.2 &lt;-\n  lm(formula = C ~ S + A,\n     data = ex5_sample.bias)\n#View of the model summary\nsummary(ex5_Model.2)\n\n\nCall:\nlm(formula = C ~ S + A, data = ex5_sample.bias)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.456  -2.763   0.013   2.757  15.015 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.956227   0.231256 181.428  &lt; 2e-16 ***\nSfemale     -1.001084   0.123454  -8.109 6.37e-16 ***\nA            0.285392   0.008283  34.457  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.121 on 4997 degrees of freedom\nMultiple R-squared:  0.192, Adjusted R-squared:  0.1917 \nF-statistic: 593.8 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\n\n#Fit the linear regression model with S, N and C (Model 3)\nex5_Model.3 &lt;-\n  lm(formula = C ~ S + N,\n     data = ex5_sample.bias)\n#View of the model summary\nsummary(ex5_Model.3)\n\n\nCall:\nlm(formula = C ~ S + N, data = ex5_sample.bias)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.545  -2.523  -0.049   2.505  14.146 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.18180    0.59013  30.810   &lt;2e-16 ***\nSfemale     -0.01190    0.10713  -0.111    0.912    \nN            0.68816    0.01295  53.140   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.665 on 4997 degrees of freedom\nMultiple R-squared:  0.3611,    Adjusted R-squared:  0.3608 \nF-statistic:  1412 on 2 and 4997 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated coefficients for the three models are then plotted in Fig. 5.3.\n\nCodeb_sex.conf = c(0, -1)\n\ndata.frame(model = c('Model.1', 'Model.2', 'Model.3'),\n           estimate = c(coef(ex5_Model.1)['Sfemale'], \n                        coef(ex5_Model.2)['Sfemale'],\n                        coef(ex5_Model.3)['Sfemale']),\n           lower.95.CI = c(confint(ex5_Model.1, level = 0.95, type = 'Wald')['Sfemale', 1], \n                           confint(ex5_Model.2, level = 0.95, type = 'Wald')['Sfemale', 1],\n                           confint(ex5_Model.3, level = 0.95, type = 'Wald')['Sfemale', 1]),\n           upper.95.CI = c(confint(ex5_Model.1, level = 0.95, type = 'Wald')['Sfemale', 2], \n                           confint(ex5_Model.2, level = 0.95, type = 'Wald')['Sfemale', 2],\n                           confint(ex5_Model.3, level = 0.95, type = 'Wald')['Sfemale', 2])) %&gt;%\n  \nggplot(aes(x = estimate, y = model, xmin = lower.95.CI, xmax = upper.95.CI)) + \n  geom_vline(xintercept = b_sex.conf[2], alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 2, fill = 'white', stroke = 1) +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -10, to = 10, by = 0.25),\n                     limits = c(-1.55, 0.55)\n                     ) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 5.3. Estimates of the biological sex coefficient for the three models. The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nFig. 5.3 shows the estimates (point estimate and 95% confidence interval) of the coefficient for biological sex for Model.1, Model.2and Model.3.\nFor Model.1 we found a negative coefficient between biological sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\) equal to -0.072 with 95% confidence interval (CI) [-0.334, 0.191]. Since the 95% CI includes zero, the regression coefficient is not statistically significantly different from zero at the 0.05 level (p-value = 0.594). The observed data are consistent with zero, but also consistent with other non-zero values (i.e., all numbers within the 95% confidence interval). Additionally, since the 95% CI does not include the data-generating parameter for sex (i.e., b_sex.conf = -1), we can deduce that the estimated coefficient for sex is statistically significantly different from -1 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for Sfemale = -1\ncar::linearHypothesis(ex5_Model.1, 'Sfemale = -1') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nSfemale = - 1\n\nModel 1: restricted model\nModel 2: C ~ S\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   4999 106053                                  \n2   4998 105044  1    1008.9 48.004 4.791e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 4.79e-12, which indicates that we can reject the null hypothesis (i.e., Sfemale = -1) at the 0.05 level. This result suggests that the regression coefficient for sex (i.e., -0.072) is statistically significantly different from -1. Since the estimated causal effect from Model.1 is biased, we would erroneously conclude that changing biological sex from male to female causes a decrease in acoustic sensation by -0.072 units (95% CI [-0.334, 0.191]).\nFor Model.2 we found a negative coefficient between biological sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\) equal to -1.001 with 95% CI [-1.243, -0.759]. Since the 95% CI excludes zero, the regression coefficient is statistically significantly different from zero at the 0.05 level (p-value = 6.37e-16). Additionally, since the 95% CI includes the data-generating parameter for sex (i.e., b_sex.conf = -1), we can deduce that the estimated coefficient for sex is not statistically significantly different from -1 at the 0.05 level (although this will be the case for all numbers within the 95% confidence interval). We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for Sfemale = -1\ncar::linearHypothesis(ex5_Model.2, 'Sfemale = -1') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nSfemale = - 1\n\nModel 1: restricted model\nModel 2: C ~ S + A\n\n  Res.Df   RSS Df Sum of Sq     F Pr(&gt;F)\n1   4998 84877                          \n2   4997 84877  1 0.0013098 1e-04  0.993\n\n\nThe resulting p-value is 0.993, which indicates that we fail to reject the null hypothesis (i.e., Sfemale = -1) at the 0.05 level. This result suggests that the regression coefficient for sex (i.e., -1.001) is not statistically significantly different from -1. Since the estimated causal effect from Model.2 is unbiased, we would correctly conclude that changing biological sex from male to female causes a decrease in acoustic sensation by -1.001 units (95% CI [-1.243, -0.759]).\nFor Model.3 we found a negative coefficient between biological sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\) equal to -0.012 with 95% CI [-0.222, 0.198]. Since the 95% CI includes zero, the regression coefficient is not statistically significantly different from zero at the 0.05 level (p-value = 0.912). The observed data are consistent with zero, but also consistent with other non-zero values (i.e., all numbers within the 95% confidence interval). Additionally, since the 95% CI does not include the data-generating parameter for sex (i.e., b_sex.conf = -1), we can deduce that the estimated coefficient for sex is statistically significantly different from -1 at the 0.05 level. We can test this more formally by using the linearHypothesis() function. Specifically,\n\n#Test for Sfemale = -1\ncar::linearHypothesis(ex5_Model.3, 'Sfemale = -1') # car:: access the function from the car package without loading it in the environment \n\n\nLinear hypothesis test:\nSfemale = - 1\n\nModel 1: restricted model\nModel 2: C ~ S + N\n\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   4998 68259                                  \n2   4997 67116  1    1142.5 85.066 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe resulting p-value is 4.16e-20, which indicates that we can reject the null hypothesis (i.e., Sfemale = -1) at the 0.05 level. This result suggests that the regression coefficient for sex (i.e., -0.012) is statistically significantly different from -1. Since the estimated causal effect from Model.1 is biased, we would erroneously conclude that changing biological sex from male to female causes a decrease in acoustic sensation by -0.012 units (95% CI [-0.222, 0.198]).\nImportantly, for Model.1, Model.2 and Model.3, the 95% confidence interval means that if we were to repeat the sampling process and calculate the interval many times, 95% of those calculated intervals would contain the true population parameter. To highlight this, we can repeat the analysis by fitting the two models to one thousand data sets randomly selected from our population.\nThe for-loop shown in the code below performs the following operations. First, sample (using simple random sampling) a data set of 5,000 observations from the target population. Subsequently, perform linear regression using Model.1, Model.2 and Model.3 and store the estimated coefficients for sex, its standard error and 95% confidence interval in the data frame coefs_ex5. This operation is repeated a thousand times, resulting in the data frame coefs_ex5 containing the estimates (point estimate, standard error and confidence interval) of a thousand random samples of size 5,000 using both Model.1, Model.2 and Model.3.\n\nn_model &lt;- c('mod.1', 'mod.2', 'mod.3')     \n\nn_row &lt;- n_sims*length(n_model)\n\n#Create an empty data frame\nempty.df &lt;- data.frame(matrix(NA, nrow = n_row, ncol = 7))\n#Rename the data frame columns\ncolnames(empty.df) &lt;- c('sim.id', 'estimate', 'se', 'CI_2.5', 'CI_97.5',\n                        'model', 'coverage')\n\n#Sample a thousand data sets of 5,000 observations and perform linear regression \ncoefs_ex5 &lt;- empty.df #assign the empty data frame\nk = 1\nfor (i in 1:n_sims){\n  set.seed(ex5_random.seed[i]) #set unique seed for each simulation \n#Sample data set from population   \n  sample.bias &lt;- \n    ex5_population %&gt;% \n    filter(P == 1) %&gt;% \n    slice_sample(n = 5e3) #take a simple random sample of size 5,000\n#Fit models\n  for (j in 1:length(n_model)){\n    if (n_model[j] == 'mod.1'){\n      fit &lt;- lm(formula = C ~ S,\n                data = sample.bias)\n    } else if (n_model[j] == 'mod.2'){\n      fit &lt;- lm(formula = C ~ S + A,\n                data = sample.bias)  \n    } else {\n      fit &lt;- lm(formula = C ~ S + N,\n                data = sample.bias)\n    }  \n#Compile matrix  \n  coefs_ex5[k, 1] &lt;- i #simulation ID\n  coefs_ex5[k, 2] &lt;- coef(fit)['Sfemale'] #point estimate\n  coefs_ex5[k, 3] &lt;- summary(fit)$coef['Sfemale','Std. Error'] #standard error\n  coefs_ex5[k, 4:5] &lt;- confint(fit, level = 0.95, type = 'Wald')['Sfemale', ] #confidence interval (Wald)\n  coefs_ex5[k, 6] &lt;- n_model[j] #sample size\n  k = k + 1\n  }\n}\ncoefs_ex5 &lt;- as_tibble(coefs_ex5)\n#View the data frame\ncoefs_ex5\n\n# A tibble: 3,000 × 7\n   sim.id estimate    se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;   \n 1      1  -0.0715 0.134 -0.334   0.191 mod.1 NA      \n 2      1  -1.00   0.123 -1.24   -0.759 mod.2 NA      \n 3      1  -0.0119 0.107 -0.222   0.198 mod.3 NA      \n 4      2   0.0547 0.136 -0.211   0.321 mod.1 NA      \n 5      2  -0.973  0.125 -1.22   -0.727 mod.2 NA      \n 6      2   0.0312 0.108 -0.180   0.243 mod.3 NA      \n 7      3  -0.0387 0.137 -0.306   0.229 mod.1 NA      \n 8      3  -1.08   0.125 -1.32   -0.831 mod.2 NA      \n 9      3   0.0386 0.108 -0.173   0.250 mod.3 NA      \n10      4  -0.0152 0.136 -0.282   0.252 mod.1 NA      \n# ℹ 2,990 more rows\n\n\nThe coverage is defined by setting its value to 1 if the confidence interval overlaps the data-generating parameter for sex (i.e., b_sex.conf = -1) and 0 otherwise.\n\n#Calculate coverage\ncoefs_ex5 &lt;-\n  coefs_ex5 %&gt;%\n  mutate(coverage = case_when(CI_2.5 &gt; b_sex.conf[2] | CI_97.5 &lt; b_sex.conf[2] ~ 0,\n                              CI_2.5 &lt;= b_sex.conf[2] & CI_97.5 &gt;= b_sex.conf[2] ~ 1,\n                              .default = NA))\n#View the data frame\ncoefs_ex5\n\n# A tibble: 3,000 × 7\n   sim.id estimate    se CI_2.5 CI_97.5 model coverage\n    &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1      1  -0.0715 0.134 -0.334   0.191 mod.1        0\n 2      1  -1.00   0.123 -1.24   -0.759 mod.2        1\n 3      1  -0.0119 0.107 -0.222   0.198 mod.3        0\n 4      2   0.0547 0.136 -0.211   0.321 mod.1        0\n 5      2  -0.973  0.125 -1.22   -0.727 mod.2        1\n 6      2   0.0312 0.108 -0.180   0.243 mod.3        0\n 7      3  -0.0387 0.137 -0.306   0.229 mod.1        0\n 8      3  -1.08   0.125 -1.32   -0.831 mod.2        1\n 9      3   0.0386 0.108 -0.173   0.250 mod.3        0\n10      4  -0.0152 0.136 -0.282   0.252 mod.1        0\n# ℹ 2,990 more rows\n\n\nThe results are then plotted in Fig. 5.4.\n\nCodemodel_names &lt;- c('mod.1' = 'Model.1', \n                 'mod.2' = 'Model.2',\n                 'mod.3' = 'Model.3')\n\nggplot(data = subset(coefs_ex5, sim.id &lt;= 2e2), aes(x = sim.id, y = estimate, ymin = CI_2.5, ymax = CI_97.5, colour = as.factor(coverage))) + \n  geom_hline(yintercept = b_sex.conf[2], alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  geom_linerange() +\n  geom_point(shape = 21, size = 1, fill = 'white', stroke = 0.5) +\n  scale_colour_manual('', \n                      values = c('black', '#FF4040'),\n                      breaks = c('1','0')) +\n  scale_y_continuous('Estimate', \n                     breaks = seq(from = -10, to = 10, by = 0.5),\n                     limits = c(-1.75, 1)) +\n  scale_x_continuous('Simulation ID') +\n  facet_wrap(model~., \n             labeller = labeller(model = model_names),\n             nrow = 3,\n             scales = 'fixed') +\n  theme(legend.position = 'none', \n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 5.4. Estimates of the biological sex coefficient (only the first hundreds of the thousand simulations are shown). The white dots represent the point estimate, and the black lines represent the 95% confidence intervals. In red are highlighted the confidence intervals that do not overlap the parameter used to generate the data (dashed blue line).\n\n\n\n\nFig. 5.4 shows the first 200 estimates (point estimate and confidence interval) of the coefficient for biological sex for Model.1, Model.2 and Model.3. If all the thousand simulations are considered, the frequency of the coverage of the calculated confidence intervals (i.e., how many times the confidence intervals overlap the data-generating parameter) is 0.0%, 95.7% and 0.0% for Model.1, Model.2 and Model.3, respectively. Since the estimates of the causal effect for Model.1 and Model.3 are biased, the calculated confidence intervals for these models do not have the expected coverage. In fact, confidence intervals only quantify the uncertainty due to random error (i.e., sample variability), not systematic error (i.e., bias). Instead, the estimates from Model.2 are unbiased and the calculated 95% confidence intervals have the expected coverage (i.e., overlap the data-generating parameter 95% of the times).\nWe can also visualize all the 1,000 estimates of the coefficient for biological sex for Model.1, Model.2 and Model.3 (the estimates were shown as white dots in Fig. 5.4).\n\nCodeggplot(data = coefs_ex5, aes(x = estimate, y = ifelse(after_stat(count) &gt; 0, after_stat(count), NA))) +\n  geom_histogram(binwidth = 0.05, fill = 'white', colour = 'grey50') +\n  geom_vline(xintercept = b_sex.conf[2], colour = 'blue', linetype = 'dashed') +\n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -10, to = 10, by = 0.25),\n                     limits = c(-1.55, 0.55)) +\n  facet_grid(model~., \n             labeller = labeller(model = model_names),\n             scales = 'fixed') +\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 5.5. Histogram of the 1,000 estimates of the biological sex coefficient for the three models. The blue dashed line represents the parameter used to generate the data.\n\n\n\n\nIn Fig. 5.5, the estimated coefficients are visualized through a histogram. Here, we can see that the estimates for Model.1 and Model.3are not clustered around the data-generating parameter (blue dashed line) having a mean estimate of 0.024 (with 0.135 standard deviation) and 0.027 (with 0.104 standard deviation), respectively. In contrast, the estimates for Model.2 having a mean estimate of -0.998 (with 0.122 standard deviation) are centered around the data-generating parameter.\nAs expected, the inclusion of age \\((\\text{A})\\) in the regression model (i.e., using Model.2) leads to the correct estimate of the total average causal effect, while its exclusion (i.e., using Model.1 and Model.3) leads to bias.\nThe bias occurs because there is selection bias. Generally, selection bias takes place when the participants selected for a study are not representative of the larger population intended to be analyzed. In our example, participation is not random but influenced by variables that also affect the outcome, specifically, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\). As a result, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) are associated in the biased sample, while they are not in the population (and consequently the random sample).\nTo better explain this bias, we visualize the relationship between biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) in the biased and random sample.\n\nCodetruth.data.sim_ex5 &lt;- function(n) {\n  b_age.conf = 0.3 #direct causal effect of A on C 1 or 0.2\n  b_sex.conf = c(0, -1) #direct causal effect of S on C\n  b_noise.conf = 0.7 #direct causal effect of N on C\n\n  A &lt;- rep(18:59, length.out = n)\n  S &lt;- c(rep('male', times = 84/2), rep('female', times = 84/2))\n  N &lt;- 45\n  #Simulate acoustic sensation\n  C &lt;- ifelse(S == 'male', 10 + b_sex.conf[1] + b_noise.conf * N + b_age.conf * A, 10 + b_sex.conf[2] + b_noise.conf * N + b_age.conf * A)\n  #Return tibble with simulated values\n  return(tibble(A, S, N, C))\n  }\n\ntruth &lt;- truth.data.sim_ex5(84) %&gt;%\n                    mutate(sample = 'population')\n\n\nex5_plot &lt;- rbind(ex5_sample.random %&gt;%\n                    mutate(sample = 'random'),\n                  ex5_sample.bias %&gt;%\n                    mutate(sample = 'biased'))\n\n\n#Plot\nmean_val &lt;- \n  ex5_plot %&gt;% \n  group_by(S, sample) %&gt;% \n  summarise(A = mean(A)) \n\nline_val &lt;- \n  ex5_plot %&gt;% \n  group_by(S, sample) %&gt;% \n  summarise(A = mean(A)) %&gt;%\n  pivot_wider(names_from = S, values_from = A)\n\ntruth_line &lt;-\n  truth %&gt;% \n  summarise(A = mean(A))\n\nex5_plot %&gt;%\n  ggplot(aes(x = as.numeric(S), y = A)) +\n  geom_jitter(aes(fill = S, colour = S), shape = 19, size = 2, alpha = 0.1, stroke = NA, width = 0.2) + \n  geom_hline(data = truth_line, mapping = aes(yintercept = A , linetype = 'linea'), colour = 'black') +\n  geom_segment(data = line_val, aes(x = 1, y = male, xend = 2, yend = female), colour = 'black', linetype = 'solid', linewidth = 0.5) +\n  geom_point(data = mean_val, aes(fill = S, shape = '2'), colour = 'black', size = 3, stroke = 0.75) + #23\n  scale_colour_manual('',\n                      breaks = c('male', 'female', 'black'),\n                      values = c('#0072B2', '#D55E00', 'black')) +\n  scale_fill_manual('',\n                    breaks = c('male', 'female', 'black'),\n                    values = c('#0072B2', '#D55E00', NA)) +\n  scale_shape_manual('',\n                     breaks = c('2'),\n                     labels = c('mean age (sample)'),\n                     values = c(23)) +\n  scale_linetype_manual('',\n                        values = c('dashed'),\n                        breaks = c('linea'),\n                        labels = c('mean age (population)')) +\n  scale_x_continuous('Biological sex',\n                     breaks = c(1, 2),\n                     labels = c('male', 'female')) + \n  scale_y_continuous('Age', \n                     breaks = seq(from = 0, to = 100, by = 10)) +\n  guides(colour = guide_legend(override.aes = list(alpha = 1, fill = NA, size = 2), order = 1),\n         fill = 'none',\n         shape = guide_legend(override.aes = list(alpha = 1, size = 2), order = 2),\n         linetype = guide_legend(override.aes = list(alpha = 1, size = 2), order = 3)\n         ) +\n  facet_grid(.~sample) + \n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 5.6. Visualization of the relationship between biological sex and age in the biased and random sample.\n\n\n\n\nFig. 5.6 shows the relationship between biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) in the biased and random sample. The horizontal dashed black line represents the average age in the population (i.e., 38.50 years) for both males and females. In the random sample, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) are not associated and the average age for males (i.e., 38.66 years) and females (i.e., 38.85 years) are similar to one another with a difference of 0.20 years. However, due to selection bias, the average age for males and females in the biased sample is clearly different from the population average. Additionally, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) are associated because females are more likely to be selected than males, and older people are less likely to be selected than younger people. As a result, females in the biased sample are, on average, older than males (average of 28.70 years for females, 25.44 years for males and a difference of 3.26 years).\nWe can test for a difference between the average age of males and females more formally using bootstrapping. We will do so by using the custom functions my_bootstrap() and summary_bootstrap() (see code below).\n\nCode#Perform bootstrapping\nmy_bootstrap &lt;- \n  function(data, variable, group){\n    samples &lt;- tapply(X = data[[variable]], INDEX = data[[group]], FUN = function(x) sample(x, length(x), TRUE))\n    sapply(samples, mean)\n    }\n\n#Summarize the results from the bootstrapping\nsummary_bootstrap &lt;-\n  function(bootstrap){\n    data.frame(t(bootstrap)) %&gt;% \n      mutate(diff_f.m = female - male) %&gt;%\n      mean_qi(.width = 0.95) %&gt;%\n      rownames_to_column() %&gt;%\n      select(-c(.width, .point, .interval)) %&gt;%\n      pivot_longer(cols = !rowname, \n                   names_to = 'variable',\n                   values_to = 'value') %&gt;%\n      select(-rowname) %&gt;%\n      mutate(type = case_when(grepl('.lower', variable) ~ 'CI.lower',\n                              grepl('.upper', variable) ~ 'CI.upper',\n                              .default = 'estimate'),\n             variable = case_when(grepl('female', variable) ~ 'female',\n                                  grepl('male', variable) ~ 'male',\n                                  grepl('diff_f.m', variable) ~ 'diff_f.m',\n                                  .default = NA)) %&gt;%\n      pivot_wider(names_from = 'type',\n                  values_from = 'value')\n    }\n\n\nFirst, we define the number of times to run the my_bootstrap() function. We fixed the number of bootstrap samples equal to 10,000 (i.e., n_bootstrap = 1e4). Subsequently, we run the my_bootstrap() function using as input both the random and biased sample.\n\n#Number of bootstrap samples\nn_bootstrap = 1e4\nset.seed(2025) #for reproducibility\n#Bootstrap for random sample\nbootstrap_sample.random &lt;- \n  replicate(n_bootstrap, my_bootstrap(data = ex5_sample.random, \n                                      variable = 'A', \n                                      group = 'S'))\nset.seed(2025) #for reproducibility\n#Bootstrap for biased sample\nbootstrap_sample.bias &lt;- \n  replicate(n_bootstrap, my_bootstrap(data = ex5_sample.bias, \n                                      variable = 'A', \n                                      group = 'S'))\n\nThe results from the bootstrapping are summarized using the summary_bootstrap() function.\n\n#summarize bootstrap for random sample\nsummary_sample.random &lt;-\n  summary_bootstrap(bootstrap_sample.random)\n#View summary\nsummary_sample.random\n\n# A tibble: 3 × 4\n  variable estimate CI.lower CI.upper\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 male       38.7     38.2     39.1  \n2 female     38.9     38.4     39.3  \n3 diff_f.m    0.199   -0.476    0.883\n\n#summarize bootstrap for biased sample\nsummary_sample.bias &lt;- \n  summary_bootstrap(bootstrap_sample.bias)\n#View summary\nsummary_sample.bias\n\n# A tibble: 3 × 4\n  variable estimate CI.lower CI.upper\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 male        25.4     25.2     25.7 \n2 female      28.7     28.4     29.0 \n3 diff_f.m     3.26     2.88     3.65\n\n\nFor the random sample, the age difference between males and females is 0.20 with 95% CI [-0.48, 0.88]. Since the 95% CI includes zero, the average age of females is not statistically significantly different from the average age of males, at the 0.05 level. Instead, for the biased sample, the age difference between males and females is 3.26 with 95% CI [2.88, 3.65]. Since the 95% CI does not include zero, the average age of females is statistically significantly different from the average age of males, at the 0.05 level.\nHowever, once we adjust for age \\((\\text{A})\\) in the regression model (i.e., using Model.2) the bias disappears. We can visualize this in Fig. 5.7\n\nCodepred_ex5_Model.2 &lt;- cbind(ex5_sample.bias, \n                          predict(ex5_Model.2, interval = 'confidence', level = 0.95))\n\nex5_sample.bias %&gt;%\n  ggplot(aes(x = A, y = C)) +\n  geom_point(aes(colour = S), shape = 19, size = 2, alpha = 0.07, stroke = NA) +\n  geom_ribbon(data = pred_ex5_Model.2, aes(x = A , y = fit, ymin = lwr, ymax = upr, fill = S), alpha = 0.2, show.legend = NA) +\n  geom_line(data = pred_ex5_Model.2, aes(x = A , y = fit, colour = S, linetype = 'sample', linewidth = 'sample')) + \n  geom_line(data = truth, mapping = aes(colour = S, linetype = 'pop', linewidth = 'pop')) +\nscale_colour_manual('',\n                      breaks = c('male', 'female'),\n                      values = c('#0072B2', '#D55E00')) +\n  scale_fill_manual('',\n                      breaks = c('male', 'female'),\n                      values = c('#0072B2', '#D55E00')) +\n  scale_linetype_manual('',\n                        breaks = c('pop', 'sample'),\n                        labels = c('population', 'sample'),\n                        values = c('dashed', 'solid')) +\n  scale_linewidth_manual('',\n                         breaks = c('pop', 'sample'),\n                         labels = c('population', 'sample'),\n                         values = c(1, 0.5)) +\n  scale_x_continuous('Age') + \n  scale_y_continuous('Acoustic sensation', \n                     breaks = seq(from = 0, to = 100, by = 10)) +\n  facet_grid(.~S) +\n  guides(linetype = guide_legend(override.aes = list(alpha = 1, colour = 'black', fill = NA, linewidth = 0.5))) +\n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 5.7. Visualization of the relationship between biological sex and acoustic sensation conditioning on age.\n\n\n\n\nIn Fig. 5.7, we can observe that within each age level the regression lines for the biased sample for males (i.e., solid blue line) and females (i.e., solid orange line) is very similar to the population lines (i.e., dashed lines). Adjusting for age closed the backdoor path leading to the correct estimate of the total average causal effect.\nBayesian framework\n\nClick to expand\n\n#Fit the linear regression model with S and C (Model 1)\nex5_Model.1 &lt;- stan_glm(formula = C ~ S,\n                        family = gaussian(),\n                        data = ex5_sample.bias,\n                        #Prior coefficients\n                        prior = normal(location = c(0), scale = c(2)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 10, scale = 6),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility   \n#View of the model\nex5_Model.1\n\nstan_glm\n family:       gaussian [identity]\n formula:      C ~ S\n observations: 5000\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 49.2    0.1  \nSfemale     -0.1    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 4.6    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with S, A and C (Model 2)\nex5_Model.2 &lt;- stan_glm(formula = C ~ S + A,\n                        family = gaussian(),\n                        data = ex5_sample.bias,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(2, 1)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 10, scale = 6),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex5_Model.2\n\nstan_glm\n family:       gaussian [identity]\n formula:      C ~ S + A\n observations: 5000\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 42.0    0.2  \nSfemale     -1.0    0.1  \nA            0.3    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 4.1    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n#Fit the linear regression model with S, N and C (Model 3)\nex5_Model.3 &lt;- stan_glm(formula =  C ~ S + N,\n                        family = gaussian(),\n                        data = ex5_sample.bias,\n                        #Prior coefficients\n                        prior = normal(location = c(0, 0), scale = c(2, 1.4)),\n                        #Prior intercept\n                        prior_intercept = normal(location = 10, scale = 6),\n                        #Prior sigma\n                        prior_aux = exponential(rate = 0.5),\n                        iter = 4000, warmup = 1000, \n                        save_warmup = TRUE,\n                        chains = 4, cores = 4,\n                        seed = 2025) #for reproducibility\n#View of the model\nex5_Model.3\n\nstan_glm\n family:       gaussian [identity]\n formula:      C ~ S + N\n observations: 5000\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 18.2    0.6  \nSfemale      0.0    0.1  \nN            0.7    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.7    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nIn Bayesian analysis, there are important diagnostics that have to be carried out in order to assess the convergence and efficiency of the Markov Chains. This is done by using the monitor() function which computes summaries of MCMC (Markov Chain Monte Carlo) draws and monitor convergence. Specifically, we will look at Rhat, Bulk_ESS and Tail_ESS metrics.\n\n#Diagnostics for model 1\nmonitor(ex5_Model.1$stanfit)  \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       49.0     49.2     49.4     49.2 0.1     1    11813     8119\nSfemale           -0.3     -0.1      0.2     -0.1 0.1     1    11714     7257\nsigma              4.5      4.6      4.7      4.6 0.0     1    11962     8118\nmean_PPD          49.0     49.2     49.3     49.2 0.1     1    12216    10407\nlog-posterior -14737.4 -14734.6 -14733.6 -14735.0 1.2     1     4766     7546\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 2\nmonitor(ex5_Model.2$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       41.6     42.0     42.3     42.0 0.2     1    15213     8716\nSfemale           -1.2     -1.0     -0.8     -1.0 0.1     1    13319     7901\nA                  0.3      0.3      0.3      0.3 0.0     1    13720     9289\nsigma              4.1      4.1      4.2      4.1 0.0     1    15589     8367\nmean_PPD          49.0     49.2     49.3     49.2 0.1     1    12840    11249\nlog-posterior -14206.2 -14203.2 -14201.8 -14203.5 1.4     1     5074     7215\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\n\n#Diagnostics for model 2\nmonitor(ex5_Model.3$stanfit) \n\nInference for the input samples (4 chains: each with iter = 4000; warmup = 0):\n\n                    Q5      Q50      Q95     Mean  SD  Rhat Bulk_ESS Tail_ESS\n(Intercept)       17.2     18.2     19.1     18.2 0.6     1    14685     8872\nSfemale           -0.2      0.0      0.2      0.0 0.1     1    15154     8110\nN                  0.7      0.7      0.7      0.7 0.0     1    14833     8453\nsigma              3.6      3.7      3.7      3.7 0.0     1    14710     8520\nmean_PPD          49.0     49.2     49.3     49.2 0.1     1    13218    10150\nlog-posterior -13619.1 -13616.0 -13614.7 -13616.4 1.4     1     5176     7062\n\nFor each parameter, Bulk_ESS and Tail_ESS are crude measures of \neffective sample size for bulk and tail quantities respectively (an ESS &gt; 100 \nper chain is considered good), and Rhat is the potential scale reduction \nfactor on rank normalized split chains (at convergence, Rhat &lt;= 1.05).\n\n\nRhat is a metric used to assess the convergence of Markov Chain Monte Carlo (MCMC) simulations. It helps determine if the MCMC chains have adequately explored the target posterior distribution. Specifically, it compares the between- and within-chain estimates for model parameters: If chains have not mixed well (i.e., the between- and within-chain estimates do not agree), R-hat is larger than 1. A general rule of thumb is to use the sample only if R-hat is less than 1.05; a larger value suggests that the chains have not mixed well, and the results might not be reliable. In our two models, all Rhat are equal to 1 indicating that the chains have mixed well and have adequately explored the target posterior distribution.\nBulk_ESS and Tail_ESS stand for ‘Bulk Effective Sample Size’ and ‘Tail Effective Sample Size,’ respectively. Since MCMC samples are not truly independent (they are correlated), these metrics assess the sampling efficiencies, that is, they help evaluate how efficiently the MCMC sampler is exploring the parameter space.\n\n\nBulk_ESS is a useful measure for sampling efficiency in the bulk (center) of the distribution (e.g., efficiency of mean and median estimates);\n\nTail_ESS is a useful measure for sampling efficiency in the tails of the distribution (e.g., efficiency of variance and tail quantile estimates). A general rule of thumb is that both Bulk-ESS and Tail-ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable. In our two models, all Bulk-ESS and Tail-ESS are well above 400 (i.e., 100 multiplied by 4, the number of chains we used) indicating that estimates of posterior quantiles are reliable.\n\nSince we have established that the posteriors are reliable, we can now explore the model estimates. The estimated coefficients for the two models are then plotted in Fig. 5.8.\n\nCode#Extract draws from model 1 \npost_ex5_Model.1 &lt;-\n  ex5_Model.1 %&gt;% \n  spread_draws(Sfemale) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.1') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex5_Model.2 &lt;-\n  ex5_Model.2 %&gt;% \n  spread_draws(Sfemale) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.2') #add a new column to specify that the model\n\n#Extract draws from model 2\npost_ex5_Model.3 &lt;-\n  ex5_Model.3 %&gt;% \n  spread_draws(Sfemale) %&gt;% #extract draws from the fitted model\n  mutate(model = 'Model.3') #add a new column to specify that the model\n\n#Combine draws\nplot.post &lt;- rbind(post_ex5_Model.1, post_ex5_Model.2, post_ex5_Model.3)\n\n# Plot\nplot.post  %&gt;%\n  ggplot(aes(y = model, x = Sfemale)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = b_sex.conf[2], alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.25),\n                     limits = c(-1.55, 0.55)\n                     ) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 5.8. Posterior distribution of the biological sex coefficient for the three models. The black line and dot at the bottom of each distribution represent the highest density interval (HDI) and the mean, respectively.\n\n\n\n\nFig. 5.8 shows the estimates (mean and 95% HDI) of the coefficient for biological sex for Model.1, Model.2, and Model.3.\nFor Model.1 we found a negative coefficient between biological sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\) (mean = -0.072, 95% HDI [-0.337, 0.193]). The estimated causal effect is biased, leading to the wrong conclusion that changing biological sex from male to female causes a decrease in acoustic sensation by -0.072 units, on average.\nFor Model.2 we found a negative coefficient between biological sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\) (mean = -0.997, 95% HDI [-1.243, -0.756]). The estimated causal effect is unbiased, leading to the correct conclusion that changing biological sex from male to female causes a decrease in acoustic sensation by -0.997 units, on average.\nFor Model.3 we found a positive coefficient between biological sex \\((\\text{S})\\) and acoustic sensation \\((\\text{C})\\) (mean = -0.011, 95% HDI [-0.224, 0.195]). The estimated causal effect is biased, leading to the wrong conclusion that changing biological sex from male to female causes a decrease in acoustic sensation by -0.011 units, on average.\nImportantly, for both Model.1, Model.2 and Model.3, the 95% HDI is the range of parameter values within which the most credible 95% of the posterior distribution falls. Unlike a frequentist confidence interval, the Bayesian 95% HDI has a direct probabilistic meaning: every point inside the HDI has a higher probability density than any point outside the interval. Therefore, given the model, the prior and the data, we can say that there is a 95% probability that the data-generating parameter (i.e., b_sex.conf = -1) lies within the HDI. However, since Model.1 and Model.3 lead to biased estimates, we will reach the wrong conclusion by stating that there is a 95% probability that the data-generating parameter lies within the [-0.337, 0.193] and [-0.224, 0.195] intervals, for Model.1 and Model.3 respectively. This probability is 0%.\nAs expected, the inclusion of age \\((\\text{A})\\) in the regression model (i.e., using Model.2) leads to the correct estimate of the total average causal effect, while its exclusion (i.e., using Model.1 and Model.3) leads to bias.\nThe bias occurs because there is selection bias. Generally, selection bias takes place when the participants selected for a study, are not representative of the larger population intended to be analyzed. In our example, participation is not random but influenced by variables that also affect the outcome, specifically, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\). As a result, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) are associated in the biased sample, while they are not in the population (and consequently the random sample).\nTo better explain this bias, we visualize the relationship between biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) in the biased and random sample.\n\nCodeex5_plot %&gt;%\n  ggplot(aes(x = as.numeric(S), y = A)) +\n  geom_jitter(aes(fill = S, colour = S), shape = 19, size = 2, alpha = 0.1, stroke = NA, width = 0.2) + \n  geom_hline(data = truth_line, mapping = aes(yintercept = A , linetype = 'linea'), colour = 'black') +\n  geom_segment(data = line_val, aes(x = 1, y = male, xend = 2, yend = female), colour = 'black', linetype = 'solid', linewidth = 0.5) +\n  geom_point(data = mean_val, aes(fill = S, shape = '2'), colour = 'black', size = 3, stroke = 0.75) + \n  scale_colour_manual('',\n                      breaks = c('male', 'female', 'black'),\n                      values = c('#0072B2', '#D55E00', 'black')) +\n  scale_fill_manual('',\n                    breaks = c('male', 'female', 'black'),\n                    values = c('#0072B2', '#D55E00', NA)) +\n  scale_shape_manual('',\n                     breaks = c('2'),\n                     labels = c('mean age (sample)'),\n                     values = c(23)) +\n  scale_linetype_manual('',\n                        values = c('dashed'),\n                        breaks = c('linea'),\n                        labels = c('mean age (population)')) +\n  scale_x_continuous('Biological sex',\n                     breaks = c(1, 2),\n                     labels = c('male', 'female')) + \n  scale_y_continuous('Age', \n                     breaks = seq(from = 0, to = 100, by = 10)) +\n  guides(colour = guide_legend(override.aes = list(alpha = 1, fill = NA, size = 2), order = 1),\n         fill = 'none',\n         shape = guide_legend(override.aes = list(alpha = 1, size = 2), order = 2),\n         linetype = guide_legend(override.aes = list(alpha = 1, size = 2), order = 3)\n         ) +\n  facet_grid(.~sample) + \n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 5.9. Visualization of the relationship between biological sex and age in the biased and random sample.\n\n\n\n\nFig. 5.9 shows the relationship between biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) in the biased and random sample. The horizontal dashed black line represents the average age in the population (i.e., 38.50 years) for both males and females. In the random sample biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) are not associated and the average age for males (i.e., 38.66 years) and females (i.e., 38.85 years) are similar to one another with a difference of 0.20 years. However, due to selection bias, the average age for males and females in the biased sample is clearly different from the population average. Additionally, biological sex \\((\\text{S})\\) and age \\((\\text{A})\\) are associated because females are more likely to be selected than males, and older people are less likely to be selected than younger people. As a result, females in the biased sample are, on average, older than males (average of 28.70 years for females, 25.44 years for males and a difference of 3.26 years).\nWe can test for a difference between the average age of males and females more formally using Bayesian bootstrapping. We will do so by using the custom functions my_Bayesian.bootstrap() (see code below).\n\nCode#Perform bootstrapping\nmy_Bayesian.bootstrap &lt;-\n  function(data, n_bootstrap, variable, group, group.level){\n    data &lt;- as.data.frame(data)\n    bootstrap &lt;- data.frame(c(bayesboot(data = data[ data[[group]] == group.level[1],  variable], R = n_bootstrap, weighted.mean, use.weights = TRUE) %&gt;% pull()),\n                            c(bayesboot(data = data[ data[[group]] == group.level[2],  variable], R = n_bootstrap, weighted.mean, use.weights = TRUE) %&gt;% pull())) \n    \n    colnames(bootstrap) &lt;- c(group.level[1], group.level[2])\n    return(bootstrap)\n  }\n\n\nFirst, we define the number of times to run the my_Bayesian.bootstrap() function. We fixed the number of bootstrap samples equal to 10,000 (i.e., n_bootstrap = 1e4). Subsequently, we run the my_Bayesian.bootstrap() function using as input both the random and biased sample.\n\nset.seed(2025) #for reproducibility\n#Bootstrap for random sample\nBayesian.bootstrap_sample.random &lt;- \n  my_Bayesian.bootstrap(data = ex5_sample.random, \n                        n_bootstrap = 1e4,\n                        variable = 'A', \n                        group = 'S',\n                        group.level = c('male', 'female'))\n\nset.seed(2025) #for reproducibility\n#Bootstrap for biased sample\nBayesian.bootstrap_sample.biased &lt;- \n  my_Bayesian.bootstrap(data = ex5_sample.bias, \n                        n_bootstrap = 1e4,\n                        variable = 'A', \n                        group = 'S',\n                        group.level = c('male', 'female'))\n\nWe can then calculate and visualize the age difference of the posterior sample between females and males (see Fig. 5.10).\n\nCode#Extract draws from model (random sample)\npost_Bayesian.bootstrap_sample.random &lt;-\n  Bayesian.bootstrap_sample.random %&gt;% \n  mutate(diff = female - male,\n         sample = 'random') #add a new column to specify that the model\n\n#Extract draws from model (biased sample)\npost_Bayesian.bootstrap_sample.biased &lt;-\n  Bayesian.bootstrap_sample.biased %&gt;% \n    mutate(diff = female - male,\n           sample = 'biased') #add a new column to specify that the model\n\n#Combine draws\nplot.post.corr &lt;- rbind(post_Bayesian.bootstrap_sample.random, post_Bayesian.bootstrap_sample.biased)\n\n# Plot\nplot.post.corr %&gt;%\n  ggplot(aes(y = sample, x = diff)) +\n  stat_slabinterval(point_interval = 'mean_hdi',\n                    .width = c(.95)) +\n  geom_vline(xintercept = 0, alpha = 0.8, linetype = 'dashed', colour = 'blue') + \n  scale_x_continuous('Estimate', \n                     breaks = seq(from = -5, to = 5, by = 0.5),\n                     ) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA),\n        axis.title.y = element_blank())\n\n\n\n\n\n\nFig. 5.10. Age difference between males and females in the biased and random sample.\n\n\n\n\nFor the random sample, the age difference between males and females has a mean of 0.20 with 95% HDI [-0.51, 0.83]. Instead, for the biased sample, the age difference between males and females has a mean of 3.26 with 95% HDI [2.88, 3.64].\nHowever, once we adjust for age \\((\\text{A})\\) in the regression model (i.e., using Model.2) the bias disappears. We can visualize this in Fig. 5.11.\n\nCode#Expected value of the posterior predictive distribution (epred) for model 2\nepred_ex5_Model.2 &lt;- \n  ex5_Model.2 %&gt;%\n  epred_draws(newdata = ex5_sample.bias, \n              seed = 2025) %&gt;%\n  group_by(A, S) %&gt;%\n  mean_hdi(.epred, .width = .95) %&gt;%\n  ungroup()\n\nex5_sample.bias %&gt;%\n  ggplot(aes(x = A, y = C)) +\n  geom_point(aes(colour = S), shape = 19, size = 2, alpha = 0.07, stroke = NA) +\n  geom_ribbon(data = epred_ex5_Model.2, aes(x = A , y = .epred, ymin = .lower, ymax = .upper, fill = S), alpha = 0.2, show.legend = NA) +\n  geom_line(data = epred_ex5_Model.2, aes(x = A , y = .epred, colour = S, linetype = 'sample', linewidth = 'sample')) + \n  geom_line(data = truth, mapping = aes(colour = S, linetype = 'pop', linewidth = 'pop')) +\n\n  scale_colour_manual('',\n                      breaks = c('male', 'female'),\n                      values = c('#0072B2', '#D55E00')) +\n  scale_fill_manual('',\n                      breaks = c('male', 'female'),\n                      values = c('#0072B2', '#D55E00')) +\n  scale_linetype_manual('',\n                        breaks = c('pop', 'sample'),\n                        labels = c('population', 'sample'),\n                        values = c('dashed', 'solid')) +\n  scale_linewidth_manual('',\n                         breaks = c('pop', 'sample'),\n                         labels = c('population', 'sample'),\n                         values = c(1, 0.5)) +\n  scale_x_continuous('Age') + \n  scale_y_continuous('acoustic sensation', \n                     breaks = seq(from = 0, to = 100, by = 10)) +\n  facet_grid(.~S) + \n  guides(linetype = guide_legend(override.aes = list(alpha = 1, colour = 'black', fill = NA, linewidth = 0.5))) +\n  theme(legend.position = 'bottom',\n        legend.direction = 'horizontal',\n        panel.grid = element_blank(),\n        panel.background = element_blank(),\n        panel.border = element_rect(colour = 'black', fill = NA))\n\n\n\n\n\n\nFig. 5.11. Visualization of the relationship between biological sex and acoustic sensation conditioning on age.\n\n\n\n\nIn Fig. 5.11, we can observe that within each age level the regression lines for the biased sample for males (i.e., solid blue line) and females (i.e., solid orange line) is very similar to the population lines (i.e., dashed lines). Adjusting for age closed the backdoor path leading to the correct estimate of the total average causal effect.",
    "crumbs": [
      "Selection bias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "ex5_simulation.html#session-info",
    "href": "ex5_simulation.html#session-info",
    "title": "5  Simulation example",
    "section": "Session info",
    "text": "Session info\nVersion information about R, the OS and attached or loaded packages.\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 26200)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] bayesboot_0.2.3     rstan_2.32.7        StanHeaders_2.32.10\n [4] gt_1.1.0            lubridate_1.9.4     forcats_1.0.1      \n [7] stringr_1.6.0       dplyr_1.1.4         purrr_1.2.0        \n[10] readr_2.1.6         tidyr_1.3.1         tibble_3.3.0       \n[13] ggplot2_4.0.1       tidyverse_2.0.0     ggdist_3.3.3       \n[16] tidybayes_3.0.7     rstanarm_2.32.2     Rcpp_1.1.0         \n[19] dagitty_0.3-4       ggdag_0.2.13       \n\nloaded via a namespace (and not attached):\n  [1] backports_1.5.0       plyr_1.8.9            igraph_2.2.1         \n  [4] splines_4.2.3         svUnit_1.0.8          crosstalk_1.2.2      \n  [7] rstantools_2.5.0      inline_0.3.21         digest_0.6.38        \n [10] htmltools_0.5.8.1     viridis_0.6.5         magrittr_2.0.4       \n [13] checkmate_2.3.3       memoise_2.0.1         tzdb_0.5.0           \n [16] graphlayouts_1.2.2    RcppParallel_5.1.11-1 matrixStats_1.5.0    \n [19] xts_0.14.1            timechange_0.3.0      colorspace_2.1-2     \n [22] ggrepel_0.9.6         rbibutils_2.4         xfun_0.54            \n [25] jsonlite_2.0.0        lme4_1.1-37           survival_3.8-3       \n [28] zoo_1.8-14            glue_1.8.0            reformulas_0.4.2     \n [31] polyclip_1.10-7       gtable_0.3.6          V8_8.0.1             \n [34] distributional_0.5.0  car_3.1-3             pkgbuild_1.4.8       \n [37] abind_1.4-8           scales_1.4.0          miniUI_0.1.2         \n [40] viridisLite_0.4.2     xtable_1.8-4          Formula_1.2-5        \n [43] stats4_4.2.3          DT_0.34.0             htmlwidgets_1.6.4    \n [46] threejs_0.3.4         arrayhelpers_1.1-0    RColorBrewer_1.1-3   \n [49] posterior_1.6.1       pkgconfig_2.0.3       loo_2.8.0            \n [52] farver_2.1.2          sass_0.4.10           utf8_1.2.6           \n [55] tidyselect_1.2.1      labeling_0.4.3        rlang_1.1.6          \n [58] reshape2_1.4.5        later_1.4.4           tools_4.2.3          \n [61] cachem_1.1.0          cli_3.6.5             generics_0.1.4       \n [64] evaluate_1.0.5        fastmap_1.2.0         yaml_2.3.10          \n [67] knitr_1.50            fs_1.6.6              tidygraph_1.3.1      \n [70] ggraph_2.2.2          nlme_3.1-168          mime_0.13            \n [73] xml2_1.5.0            compiler_4.2.3        bayesplot_1.14.0     \n [76] shinythemes_1.2.0     rstudioapi_0.17.1     curl_7.0.0           \n [79] tweenr_2.0.3          stringi_1.8.7         lattice_0.22-7       \n [82] Matrix_1.6-5          nloptr_2.2.1          markdown_2.0         \n [85] shinyjs_2.1.0         tensorA_0.36.2.1      vctrs_0.6.5          \n [88] pillar_1.11.1         lifecycle_1.0.4       Rdpack_2.6.4         \n [91] httpuv_1.6.16         QuickJSR_1.8.1        R6_2.6.1             \n [94] promises_1.5.0        gridExtra_2.3         codetools_0.2-20     \n [97] boot_1.3-32           colourpicker_1.3.0    MASS_7.3-58.2        \n[100] gtools_3.9.5          withr_3.0.2           shinystan_2.6.0      \n[103] parallel_4.2.3        hms_1.1.4             grid_4.2.3           \n[106] coda_0.19-4.1         minqa_1.2.8           rmarkdown_2.30       \n[109] S7_0.2.1              carData_3.0-5         otel_0.2.0           \n[112] ggforce_0.5.0         shiny_1.11.1          base64enc_0.1-3      \n[115] dygraphs_1.1.1.6     \n\n\n\n\n\n\nBååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian bootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and create elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to applied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian applied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., Roy, O., 2025. Gt: Easily create presentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of distributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of distributions and uncertainty in the grammar of graphics. IEEE Transactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated development environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R interface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M., Ellison, G.T., 2016. Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D., Seidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., Yutani, H., 2019. Welcome to the tidyverse. Journal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Selection bias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation example</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bååth, R., 2025. Bayesboot: An implementation of rubin’s (1981) bayesian\nbootstrap. https://doi.org/10.32614/CRAN.package.bayesboot\n\n\nBarrett, M., 2024. Ggdag: Analyze and\ncreate elegant directed acyclic graphs.\n\n\nFox, J., Weisberg, S., 2019. An R companion to\napplied regression, Third. ed. Sage, Thousand Oaks CA.\n\n\nGoodrich, B., Gabry, J., Ali, I., Brilleman, S., 2024. Rstanarm: Bayesian\napplied regression modeling via Stan.\n\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J.,\nBrevoort, K., Roy, O., 2025. Gt: Easily create\npresentation-ready display tables.\n\n\nKay, M., 2025. ggdist: Visualizations of\ndistributions and uncertainty. https://doi.org/10.5281/zenodo.3879620\n\n\nKay, M., 2024b. tidybayes: Tidy data and\ngeoms for Bayesian models. https://doi.org/10.5281/zenodo.1308151\n\n\nKay, M., 2024a. ggdist: Visualizations of\ndistributions and uncertainty in the grammar of graphics. IEEE\nTransactions on Visualization and Computer Graphics 30, 414–424. https://doi.org/10.1109/TVCG.2023.3327195\n\n\nPosit team, 2025. RStudio: Integrated\ndevelopment environment for r. Posit Software, PBC, Boston, MA.\n\n\nR Core Team, 2023. R: A language\nand environment for statistical computing. R Foundation for\nStatistical Computing, Vienna, Austria.\n\n\nStan Development Team, 2025. RStan: The R\ninterface to Stan.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M.S., Liśkiewicz, M.,\nEllison, G.T., 2016. Robust causal inference using directed acyclic\ngraphs: The R package ’dagitty’. International Journal of\nEpidemiology 45, 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François,\nR., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen,\nT.L., Miller, E., Bache, S.M., Müller, K., Ooms, J., Robinson, D.,\nSeidel, D.P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K.,\nYutani, H., 2019. Welcome to the tidyverse.\nJournal of Open Source Software 4, 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]